# IPFS/IPLD Native Graph Database: Comprehensive Refactoring Plan

**Version:** 1.0  
**Date:** 2026-02-15  
**Status:** Planning Phase  
**Target Completion:** 8 weeks

---

## Executive Summary

This document outlines a comprehensive plan to transform the `ipfs_datasets_py/knowledge_graphs/` folder into a **fully-fledged IPFS/IPLD-native graph database** with Neo4j feature parity and drop-in API compatibility. The goal is to create a decentralized, content-addressed graph database that enables seamless migration from Neo4j while leveraging IPFS's distributed infrastructure.

### Key Objectives

1. **Neo4j Feature Parity**: Implement all core Neo4j capabilities (Cypher queries, ACID transactions, indexing, constraints)
2. **Drop-in API Compatibility**: Provide Neo4j-compatible Python driver API for zero-code migration
3. **IPFS/IPLD Native**: Build on content-addressed storage with libp2p networking
4. **JSON-LD Translation**: Convert semantic web data to IPLD format automatically
5. **Performance**: Match or exceed Neo4j performance for read-heavy workloads using IPFS caching

---

## Current State Analysis

### Existing Assets

#### 1. Knowledge Graphs Module (`knowledge_graphs/`)
- **10 Python modules** with 5,000+ lines of production code
- **Entity/Relationship Extraction**: Domain-specific patterns (academic, technical, finance)
- **IPLD Storage**: Content-addressed graph persistence with automatic chunking
- **Cross-Document Reasoning**: Multi-document lineage tracking and entity mediation
- **IR Query System**: Query compiler with budget management
- **SPARQL Integration**: External knowledge base validation

#### 2. GraphRAG Processors (`processors/graphrag/`)
- **6 processor files** with 8-phase production pipeline
- **Hybrid Search**: Vector + graph traversal with configurable weighting
- **LLM Integration**: Enhanced reasoning with semantic validation
- **Multi-modal Processing**: HTML, PDF, audio, video, image content
- **Performance Monitoring**: Analytics and optimization profiling

#### 3. Search Integration (`search/graphrag_query/`, `search/graphrag_integration/`)
- **GraphQueryExecutor**: IR-based query execution with strict budget enforcement
- **Backend Abstraction**: IPLDKnowledgeGraphBackend, ShardedCARBackend
- **Evidence Chains**: Document connection paths with reasoning traces
- **Confidence Scoring**: Uncertainty handling and validation

#### 4. IPLD Infrastructure (`data_transformation/ipld/`)
- **IPLDStorage**: Content-addressed storage with CID management
- **CAR File Support**: Import/export for data portability
- **Vector Store**: IPLD-based vector search with FAISS integration
- **Dual-mode Operation**: IPFS daemon + local fallback
- **Batch Processing**: Parallel processing with performance stats

### Gap Analysis: Neo4j Features vs Current Implementation

| Feature Category | Neo4j | Current Implementation | Gap |
|-----------------|-------|------------------------|-----|
| **Query Language** | Cypher (declarative, pattern-matching) | IR-based (JSON operations) | âŒ Need Cypher parser |
| **ACID Transactions** | Full ACID with rollback | No transactions | âŒ Need transaction layer |
| **Indexing** | B-tree, text, spatial indexes | Type-based indexing only | âš ï¸ Need advanced indexing |
| **Constraints** | Unique, existence, node key | None | âŒ Need constraint system |
| **Native Graph Storage** | Index-free adjacency | IPLD DAG with CID links | âœ… Already native |
| **APIs** | Bolt protocol, HTTP, drivers | Python API only | âŒ Need Neo4j-compatible API |
| **Clustering** | Causal clustering | IPFS distributed storage | âš ï¸ Partial via IPFS |
| **Performance** | Optimized traversal | Vector-augmented queries | âš ï¸ Need optimization |
| **Schema** | Optional schema | Schemaless | âœ… Already flexible |
| **Backup/Restore** | Built-in tools | CAR export/import | âš ï¸ Need tooling |

**Legend**: âœ… Implemented | âš ï¸ Partial | âŒ Missing

---

## Architecture Design

### High-Level Architecture

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                     Application Layer                           â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â” â”‚
â”‚  â”‚  Neo4j Driver    â”‚  â”‚  Cypher Shell    â”‚  â”‚   REST API    â”‚ â”‚
â”‚  â”‚  Compatibility   â”‚  â”‚                  â”‚  â”‚               â”‚ â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”˜ â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
            â”‚                    â”‚                  â”‚
            â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                                 â”‚
            â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
            â”‚         Query Processing Layer            â”‚
            â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â” â”‚
            â”‚  â”‚      Cypher Parser & Compiler        â”‚ â”‚
            â”‚  â”‚  (Text â†’ AST â†’ IR â†’ Execution Plan)  â”‚ â”‚
            â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜ â”‚
            â”‚                     â”‚                      â”‚
            â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â” â”‚
            â”‚  â”‚     Query Optimizer                  â”‚ â”‚
            â”‚  â”‚  (Cost-based, Index selection)       â”‚ â”‚
            â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜ â”‚
            â”‚                     â”‚                      â”‚
            â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â” â”‚
            â”‚  â”‚     Query Executor                   â”‚ â”‚
            â”‚  â”‚  (Budget management, pagination)     â”‚ â”‚
            â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜ â”‚
            â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                                  â”‚
            â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
            â”‚        Transaction Layer                    â”‚
            â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”   â”‚
            â”‚  â”‚  Transaction Manager (ACID Support)  â”‚   â”‚
            â”‚  â”‚  - Isolation levels                  â”‚   â”‚
            â”‚  â”‚  - Write-ahead logging               â”‚   â”‚
            â”‚  â”‚  â”‚  - Conflict resolution            â”‚   â”‚
            â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜   â”‚
            â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                                  â”‚
            â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
            â”‚         Storage Layer                       â”‚
            â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”   â”‚
            â”‚  â”‚     IPLD Knowledge Graph Engine      â”‚   â”‚
            â”‚  â”‚  - Entity/Relationship management    â”‚   â”‚
            â”‚  â”‚  - Index management                  â”‚   â”‚
            â”‚  â”‚  - Constraint enforcement            â”‚   â”‚
            â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜   â”‚
            â”‚                     â”‚                        â”‚
            â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”   â”‚
            â”‚  â”‚      IPLD Storage Backend            â”‚   â”‚
            â”‚  â”‚  - Content-addressed storage         â”‚   â”‚
            â”‚  â”‚  - CID management                    â”‚   â”‚
            â”‚  â”‚  - CAR file operations               â”‚   â”‚
            â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜   â”‚
            â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                                  â”‚
            â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
            â”‚        IPFS/LibP2P Layer                    â”‚
            â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â” â”‚
            â”‚  â”‚ IPFS Daemonâ”‚  â”‚  LibP2P  â”‚  â”‚  Local  â”‚ â”‚
            â”‚  â”‚            â”‚  â”‚ Routing  â”‚  â”‚  Cache  â”‚ â”‚
            â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜ â”‚
            â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

### Component Breakdown

#### 1. Neo4j Driver Compatibility Layer

**Purpose**: Provide drop-in replacement for `neo4j` Python driver

**Key Classes**:
```python
# Drop-in replacement for neo4j.GraphDatabase
class IPFSGraphDatabase:
    @staticmethod
    def driver(uri: str, auth: tuple) -> IPFSDriver
    
class IPFSDriver:
    def session(self, database: str = None) -> IPFSSession
    def close(self)
    
class IPFSSession:
    def run(self, query: str, parameters: dict = None) -> Result
    def read_transaction(self, func)
    def write_transaction(self, func)
    def close()
    
class Result:
    def single() -> Record
    def data() -> list
    def graph() -> Graph
```

**Features**:
- Compatible with existing Neo4j driver code
- Translates Neo4j connection URIs to IPFS endpoints
- Supports both bolt:// and ipfs:// protocols
- Authentication mapping to IPFS access tokens

#### 2. Cypher Query Language Layer

**Purpose**: Parse and execute Cypher queries on IPLD graphs

**Architecture**:
```
Cypher Text â†’ Lexer â†’ Parser â†’ AST â†’ Query Planner â†’ IR â†’ Executor
```

**Implementation Approach**:
- Use ANTLR or PLY for parser generation
- Support Cypher 5.0 specification (Neo4j 5.x compatible)
- Translate Cypher patterns to graph traversal operations
- Optimize queries using cost-based query planning

**Supported Cypher Features**:

| Feature | Priority | Complexity |
|---------|----------|------------|
| MATCH patterns | P0 | Medium |
| WHERE clauses | P0 | Low |
| RETURN projections | P0 | Low |
| CREATE nodes/relationships | P0 | Medium |
| SET properties | P0 | Low |
| DELETE nodes/relationships | P0 | Medium |
| MERGE (upsert) | P1 | High |
| WITH clause (piping) | P1 | Medium |
| ORDER BY, LIMIT, SKIP | P0 | Low |
| Aggregations (COUNT, SUM, etc.) | P1 | Medium |
| Path functions (shortestPath) | P1 | High |
| Indexes (CREATE INDEX) | P1 | Medium |
| Constraints | P1 | High |
| Subqueries | P2 | High |
| CALL procedures | P2 | High |

**Example Query Translation**:

```cypher
// Input Cypher
MATCH (p:Person {name: "Alice"})-[:KNOWS]->(friend:Person)
WHERE friend.age > 25
RETURN friend.name, friend.age
ORDER BY friend.age DESC
LIMIT 10
```

Translates to IR:
```python
{
    "operations": [
        {"op": "ScanType", "type": "Person", "filters": {"name": "Alice"}},
        {"op": "Expand", "relationship": "KNOWS", "direction": "outgoing"},
        {"op": "Filter", "type": "Person", "predicate": {"age": {">": 25}}},
        {"op": "Project", "fields": ["name", "age"]},
        {"op": "OrderBy", "field": "age", "direction": "DESC"},
        {"op": "Limit", "count": 10}
    ]
}
```

#### 3. Transaction Layer

**Purpose**: Provide ACID guarantees for graph operations

**Design**:
```python
class TransactionManager:
    def begin_transaction(self, isolation_level: str) -> Transaction
    def commit(self, txn: Transaction)
    def rollback(self, txn: Transaction)
    
class Transaction:
    def write_node(self, node: Node)
    def write_relationship(self, rel: Relationship)
    def delete_node(self, node_id: str)
    def delete_relationship(self, rel_id: str)
    def get_snapshot(self) -> GraphSnapshot
```

**Implementation Strategy**:

1. **Write-Ahead Logging (WAL)**:
   - Store operations in IPLD before applying
   - Each operation gets a CID
   - WAL stored as linked list of operation blocks

2. **Optimistic Concurrency Control**:
   - Version tracking using IPLD CID chains
   - Conflict detection via CID comparison
   - Retry logic for write conflicts

3. **Isolation Levels**:
   - READ_UNCOMMITTED: No isolation (fastest)
   - READ_COMMITTED: Only see committed changes
   - REPEATABLE_READ: Snapshot isolation (default)
   - SERIALIZABLE: Full serializability

4. **Durability**:
   - Pin transaction logs to IPFS
   - Replicate across cluster nodes
   - Support for CAR file snapshots

**Transaction Flow**:
```
Begin â†’ Build WAL â†’ Validate â†’ Apply Changes â†’ Commit to IPFS â†’ Return
          â†“                                         â†“
        Rollback â† Conflict Detection â†â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

#### 4. JSON-LD to IPLD Translation Module

**Purpose**: Convert semantic web data (JSON-LD) to IPLD graph format

**Key Features**:
- Parse JSON-LD contexts and expand terms
- Convert @id references to IPLD CIDs
- Map RDF predicates to typed relationships
- Preserve semantic meaning in IPLD properties
- Support for JSON-LD framing and compaction

**Architecture**:
```python
class JSONLDTranslator:
    def jsonld_to_ipld(self, jsonld: dict, context: dict = None) -> IPLDGraph
    def ipld_to_jsonld(self, ipld_graph: IPLDGraph, context: dict = None) -> dict
    def expand_context(self, jsonld: dict) -> dict
    def compact_context(self, expanded: dict, context: dict) -> dict
```

**Translation Mapping**:

| JSON-LD Concept | IPLD Mapping |
|-----------------|--------------|
| @id | Entity.id (with CID reference) |
| @type | Entity.type |
| @context | Stored in graph metadata |
| @graph | Root graph container |
| @vocab | Default namespace mapping |
| Object properties | Relationships |
| Data properties | Entity properties |
| Blank nodes | Anonymous entities |

**Example Translation**:

```json
// Input JSON-LD
{
  "@context": "https://schema.org/",
  "@id": "https://example.com/person/alice",
  "@type": "Person",
  "name": "Alice Smith",
  "knows": {
    "@id": "https://example.com/person/bob",
    "@type": "Person",
    "name": "Bob Jones"
  }
}
```

Converts to IPLD:
```python
IPLDGraph(
    entities=[
        Entity(
            id="QmAlice123...",  # CID
            type="Person",
            properties={
                "name": "Alice Smith",
                "external_id": "https://example.com/person/alice"
            }
        ),
        Entity(
            id="QmBob456...",
            type="Person",
            properties={
                "name": "Bob Jones",
                "external_id": "https://example.com/person/bob"
            }
        )
    ],
    relationships=[
        Relationship(
            type="knows",
            source="QmAlice123...",
            target="QmBob456...",
            properties={}
        )
    ]
)
```

#### 5. Advanced Indexing System

**Purpose**: Optimize query performance with multiple index types

**Index Types**:

1. **B-Tree Indexes** (property indexes):
   ```python
   CREATE INDEX person_name FOR (p:Person) ON (p.name)
   ```
   - Fast exact match and range queries
   - Stored as IPLD-based B-tree structure
   - Support for composite indexes

2. **Full-Text Indexes**:
   ```python
   CREATE FULLTEXT INDEX person_search FOR (p:Person) ON EACH [p.name, p.bio]
   ```
   - Inverted index for text search
   - Support for stemming and stopwords
   - Stored as IPLD document index

3. **Spatial Indexes** (future):
   ```python
   CREATE POINT INDEX location FOR (p:Place) ON (p.coordinates)
   ```
   - R-tree for geospatial queries
   - Distance and bounding box queries

4. **Vector Indexes** (already implemented):
   - FAISS-based similarity search
   - Integration with IPLD vector store
   - Hybrid vector + graph queries

**Index Management**:
```python
class IndexManager:
    def create_index(self, name: str, index_type: IndexType, config: dict)
    def drop_index(self, name: str)
    def list_indexes(self) -> List[IndexInfo]
    def rebuild_index(self, name: str)
    def get_index_stats(self, name: str) -> IndexStats
```

#### 6. Constraint System

**Purpose**: Enforce data integrity rules

**Constraint Types**:

1. **Unique Constraints**:
   ```cypher
   CREATE CONSTRAINT person_email_unique
   FOR (p:Person) REQUIRE p.email IS UNIQUE
   ```

2. **Existence Constraints**:
   ```cypher
   CREATE CONSTRAINT person_name_exists
   FOR (p:Person) REQUIRE p.name IS NOT NULL
   ```

3. **Node Key Constraints** (composite unique):
   ```cypher
   CREATE CONSTRAINT person_key
   FOR (p:Person) REQUIRE (p.firstName, p.lastName) IS NODE KEY
   ```

4. **Property Type Constraints**:
   ```cypher
   CREATE CONSTRAINT person_age_type
   FOR (p:Person) REQUIRE p.age IS :: INTEGER
   ```

**Implementation**:
```python
class ConstraintManager:
    def create_constraint(self, constraint: Constraint)
    def drop_constraint(self, name: str)
    def validate(self, operation: GraphOperation) -> ValidationResult
    def list_constraints(self) -> List[Constraint]
```

---

## Implementation Plan

### Phase 1: Foundation (Weeks 1-2)

**Goals**: Set up core architecture and Neo4j compatibility stubs

**Tasks**:
1. Create module structure
2. Implement Neo4j driver compatibility interface
3. Set up basic query routing
4. Implement connection management
5. Add authentication/authorization hooks

**Deliverables**:
- `neo4j_compat/` module with driver stubs
- `cypher/` module structure
- `transactions/` module structure
- Basic integration tests

**Files to Create**:
```
knowledge_graphs/
â”œâ”€â”€ neo4j_compat/
â”‚   â”œâ”€â”€ __init__.py
â”‚   â”œâ”€â”€ driver.py          # IPFSGraphDatabase, IPFSDriver
â”‚   â”œâ”€â”€ session.py         # IPFSSession
â”‚   â”œâ”€â”€ result.py          # Result, Record
â”‚   â””â”€â”€ types.py           # Node, Relationship, Path
â”œâ”€â”€ cypher/
â”‚   â”œâ”€â”€ __init__.py
â”‚   â”œâ”€â”€ parser.py          # Cypher parser (stub)
â”‚   â”œâ”€â”€ ast.py             # AST node definitions
â”‚   â”œâ”€â”€ compiler.py        # AST â†’ IR compiler
â”‚   â””â”€â”€ optimizer.py       # Query optimization
â”œâ”€â”€ transactions/
â”‚   â”œâ”€â”€ __init__.py
â”‚   â”œâ”€â”€ manager.py         # TransactionManager
â”‚   â”œâ”€â”€ wal.py            # Write-ahead logging
â”‚   â””â”€â”€ isolation.py      # Isolation level handling
â””â”€â”€ core/
    â”œâ”€â”€ __init__.py
    â””â”€â”€ graph_engine.py    # Main graph engine
```

### Phase 2: Cypher Parser (Weeks 3-4)

**Goals**: Implement working Cypher parser and compiler

**Tasks**:
1. Define Cypher grammar (ANTLR/PLY)
2. Implement lexer and parser
3. Build AST representation
4. Create AST â†’ IR compiler
5. Add query validation

**Deliverables**:
- Full Cypher parser supporting basic operations
- Query AST structure
- Compiler generating existing IR format
- Parser test suite (100+ tests)

**Cypher Features (Phase 2)**:
- âœ… MATCH patterns (nodes and relationships)
- âœ… WHERE clauses (basic predicates)
- âœ… RETURN projections
- âœ… CREATE nodes and relationships
- âœ… SET properties
- âœ… DELETE operations
- âœ… ORDER BY, LIMIT, SKIP

### Phase 3: Transaction System (Weeks 5-6)

**Goals**: Implement ACID-compliant transaction layer

**Tasks**:
1. Implement write-ahead logging on IPLD
2. Add transaction isolation levels
3. Implement conflict detection
4. Add rollback mechanism
5. Create transaction test suite

**Deliverables**:
- TransactionManager with full ACID support
- WAL stored in IPLD blocks
- Optimistic concurrency control
- Transaction benchmarks

**Technical Details**:
- WAL block format: `{txn_id, timestamp, operations[], prev_wal_cid}`
- Conflict resolution: CID-based versioning
- Commit protocol: 2-phase commit for distributed transactions

### Phase 4: JSON-LD Translation (Week 7)

**Goals**: Enable JSON-LD to IPLD conversion

**Tasks**:
1. Implement JSON-LD context expansion
2. Create bidirectional translator
3. Add semantic preservation validation
4. Support common vocabularies (schema.org, FOAF, Dublin Core)

**Deliverables**:
- JSONLDTranslator class
- Support for 10+ common vocabularies
- Translation test suite
- Performance benchmarks

**Files to Create**:
```
knowledge_graphs/
â””â”€â”€ jsonld/
    â”œâ”€â”€ __init__.py
    â”œâ”€â”€ translator.py      # JSONLDTranslator
    â”œâ”€â”€ context.py         # Context expansion
    â”œâ”€â”€ vocabularies.py    # Common vocab mappings
    â””â”€â”€ validation.py      # Semantic validation
```

### Phase 5: Advanced Features (Week 8)

**Goals**: Add indexing, constraints, and optimization

**Tasks**:
1. Implement B-tree indexes on IPLD
2. Add constraint validation
3. Implement query optimizer
4. Add cost-based planning
5. Performance tuning

**Deliverables**:
- IndexManager with B-tree and full-text indexes
- ConstraintManager with validation
- Cost-based query optimizer
- Performance improvements (10x target)

---

## Code Refactoring Strategy

### Files to Consolidate

#### 1. Merge GraphRAG Processors â†’ Core Engine

**Current State**: 6 separate GraphRAG processor files with overlapping functionality

**Target State**: Single `GraphRAGEngine` integrated into graph database

**Consolidation Plan**:
```python
# knowledge_graphs/core/graphrag_engine.py
class GraphRAGEngine:
    """Integrated GraphRAG for graph database"""
    
    def extract_from_content(self, content: str, content_type: str) -> KnowledgeGraph:
        """Extract entities/relationships from various content types"""
        pass
    
    def hybrid_search(self, query: str, graph_context: bool = True) -> List[Result]:
        """Hybrid vector + graph search"""
        pass
    
    def cross_document_reasoning(self, query: str, docs: List[str]) -> Answer:
        """Multi-document reasoning with evidence chains"""
        pass
```

**Benefits**:
- Eliminates 2,100+ lines of duplicate code
- Unified API for graph extraction
- Better integration with transaction layer
- Single optimization pipeline

#### 2. Unify Query Systems

**Current State**: 2 separate query systems (IR-based, GraphRAG integration)

**Target State**: Single query engine with Cypher frontend

**Unification Plan**:
```
Cypher Query â†’ Parser â†’ AST â†’ QueryPlanner â†’ IR â†’ Executor
                                    â†“
                              Existing GraphQueryExecutor (reuse)
```

**Code Reuse**:
- Keep `GraphQueryExecutor` from `search/graphrag_query/`
- Keep `ExecutionBudgets` for resource management
- Integrate `HybridVectorGraphSearch` from `search/graphrag_integration/`
- Add Cypher layer on top

#### 3. IPLD Storage Consolidation

**Current State**: IPLD code split between `knowledge_graphs/ipld.py` and `data_transformation/ipld/`

**Target State**: Single `knowledge_graphs/storage/` module

**Files to Merge**:
```
knowledge_graphs/
â””â”€â”€ storage/
    â”œâ”€â”€ __init__.py
    â”œâ”€â”€ ipld_backend.py        # From data_transformation/ipld/
    â”œâ”€â”€ graph_storage.py       # From knowledge_graphs/ipld.py
    â”œâ”€â”€ car_operations.py      # CAR import/export
    â”œâ”€â”€ vector_store.py        # Vector index storage
    â””â”€â”€ cache.py               # Local caching layer
```

---

## Migration Guide

### Neo4j to IPFS Graph Database Migration

#### Step 1: Install IPFS Graph Database

```bash
pip install ipfs-datasets-py[graph-db]
```

#### Step 2: Minimal Code Changes

**Before (Neo4j)**:
```python
from neo4j import GraphDatabase

driver = GraphDatabase.driver("bolt://localhost:7687", auth=("neo4j", "password"))

with driver.session() as session:
    result = session.run("""
        MATCH (p:Person)-[:KNOWS]->(friend)
        WHERE p.name = $name
        RETURN friend.name
    """, name="Alice")
    
    for record in result:
        print(record["friend.name"])

driver.close()
```

**After (IPFS Graph Database)**:
```python
from ipfs_datasets_py.knowledge_graphs.neo4j_compat import GraphDatabase

# Option 1: Connect to IPFS daemon
driver = GraphDatabase.driver("ipfs://localhost:5001", auth=("user", "token"))

# Option 2: Use embedded mode
driver = GraphDatabase.driver("ipfs+embedded://./graph_data")

with driver.session() as session:
    result = session.run("""
        MATCH (p:Person)-[:KNOWS]->(friend)
        WHERE p.name = $name
        RETURN friend.name
    """, name="Alice")
    
    for record in result:
        print(record["friend.name"])

driver.close()
```

**Changes Required**: Only the import statement and connection URI!

#### Step 3: Bulk Data Migration

```python
from ipfs_datasets_py.knowledge_graphs.migration import Neo4jMigrator

migrator = Neo4jMigrator(
    neo4j_uri="bolt://localhost:7687",
    neo4j_auth=("neo4j", "password"),
    ipfs_uri="ipfs://localhost:5001"
)

# Export from Neo4j
migrator.export_to_car("my_graph.car")

# Import to IPFS
graph_cid = migrator.import_from_car("my_graph.car")
print(f"Graph migrated to IPFS: {graph_cid}")
```

#### Step 4: Verify Migration

```python
from ipfs_datasets_py.knowledge_graphs import IPFSGraphDatabase

driver = IPFSGraphDatabase.driver("ipfs://localhost:5001")

with driver.session() as session:
    # Count nodes
    result = session.run("MATCH (n) RETURN count(n) as count")
    print(f"Total nodes: {result.single()['count']}")
    
    # Count relationships
    result = session.run("MATCH ()-[r]->() RETURN count(r) as count")
    print(f"Total relationships: {result.single()['count']}")
```

---

## Performance Targets

### Benchmarks vs Neo4j

| Operation | Neo4j 5.x | Target (IPFS) | Strategy |
|-----------|-----------|---------------|----------|
| Single node read | 0.1 ms | 0.5 ms | IPFS local cache |
| Pattern match (1-hop) | 1 ms | 2 ms | Index optimization |
| Pattern match (3-hop) | 10 ms | 15 ms | Query planning |
| Aggregation (10K nodes) | 100 ms | 150 ms | Parallel processing |
| Write + commit | 5 ms | 10 ms | Async IPFS pinning |
| Bulk import (1M nodes) | 60 s | 90 s | Batch operations |

**Performance Optimization Strategies**:
1. **Aggressive Caching**: Cache frequently accessed CIDs locally
2. **Batch Operations**: Group multiple operations into single IPFS transaction
3. **Index Pre-warming**: Load indexes into memory on startup
4. **Query Result Caching**: Cache query results with TTL
5. **Parallel Execution**: Use asyncio for concurrent IPFS operations
6. **Compression**: Use zstd compression for large graphs

---

## Testing Strategy

### Test Categories

#### 1. Unit Tests (Target: 500+ tests)
- Cypher parser (100 tests)
- Query compiler (50 tests)
- Transaction manager (50 tests)
- Index operations (50 tests)
- Constraint validation (50 tests)
- JSON-LD translator (50 tests)
- IPLD storage (50 tests)
- Neo4j compatibility (100 tests)

#### 2. Integration Tests (Target: 100+ tests)
- End-to-end query execution (30 tests)
- Transaction rollback scenarios (20 tests)
- Multi-user concurrency (20 tests)
- CAR import/export (10 tests)
- Migration workflows (10 tests)
- Performance benchmarks (10 tests)

#### 3. Compatibility Tests (Target: 50+ tests)
- Neo4j driver compatibility (30 tests)
- Cypher query compatibility (20 tests)

#### 4. Stress Tests
- 1M+ node graphs
- 10M+ relationship graphs
- Concurrent write transactions
- Large query result sets
- Memory usage profiling

---

## Documentation Plan

### Documents to Create

1. **User Guide** (`GRAPH_DATABASE_USER_GUIDE.md`)
   - Getting started
   - Connection management
   - Query examples
   - Transaction usage
   - Performance tuning

2. **Migration Guide** (`NEO4J_MIGRATION_GUIDE.md`)
   - Step-by-step migration
   - API compatibility matrix
   - Known limitations
   - Troubleshooting

3. **Developer Guide** (`GRAPH_DATABASE_DEV_GUIDE.md`)
   - Architecture overview
   - Adding new features
   - Custom index types
   - Contributing guidelines

4. **API Reference** (Auto-generated with Sphinx)
   - Full API documentation
   - Code examples
   - Type hints

5. **JSON-LD Guide** (`JSONLD_TO_IPLD_GUIDE.md`)
   - Translation examples
   - Supported vocabularies
   - Custom contexts
   - Best practices

---

## Risk Analysis & Mitigation

### Technical Risks

| Risk | Probability | Impact | Mitigation |
|------|-------------|--------|------------|
| **IPFS Performance** | Medium | High | Implement aggressive caching, benchmark early |
| **Cypher Complexity** | Low | Medium | Start with subset, iterate based on usage |
| **Transaction Conflicts** | Medium | Medium | Implement exponential backoff, partition data |
| **Large Graph Handling** | Low | High | Already solved with IPLD chunking |
| **Migration Data Loss** | Low | Critical | Extensive testing, checksum validation |

### Operational Risks

| Risk | Probability | Impact | Mitigation |
|------|-------------|--------|------------|
| **IPFS Daemon Downtime** | Medium | High | Local fallback mode, daemon monitoring |
| **Breaking API Changes** | Low | High | Extensive compatibility tests, version pinning |
| **Community Adoption** | Medium | Medium | Excellent docs, migration tools, examples |

---

## Success Metrics

### Technical Metrics
- âœ… 95%+ Cypher query compatibility with Neo4j 5.x
- âœ… <2x performance overhead vs Neo4j for read operations
- âœ… 100% API compatibility with neo4j Python driver
- âœ… 90%+ test coverage
- âœ… Zero data loss in migration

### Adoption Metrics
- ðŸ“Š 10+ production users within 6 months
- ðŸ“Š 100+ GitHub stars within 3 months
- ðŸ“Š 5+ community contributions
- ðŸ“Š Documentation completeness: 100%

---

## Timeline & Milestones

```
Week 1-2:  Foundation & Architecture Setup
           â”œâ”€ Neo4j compatibility stubs
           â”œâ”€ Module structure
           â””â”€ Basic tests

Week 3-4:  Cypher Parser Implementation
           â”œâ”€ Grammar definition
           â”œâ”€ Parser & lexer
           â”œâ”€ AST â†’ IR compiler
           â””â”€ Parser tests (100+)

Week 5-6:  Transaction System
           â”œâ”€ Write-ahead logging
           â”œâ”€ ACID implementation
           â”œâ”€ Isolation levels
           â””â”€ Transaction tests (50+)

Week 7:    JSON-LD Translation
           â”œâ”€ Translator implementation
           â”œâ”€ Vocabulary support
           â””â”€ Translation tests (50+)

Week 8:    Advanced Features & Polish
           â”œâ”€ Indexing system
           â”œâ”€ Constraints
           â”œâ”€ Performance optimization
           â””â”€ Documentation

Week 9:    Testing & Benchmarking
           â”œâ”€ Integration tests
           â”œâ”€ Performance tests
           â””â”€ Stress tests

Week 10:   Documentation & Release
           â”œâ”€ User guides
           â”œâ”€ API docs
           â”œâ”€ Migration guide
           â””â”€ v1.0 release
```

---

## Appendix A: API Compatibility Matrix

### Neo4j Driver API Coverage

| API Method | Status | Notes |
|------------|--------|-------|
| `GraphDatabase.driver()` | âœ… Planned | Full compatibility |
| `Driver.session()` | âœ… Planned | Full compatibility |
| `Driver.close()` | âœ… Planned | Full compatibility |
| `Session.run()` | âœ… Planned | Full compatibility |
| `Session.read_transaction()` | âœ… Planned | Full compatibility |
| `Session.write_transaction()` | âœ… Planned | Full compatibility |
| `Session.begin_transaction()` | âœ… Planned | Full compatibility |
| `Result.single()` | âœ… Planned | Full compatibility |
| `Result.data()` | âœ… Planned | Full compatibility |
| `Result.graph()` | âœ… Planned | Full compatibility |
| `Result.consume()` | âœ… Planned | Full compatibility |

### Cypher Query Coverage

| Cypher Feature | Phase | Priority |
|----------------|-------|----------|
| MATCH | Phase 2 | P0 |
| WHERE | Phase 2 | P0 |
| RETURN | Phase 2 | P0 |
| CREATE | Phase 2 | P0 |
| SET | Phase 2 | P0 |
| DELETE | Phase 2 | P0 |
| ORDER BY | Phase 2 | P0 |
| LIMIT/SKIP | Phase 2 | P0 |
| WITH | Phase 3 | P1 |
| MERGE | Phase 3 | P1 |
| OPTIONAL MATCH | Phase 3 | P1 |
| UNION | Phase 3 | P1 |
| UNWIND | Phase 3 | P1 |
| CASE | Phase 3 | P1 |
| Aggregations | Phase 3 | P1 |
| Path functions | Phase 4 | P1 |
| List comprehensions | Phase 4 | P2 |
| Pattern comprehensions | Phase 4 | P2 |
| Subqueries | Phase 4 | P2 |
| CALL procedures | Phase 5 | P2 |

---

## Appendix B: JSON-LD to IPLD Examples

### Example 1: Simple Person Graph

**JSON-LD Input**:
```json
{
  "@context": {
    "@vocab": "http://schema.org/",
    "knows": {"@type": "@id"}
  },
  "@id": "http://example.com/alice",
  "@type": "Person",
  "name": "Alice",
  "age": 30,
  "knows": "http://example.com/bob"
}
```

**IPLD Output**:
```json
{
  "entities": [
    {
      "id": "bafyalice123",
      "type": "Person",
      "properties": {
        "name": "Alice",
        "age": 30,
        "schema_id": "http://example.com/alice"
      }
    }
  ],
  "relationships": [
    {
      "type": "knows",
      "source": "bafyalice123",
      "target": "bafybob456",
      "properties": {}
    }
  ]
}
```

### Example 2: Wikidata Entity

**JSON-LD Input** (Wikidata format):
```json
{
  "@context": "https://www.wikidata.org/wiki/Special:EntityData/",
  "@id": "http://www.wikidata.org/entity/Q5",
  "@type": "wikibase:Item",
  "rdfs:label": "human",
  "wdt:P31": {
    "@id": "http://www.wikidata.org/entity/Q16521"
  }
}
```

**IPLD Output**:
```json
{
  "entities": [
    {
      "id": "bafywdQ5",
      "type": "wikibase:Item",
      "properties": {
        "label": "human",
        "wikidata_id": "Q5"
      }
    }
  ],
  "relationships": [
    {
      "type": "instance_of",
      "source": "bafywdQ5",
      "target": "bafywdQ16521",
      "properties": {
        "wikidata_property": "P31"
      }
    }
  ]
}
```

---

## Appendix C: Related Work & Prior Art

### Similar Projects
1. **OrbitDB** - Distributed database on IPFS (different model)
2. **GunDB** - Decentralized graph database (different storage)
3. **GraphQL on IPFS** - Query layer (not graph database)
4. **Textile** - IPFS data management (not graph-focused)

### Differentiators
- âœ… Full Neo4j API compatibility
- âœ… Cypher query language support
- âœ… ACID transactions
- âœ… JSON-LD semantic web integration
- âœ… Production-ready GraphRAG integration

---

## Appendix D: Future Enhancements (Post v1.0)

1. **Distributed Query Execution** - Query federation across IPFS cluster
2. **Graph Algorithms** - PageRank, community detection, centrality
3. **Real-time Subscriptions** - Pub/sub for graph changes
4. **Visual Query Builder** - Web UI for Cypher queries
5. **ML Integration** - Graph neural networks, embeddings
6. **Multi-tenancy** - Isolated graphs within single instance
7. **Time-travel Queries** - Query historical graph states using CID history
8. **Sharding** - Automatic graph partitioning for scale

---

## Conclusion

This refactoring plan transforms the `knowledge_graphs/` folder into a **production-ready, Neo4j-compatible, IPFS-native graph database**. By building on existing assets (IPLD storage, GraphRAG processing, IR query execution) and adding Neo4j compatibility layers (Cypher parser, transactions, indexes), we create a unique decentralized graph database that enables:

1. **Zero-friction migration** from Neo4j
2. **Decentralized storage** with IPFS
3. **Semantic web integration** via JSON-LD
4. **Production-grade features** (ACID, indexes, constraints)
5. **AI-powered extraction** via GraphRAG

**Estimated Effort**: 8-10 weeks with 1-2 developers  
**Lines of Code**: ~15,000 new lines, ~3,000 refactored  
**Files Created**: 50+ new modules  
**Tests Added**: 650+ tests

**Next Steps**:
1. Review and approve this plan
2. Set up development branch
3. Begin Phase 1 implementation
4. Weekly progress reviews
