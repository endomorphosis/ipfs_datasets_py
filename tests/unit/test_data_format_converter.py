"""
Comprehensive tests for Universal Data Format Converter

Tests all format conversions with:
- Unit tests for each conversion path
- Fuzzing with synthetic data generation
- Round-trip conversion tests
- Edge cases and error handling
- Format-specific validation
"""

import json
import pytest
import tempfile
import os
import random
import string
from pathlib import Path
from datetime import datetime, timedelta
from typing import Any, Dict, List, Optional, Union
from unittest.mock import Mock, patch, MagicMock

# Import the converter
from ipfs_datasets_py.utils.data_format_converter import (
    UniversalDataConverter,
    get_converter,
    HAVE_ARROW,
    HAVE_PANDAS,
    HAVE_HF_DATASETS
)


class SyntheticDataGenerator:
    """
    Fuzzing-based synthetic data generator for robust testing.
    
    Generates diverse data structures with varying:
    - Data types (strings, numbers, booleans, nulls, nested structures)
    - Sizes (empty, small, medium, large datasets)
    - Complexity (flat, nested, deeply nested)
    - Edge cases (special characters, Unicode, very long strings)
    - Sequences (ordered, random, patterns)
    """
    
    def __init__(self, seed: Optional[int] = None):
        """Initialize generator with optional seed for reproducibility."""
        self.rng = random.Random(seed)
        self.unicode_ranges = [
            (0x0020, 0x007E),  # ASCII printable
            (0x00A0, 0x00FF),  # Latin-1 Supplement
            (0x0100, 0x017F),  # Latin Extended-A
            (0x4E00, 0x9FFF),  # CJK Unified Ideographs
            (0x1F600, 0x1F64F),  # Emoticons
        ]
    
    def generate_string(self, min_len: int = 0, max_len: int = 100, 
                       unicode: bool = False, special_chars: bool = False) -> str:
        """Generate random string with various characteristics."""
        length = self.rng.randint(min_len, max_len)
        
        if unicode:
            # Generate Unicode string from multiple ranges
            chars = []
            for _ in range(length):
                range_start, range_end = self.rng.choice(self.unicode_ranges)
                chars.append(chr(self.rng.randint(range_start, range_end)))
            return ''.join(chars)
        elif special_chars:
            # Include special/control characters
            charset = string.printable + '\n\r\t'
            return ''.join(self.rng.choices(charset, k=length))
        else:
            # Standard alphanumeric
            return ''.join(self.rng.choices(string.ascii_letters + string.digits, k=length))
    
    def generate_number(self, int_only: bool = False, allow_negative: bool = True) -> Union[int, float]:
        """Generate random number (integer or float)."""
        if int_only:
            max_val = self.rng.choice([10, 100, 1000, 1000000])
            num = self.rng.randint(0, max_val)
        else:
            num = self.rng.random() * self.rng.choice([1, 10, 100, 1000, 0.01])
        
        if allow_negative and self.rng.random() < 0.3:
            num = -num
        
        return int(num) if int_only else num
    
    def generate_datetime(self) -> str:
        """Generate random ISO datetime string."""
        start = datetime(2020, 1, 1)
        end = datetime(2025, 12, 31)
        delta = end - start
        random_days = self.rng.randint(0, delta.days)
        random_seconds = self.rng.randint(0, 86400)
        dt = start + timedelta(days=random_days, seconds=random_seconds)
        return dt.isoformat()
    
    def generate_primitive(self, allow_null: bool = True) -> Any:
        """Generate random primitive value."""
        choices = ['string', 'number', 'bool']
        if allow_null:
            choices.append('null')
        
        choice = self.rng.choice(choices)
        
        if choice == 'string':
            return self.generate_string(
                max_len=self.rng.choice([10, 50, 200]),
                unicode=self.rng.random() < 0.2,
                special_chars=self.rng.random() < 0.1
            )
        elif choice == 'number':
            return self.generate_number(
                int_only=self.rng.random() < 0.5
            )
        elif choice == 'bool':
            return self.rng.choice([True, False])
        else:  # null
            return None
    
    def generate_dict(self, depth: int = 0, max_depth: int = 3, 
                     min_keys: int = 1, max_keys: int = 10) -> Dict[str, Any]:
        """Generate random dictionary with nested structures."""
        num_keys = self.rng.randint(min_keys, max_keys)
        result = {}
        
        for _ in range(num_keys):
            key = self.generate_string(min_len=1, max_len=20)
            
            # Decide value type
            if depth < max_depth and self.rng.random() < 0.3:
                # Nested dict
                result[key] = self.generate_dict(depth + 1, max_depth, 1, 5)
            elif depth < max_depth and self.rng.random() < 0.2:
                # List
                result[key] = self.generate_list(depth + 1, max_depth, 1, 10)
            else:
                # Primitive
                result[key] = self.generate_primitive()
        
        return result
    
    def generate_list(self, depth: int = 0, max_depth: int = 3,
                     min_items: int = 0, max_items: int = 20) -> List[Any]:
        """Generate random list with various element types."""
        num_items = self.rng.randint(min_items, max_items)
        result = []
        
        for _ in range(num_items):
            if depth < max_depth and self.rng.random() < 0.2:
                # Nested dict
                result.append(self.generate_dict(depth + 1, max_depth, 1, 5))
            elif depth < max_depth and self.rng.random() < 0.1:
                # Nested list
                result.append(self.generate_list(depth + 1, max_depth, 0, 5))
            else:
                # Primitive
                result.append(self.generate_primitive())
        
        return result
    
    def generate_dataset(self, num_records: int = 100, 
                        schema_type: str = 'flat') -> List[Dict[str, Any]]:
        """
        Generate dataset with specified characteristics.
        
        Args:
            num_records: Number of records to generate
            schema_type: 'flat', 'nested', or 'mixed'
        """
        records = []
        
        if schema_type == 'flat':
            # Flat schema - all primitive values
            keys = [self.generate_string(5, 15) for _ in range(self.rng.randint(3, 10))]
            for _ in range(num_records):
                record = {key: self.generate_primitive(allow_null=True) for key in keys}
                records.append(record)
        
        elif schema_type == 'nested':
            # Nested schema - includes nested objects
            for _ in range(num_records):
                record = self.generate_dict(depth=0, max_depth=3, min_keys=3, max_keys=8)
                records.append(record)
        
        else:  # mixed
            # Mixed - varying structures
            for _ in range(num_records):
                if self.rng.random() < 0.7:
                    record = self.generate_dict(depth=0, max_depth=2, min_keys=2, max_keys=6)
                else:
                    record = self.generate_dict(depth=0, max_depth=4, min_keys=5, max_keys=12)
                records.append(record)
        
        return records
    
    def generate_edge_case_dataset(self) -> List[Dict[str, Any]]:
        """Generate dataset with edge cases."""
        return [
            {},  # Empty object
            {'key': None},  # Null value
            {'key': ''},  # Empty string
            {'key': 0},  # Zero
            {'key': -0.0},  # Negative zero
            {'key': float('inf')},  # Infinity (may need special handling)
            {'key': 'a' * 10000},  # Very long string
            {'unicode': '‰Ω†Â•Ω‰∏ñÁïåüåç'},  # Unicode
            {'special': 'line1\nline2\ttab'},  # Special characters
            {'nested': {'deep': {'deeper': {'deepest': 'value'}}}},  # Deep nesting
            {'list': [1, 2, 3, [4, 5, [6, 7]]]},  # Nested lists
            {'mixed': [1, 'two', None, {'four': 4}, [5]]},  # Mixed types
        ]


class TestSyntheticDataGenerator:
    """Test the synthetic data generator itself."""
    
    def test_generator_initialization(self):
        """
        GIVEN: A SyntheticDataGenerator class
        WHEN: Initializing with and without seed
        THEN: Generator should be created successfully
        """
        gen1 = SyntheticDataGenerator()
        assert gen1 is not None
        
        gen2 = SyntheticDataGenerator(seed=42)
        assert gen2 is not None
    
    def test_generate_string_basic(self):
        """
        GIVEN: A synthetic data generator
        WHEN: Generating basic strings
        THEN: Strings should be within specified length range
        """
        gen = SyntheticDataGenerator(seed=42)
        for _ in range(100):
            s = gen.generate_string(min_len=5, max_len=20)
            assert 5 <= len(s) <= 20
            assert isinstance(s, str)
    
    def test_generate_string_unicode(self):
        """
        GIVEN: A synthetic data generator
        WHEN: Generating Unicode strings
        THEN: Strings should contain Unicode characters
        """
        gen = SyntheticDataGenerator(seed=42)
        s = gen.generate_string(min_len=10, max_len=20, unicode=True)
        assert isinstance(s, str)
        assert len(s) >= 10
    
    def test_generate_number_types(self):
        """
        GIVEN: A synthetic data generator
        WHEN: Generating numbers
        THEN: Should generate both ints and floats as requested
        """
        gen = SyntheticDataGenerator(seed=42)
        
        # Integer only
        for _ in range(50):
            num = gen.generate_number(int_only=True)
            assert isinstance(num, int)
        
        # Float allowed
        floats_found = False
        for _ in range(50):
            num = gen.generate_number(int_only=False)
            if isinstance(num, float):
                floats_found = True
        assert floats_found
    
    def test_generate_primitive_types(self):
        """
        GIVEN: A synthetic data generator
        WHEN: Generating primitives
        THEN: Should generate strings, numbers, booleans, and nulls
        """
        gen = SyntheticDataGenerator(seed=42)
        types_found = set()
        
        for _ in range(100):
            val = gen.generate_primitive()
            types_found.add(type(val).__name__)
        
        # Should have variety
        assert len(types_found) >= 3
    
    def test_generate_dict_structure(self):
        """
        GIVEN: A synthetic data generator
        WHEN: Generating dictionaries
        THEN: Should create valid nested structures
        """
        gen = SyntheticDataGenerator(seed=42)
        d = gen.generate_dict(max_depth=3, min_keys=3, max_keys=10)
        
        assert isinstance(d, dict)
        assert 3 <= len(d) <= 10
        
        # Check for nested structures
        has_nested = any(isinstance(v, dict) for v in d.values())
        # With depth=3, we should sometimes get nesting
    
    def test_generate_list_structure(self):
        """
        GIVEN: A synthetic data generator
        WHEN: Generating lists
        THEN: Should create valid lists with various elements
        """
        gen = SyntheticDataGenerator(seed=42)
        lst = gen.generate_list(max_depth=2, min_items=5, max_items=15)
        
        assert isinstance(lst, list)
        assert 5 <= len(lst) <= 15
    
    def test_generate_dataset_flat(self):
        """
        GIVEN: A synthetic data generator
        WHEN: Generating flat dataset
        THEN: Should create records with consistent flat schema
        """
        gen = SyntheticDataGenerator(seed=42)
        dataset = gen.generate_dataset(num_records=50, schema_type='flat')
        
        assert len(dataset) == 50
        assert all(isinstance(record, dict) for record in dataset)
        
        # Check schema consistency
        if dataset:
            keys = set(dataset[0].keys())
            for record in dataset[1:10]:  # Check first few
                assert set(record.keys()) == keys
    
    def test_generate_dataset_nested(self):
        """
        GIVEN: A synthetic data generator
        WHEN: Generating nested dataset
        THEN: Should create records with nested structures
        """
        gen = SyntheticDataGenerator(seed=42)
        dataset = gen.generate_dataset(num_records=30, schema_type='nested')
        
        assert len(dataset) == 30
        
        # Look for nested structures
        has_nested = False
        for record in dataset:
            if any(isinstance(v, (dict, list)) for v in record.values()):
                has_nested = True
                break
        # With nested type, we should find some nesting
    
    def test_generate_edge_case_dataset(self):
        """
        GIVEN: A synthetic data generator
        WHEN: Generating edge case dataset
        THEN: Should create records with various edge cases
        """
        gen = SyntheticDataGenerator()
        dataset = gen.generate_edge_case_dataset()
        
        assert len(dataset) > 0
        assert isinstance(dataset, list)
        
        # Check for specific edge cases
        has_empty = any(len(record) == 0 for record in dataset)
        has_null = any(None in record.values() for record in dataset if record)
        
        assert has_empty
        assert has_null


class TestUniversalDataConverterBasic:
    """Basic tests for UniversalDataConverter."""
    
    def test_converter_initialization(self):
        """
        GIVEN: UniversalDataConverter class
        WHEN: Creating a new instance
        THEN: Converter should be initialized with supported formats
        """
        converter = UniversalDataConverter()
        assert converter is not None
        assert hasattr(converter, 'supported_formats')
        assert 'json' in converter.supported_formats
        assert 'jsonl' in converter.supported_formats
    
    def test_get_converter_singleton(self):
        """
        GIVEN: get_converter function
        WHEN: Calling multiple times
        THEN: Should return same instance (singleton pattern)
        """
        conv1 = get_converter()
        conv2 = get_converter()
        assert conv1 is conv2
    
    @pytest.mark.skip(reason="Method _detect_format_from_path not implemented in converter")
    def test_detect_format_from_extension(self):
        """
        GIVEN: File paths with various extensions
        WHEN: Detecting format from extension
        THEN: Should correctly identify format
        """
        converter = UniversalDataConverter()
        
        assert converter._detect_format_from_path('file.json') == 'json'
        assert converter._detect_format_from_path('file.jsonl') == 'jsonl'
        assert converter._detect_format_from_path('file.json-ld') == 'jsonld'
        assert converter._detect_format_from_path('file.parquet') == 'parquet'
        assert converter._detect_format_from_path('file.csv') == 'csv'


class TestJSONConversions:
    """Test JSON format conversions with fuzzing."""
    
    def test_json_to_jsonl_flat(self):
        """
        GIVEN: Flat JSON dataset
        WHEN: Converting to JSONL
        THEN: Each record should be on separate line
        """
        gen = SyntheticDataGenerator(seed=42)
        dataset = gen.generate_dataset(num_records=50, schema_type='flat')
        
        converter = UniversalDataConverter()
        
        with tempfile.TemporaryDirectory() as tmpdir:
            json_file = os.path.join(tmpdir, 'data.json')
            jsonl_file = os.path.join(tmpdir, 'data.jsonl')
            
            # Write JSON
            with open(json_file, 'w') as f:
                json.dump(dataset, f)
            
            # Convert
            converter.convert_file(json_file, jsonl_file, to_format='jsonl')
            
            # Verify
            assert os.path.exists(jsonl_file)
            with open(jsonl_file, 'r') as f:
                lines = f.readlines()
            assert len(lines) == 50
            
            # Parse each line
            for line in lines:
                record = json.loads(line)
                assert isinstance(record, dict)
    
    def test_json_to_jsonl_nested(self):
        """
        GIVEN: Nested JSON dataset
        WHEN: Converting to JSONL
        THEN: Should preserve nested structure
        """
        gen = SyntheticDataGenerator(seed=42)
        dataset = gen.generate_dataset(num_records=30, schema_type='nested')
        
        converter = UniversalDataConverter()
        
        with tempfile.TemporaryDirectory() as tmpdir:
            json_file = os.path.join(tmpdir, 'data.json')
            jsonl_file = os.path.join(tmpdir, 'data.jsonl')
            
            with open(json_file, 'w') as f:
                json.dump(dataset, f)
            
            converter.convert_file(json_file, jsonl_file, to_format='jsonl')
            
            with open(jsonl_file, 'r') as f:
                converted = [json.loads(line) for line in f]
            
            assert len(converted) == len(dataset)
            # Spot check first record
            assert converted[0].keys() == dataset[0].keys()
    
    def test_json_to_jsonl_edge_cases(self):
        """
        GIVEN: JSON with edge cases
        WHEN: Converting to JSONL
        THEN: Should handle edge cases correctly
        """
        gen = SyntheticDataGenerator()
        dataset = gen.generate_edge_case_dataset()
        
        converter = UniversalDataConverter()
        
        with tempfile.TemporaryDirectory() as tmpdir:
            json_file = os.path.join(tmpdir, 'edge.json')
            jsonl_file = os.path.join(tmpdir, 'edge.jsonl')
            
            # Filter out problematic infinity values for JSON
            filtered_dataset = []
            for record in dataset:
                filtered_record = {}
                for k, v in record.items():
                    if not isinstance(v, float) or not (v == float('inf') or v == float('-inf')):
                        filtered_record[k] = v
                if filtered_record or not record:  # Include empty dicts
                    filtered_dataset.append(filtered_record if filtered_record else record)
            
            with open(json_file, 'w') as f:
                json.dump(filtered_dataset, f)
            
            converter.convert_file(json_file, jsonl_file, to_format='jsonl')
            
            assert os.path.exists(jsonl_file)
    
    def test_jsonl_to_json_roundtrip(self):
        """
        GIVEN: JSONL file
        WHEN: Converting to JSON and back
        THEN: Data should be preserved
        """
        gen = SyntheticDataGenerator(seed=42)
        dataset = gen.generate_dataset(num_records=40, schema_type='mixed')
        
        converter = UniversalDataConverter()
        
        with tempfile.TemporaryDirectory() as tmpdir:
            jsonl_file = os.path.join(tmpdir, 'data.jsonl')
            json_file = os.path.join(tmpdir, 'data.json')
            jsonl_file2 = os.path.join(tmpdir, 'data2.jsonl')
            
            # Write JSONL
            with open(jsonl_file, 'w') as f:
                for record in dataset:
                    f.write(json.dumps(record) + '\n')
            
            # Convert JSONL -> JSON
            converter.convert_file(jsonl_file, json_file, to_format='json')
            
            # Convert JSON -> JSONL
            converter.convert_file(json_file, jsonl_file2, to_format='jsonl')
            
            # Verify
            with open(jsonl_file2, 'r') as f:
                converted = [json.loads(line) for line in f]
            
            assert len(converted) == len(dataset)


class TestJSONLDConversions:
    """Test JSON-LD format conversions."""
    
    def test_json_to_jsonld_basic(self):
        """
        GIVEN: JSON data
        WHEN: Converting to JSON-LD
        THEN: Should add @context and @id fields
        """
        gen = SyntheticDataGenerator(seed=42)
        dataset = gen.generate_dataset(num_records=20, schema_type='flat')
        
        converter = UniversalDataConverter()
        
        with tempfile.TemporaryDirectory() as tmpdir:
            json_file = os.path.join(tmpdir, 'data.json')
            jsonld_file = os.path.join(tmpdir, 'data.json-ld')
            
            with open(json_file, 'w') as f:
                json.dump(dataset, f)
            
            converter.convert_file(json_file, jsonld_file, to_format='jsonld')
            
            with open(jsonld_file, 'r') as f:
                result = json.load(f)
            
            # Check JSON-LD structure
            if isinstance(result, dict):
                # Single document with @graph
                assert '@context' in result
                if '@graph' in result:
                    assert isinstance(result['@graph'], list)
            elif isinstance(result, list):
                # List of items, each might have @context
                for item in result:
                    if isinstance(item, dict):
                        # May or may not have @id depending on implementation
                        pass
    
    def test_json_to_jsonld_logic(self):
        """
        GIVEN: JSON data
        WHEN: Converting to JSON-LD-LOGIC
        THEN: Should add logic annotations
        """
        gen = SyntheticDataGenerator(seed=42)
        dataset = gen.generate_dataset(num_records=15, schema_type='flat')
        
        converter = UniversalDataConverter()
        
        with tempfile.TemporaryDirectory() as tmpdir:
            json_file = os.path.join(tmpdir, 'data.json')
            logic_file = os.path.join(tmpdir, 'data.logic.json-ld')
            
            with open(json_file, 'w') as f:
                json.dump(dataset, f)
            
            converter.convert_file(json_file, logic_file, to_format='jsonld-logic')
            
            with open(logic_file, 'r') as f:
                result = json.load(f)
            
            # Check for logic annotations
            assert isinstance(result, (dict, list))
            # Structure depends on implementation


@pytest.mark.skipif(not HAVE_ARROW, reason="PyArrow not available")
class TestParquetConversions:
    """Test Parquet format conversions."""
    
    def test_json_to_parquet_flat(self):
        """
        GIVEN: Flat JSON dataset
        WHEN: Converting to Parquet
        THEN: Should create valid Parquet file
        """
        gen = SyntheticDataGenerator(seed=42)
        # Use only simple types for Parquet compatibility
        dataset = []
        for _ in range(50):
            dataset.append({
                'id': gen.generate_number(int_only=True, allow_negative=False),
                'name': gen.generate_string(5, 20),
                'value': gen.generate_number(int_only=False),
                'active': gen.rng.choice([True, False])
            })
        
        converter = UniversalDataConverter()
        
        with tempfile.TemporaryDirectory() as tmpdir:
            json_file = os.path.join(tmpdir, 'data.json')
            parquet_file = os.path.join(tmpdir, 'data.parquet')
            
            with open(json_file, 'w') as f:
                json.dump(dataset, f)
            
            converter.convert_file(json_file, parquet_file, to_format='parquet')
            
            assert os.path.exists(parquet_file)
            
            # Read back and verify
            import pyarrow.parquet as pq
            table = pq.read_table(parquet_file)
            assert table.num_rows == 50
    
    def test_parquet_to_json_roundtrip(self):
        """
        GIVEN: Parquet file
        WHEN: Converting to JSON and back to Parquet
        THEN: Data should be preserved
        """
        gen = SyntheticDataGenerator(seed=42)
        dataset = []
        for _ in range(30):
            dataset.append({
                'id': gen.generate_number(int_only=True, allow_negative=False),
                'text': gen.generate_string(5, 50),
                'score': gen.generate_number(int_only=False),
            })
        
        converter = UniversalDataConverter()
        
        with tempfile.TemporaryDirectory() as tmpdir:
            json_file = os.path.join(tmpdir, 'data.json')
            parquet_file = os.path.join(tmpdir, 'data.parquet')
            json_file2 = os.path.join(tmpdir, 'data2.json')
            
            # Write JSON
            with open(json_file, 'w') as f:
                json.dump(dataset, f)
            
            # JSON -> Parquet
            converter.convert_file(json_file, parquet_file, to_format='parquet')
            
            # Parquet -> JSON
            converter.convert_file(parquet_file, json_file2, to_format='json')
            
            # Verify
            with open(json_file2, 'r') as f:
                converted = json.load(f)
            
            assert len(converted) == len(dataset)
            # Check schema matches
            if converted and dataset:
                assert set(converted[0].keys()) == set(dataset[0].keys())


@pytest.mark.skipif(not HAVE_PANDAS, reason="Pandas not available")
class TestCSVConversions:
    """Test CSV format conversions."""
    
    def test_json_to_csv_flat(self):
        """
        GIVEN: Flat JSON dataset
        WHEN: Converting to CSV
        THEN: Should create valid CSV file
        """
        gen = SyntheticDataGenerator(seed=42)
        dataset = []
        for _ in range(40):
            dataset.append({
                'id': gen.generate_number(int_only=True, allow_negative=False),
                'name': gen.generate_string(5, 20, special_chars=False),
                'value': gen.generate_number(int_only=False),
            })
        
        converter = UniversalDataConverter()
        
        with tempfile.TemporaryDirectory() as tmpdir:
            json_file = os.path.join(tmpdir, 'data.json')
            csv_file = os.path.join(tmpdir, 'data.csv')
            
            with open(json_file, 'w') as f:
                json.dump(dataset, f)
            
            converter.convert_file(json_file, csv_file, to_format='csv')
            
            assert os.path.exists(csv_file)
            
            # Read back and verify
            import pandas as pd
            df = pd.read_csv(csv_file)
            assert len(df) == 40
    
    def test_csv_to_json_roundtrip(self):
        """
        GIVEN: CSV file
        WHEN: Converting to JSON and back to CSV
        THEN: Data should be preserved
        """
        gen = SyntheticDataGenerator(seed=42)
        dataset = []
        for _ in range(25):
            dataset.append({
                'id': gen.generate_number(int_only=True, allow_negative=False),
                'category': gen.generate_string(3, 10),
                'amount': round(gen.generate_number(int_only=False), 2),
            })
        
        converter = UniversalDataConverter()
        
        with tempfile.TemporaryDirectory() as tmpdir:
            json_file = os.path.join(tmpdir, 'data.json')
            csv_file = os.path.join(tmpdir, 'data.csv')
            json_file2 = os.path.join(tmpdir, 'data2.json')
            
            with open(json_file, 'w') as f:
                json.dump(dataset, f)
            
            # JSON -> CSV
            converter.convert_file(json_file, csv_file, to_format='csv')
            
            # CSV -> JSON
            converter.convert_file(csv_file, json_file2, to_format='json')
            
            with open(json_file2, 'r') as f:
                converted = json.load(f)
            
            assert len(converted) == len(dataset)


class TestConversionErrors:
    """Test error handling in conversions."""
    
    def test_invalid_input_file(self):
        """
        GIVEN: Non-existent input file
        WHEN: Attempting conversion
        THEN: Should raise FileNotFoundError
        """
        converter = UniversalDataConverter()
        
        with tempfile.TemporaryDirectory() as tmpdir:
            input_file = os.path.join(tmpdir, 'nonexistent.json')
            output_file = os.path.join(tmpdir, 'output.jsonl')
            
            with pytest.raises(FileNotFoundError):
                converter.convert_file(input_file, output_file, to_format='jsonl')
    
    def test_unsupported_format(self):
        """
        GIVEN: Unsupported output format
        WHEN: Attempting conversion
        THEN: Should raise ValueError
        """
        gen = SyntheticDataGenerator(seed=42)
        dataset = gen.generate_dataset(num_records=10, schema_type='flat')
        
        converter = UniversalDataConverter()
        
        with tempfile.TemporaryDirectory() as tmpdir:
            json_file = os.path.join(tmpdir, 'data.json')
            output_file = os.path.join(tmpdir, 'data.xyz')
            
            with open(json_file, 'w') as f:
                json.dump(dataset, f)
            
            with pytest.raises(ValueError):
                converter.convert_file(json_file, output_file, to_format='unsupported')
    
    def test_malformed_json(self):
        """
        GIVEN: Malformed JSON file
        WHEN: Attempting to read
        THEN: Should raise JSONDecodeError
        """
        converter = UniversalDataConverter()
        
        with tempfile.TemporaryDirectory() as tmpdir:
            json_file = os.path.join(tmpdir, 'bad.json')
            output_file = os.path.join(tmpdir, 'output.jsonl')
            
            # Write malformed JSON
            with open(json_file, 'w') as f:
                f.write('{"key": invalid}')
            
            with pytest.raises(json.JSONDecodeError):
                converter.convert_file(json_file, output_file, to_format='jsonl')


class TestFuzzingConversions:
    """Comprehensive fuzzing tests for all conversion paths."""
    
    @pytest.mark.parametrize("seed,num_records,schema_type", [
        (42, 50, 'flat'),
        (123, 100, 'nested'),
        (456, 75, 'mixed'),
        (789, 30, 'flat'),
    ])
    def test_fuzz_json_jsonl_conversions(self, seed, num_records, schema_type):
        """
        GIVEN: Randomly generated datasets with various characteristics
        WHEN: Converting between JSON and JSONL
        THEN: Conversions should succeed and preserve data
        """
        gen = SyntheticDataGenerator(seed=seed)
        dataset = gen.generate_dataset(num_records=num_records, schema_type=schema_type)
        
        converter = UniversalDataConverter()
        
        with tempfile.TemporaryDirectory() as tmpdir:
            json_file = os.path.join(tmpdir, 'data.json')
            jsonl_file = os.path.join(tmpdir, 'data.jsonl')
            json_file2 = os.path.join(tmpdir, 'data2.json')
            
            # Write original JSON
            with open(json_file, 'w') as f:
                json.dump(dataset, f)
            
            # JSON -> JSONL
            converter.convert_file(json_file, jsonl_file, to_format='jsonl')
            
            # JSONL -> JSON
            converter.convert_file(jsonl_file, json_file2, to_format='json')
            
            # Verify
            with open(json_file2, 'r') as f:
                converted = json.load(f)
            
            assert len(converted) == len(dataset)
    
    @pytest.mark.parametrize("seed", [42, 123, 456, 789, 999])
    def test_fuzz_edge_cases(self, seed):
        """
        GIVEN: Edge case datasets
        WHEN: Converting between formats
        THEN: Should handle edge cases gracefully
        """
        gen = SyntheticDataGenerator(seed=seed)
        dataset = gen.generate_edge_case_dataset()
        
        # Filter out problematic values
        safe_dataset = []
        for record in dataset:
            safe_record = {}
            for k, v in record.items():
                if isinstance(v, float) and (v == float('inf') or v == float('-inf')):
                    continue
                safe_record[k] = v
            if safe_record or not record:
                safe_dataset.append(safe_record if safe_record else record)
        
        converter = UniversalDataConverter()
        
        with tempfile.TemporaryDirectory() as tmpdir:
            json_file = os.path.join(tmpdir, 'edge.json')
            jsonl_file = os.path.join(tmpdir, 'edge.jsonl')
            
            with open(json_file, 'w') as f:
                json.dump(safe_dataset, f)
            
            # Should not raise
            converter.convert_file(json_file, jsonl_file, to_format='jsonl')
            assert os.path.exists(jsonl_file)


class TestBatchConversions:
    """Test batch conversion functionality."""
    
    @pytest.mark.skip(reason="Method batch_convert_directory not implemented in converter")
    def test_batch_convert_directory(self):
        """
        GIVEN: Directory with multiple JSON files
        WHEN: Batch converting to JSONL
        THEN: All files should be converted
        """
        gen = SyntheticDataGenerator(seed=42)
        
        with tempfile.TemporaryDirectory() as tmpdir:
            input_dir = os.path.join(tmpdir, 'input')
            output_dir = os.path.join(tmpdir, 'output')
            os.makedirs(input_dir)
            os.makedirs(output_dir)
            
            # Create multiple JSON files
            for i in range(5):
                dataset = gen.generate_dataset(num_records=20, schema_type='flat')
                json_file = os.path.join(input_dir, f'data{i}.json')
                with open(json_file, 'w') as f:
                    json.dump(dataset, f)
            
            # Batch convert
            converter = UniversalDataConverter()
            results = converter.batch_convert_directory(
                input_dir, output_dir, to_format='jsonl'
            )
            
            assert len(results) == 5
            for result in results:
                assert result['success']
                assert os.path.exists(result['output_path'])


# Summary test to validate all critical paths
class TestComprehensiveSuite:
    """Comprehensive test suite validating all major functionality."""
    
    def test_all_supported_formats_accessible(self):
        """
        GIVEN: UniversalDataConverter
        WHEN: Checking supported formats
        THEN: Should support all advertised formats
        """
        converter = UniversalDataConverter()
        
        required_formats = ['json', 'jsonl', 'jsonld', 'jsonld-logic']
        for fmt in required_formats:
            assert fmt in converter.supported_formats
        
        # Optional formats based on dependencies
        if HAVE_ARROW:
            assert 'parquet' in converter.supported_formats
        if HAVE_PANDAS:
            assert 'csv' in converter.supported_formats
    
    def test_synthetic_data_diversity(self):
        """
        GIVEN: SyntheticDataGenerator
        WHEN: Generating multiple datasets
        THEN: Should produce diverse data
        """
        gen = SyntheticDataGenerator(seed=42)
        
        datasets = [
            gen.generate_dataset(50, 'flat'),
            gen.generate_dataset(50, 'nested'),
            gen.generate_dataset(50, 'mixed'),
        ]
        
        # Check they're all different
        assert len(datasets) == 3
        for dataset in datasets:
            assert len(dataset) == 50
