# Neurosymbolic Architecture Implementation Plan

## Executive Summary

This document outlines the comprehensive plan to create a **true neurosymbolic architecture** for the ipfs_datasets_py project, integrating:

1. **Symbolic Logic Systems:** TDFOL (Temporal Deontic First-Order Logic) with 50+ inference rules
2. **Neural Components:** Embeddings, LLM-based reasoning, pattern matching
3. **Knowledge Graphs:** Logic-aware GraphRAG with theorem-augmented retrieval
4. **Theorem Provers:** CEC native prover (87 rules) + modal tableaux + TDFOL prover

**Timeline:** 12 weeks (3 months)  
**Status:** Phase 1 Complete ‚úÖ (Weeks 1-2)  
**Next:** Phase 2 (Weeks 3-4) üîÑ

---

## Architecture Overview

```
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ              Neurosymbolic Architecture                         ‚îÇ
‚îÇ                                                                 ‚îÇ
‚îÇ  ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê  ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê  ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê        ‚îÇ
‚îÇ  ‚îÇ   Symbolic   ‚îÇ  ‚îÇ    Neural    ‚îÇ  ‚îÇ  Knowledge   ‚îÇ        ‚îÇ
‚îÇ  ‚îÇ   Reasoning  ‚îÇ‚óÑ‚îÄ‚î§   Networks   ‚îÇ‚îÄ‚ñ∫‚îÇ    Graphs    ‚îÇ        ‚îÇ
‚îÇ  ‚îÇ  (TDFOL +    ‚îÇ  ‚îÇ (Embeddings, ‚îÇ  ‚îÇ  (GraphRAG   ‚îÇ        ‚îÇ
‚îÇ  ‚îÇ   CEC)       ‚îÇ  ‚îÇ   LLM)       ‚îÇ  ‚îÇ   + Logic)   ‚îÇ        ‚îÇ
‚îÇ  ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò  ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò  ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò        ‚îÇ
‚îÇ         ‚îÇ                 ‚îÇ                    ‚îÇ               ‚îÇ
‚îÇ         ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¥‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò               ‚îÇ
‚îÇ                           ‚îÇ                                    ‚îÇ
‚îÇ                ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚ñº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê                        ‚îÇ
‚îÇ                ‚îÇ  Reasoning Engine   ‚îÇ                        ‚îÇ
‚îÇ                ‚îÇ  ‚Ä¢ Hybrid search    ‚îÇ                        ‚îÇ
‚îÇ                ‚îÇ  ‚Ä¢ Proof + neural   ‚îÇ                        ‚îÇ
‚îÇ                ‚îÇ  ‚Ä¢ Consistency      ‚îÇ                        ‚îÇ
‚îÇ                ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò                        ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
```

### Key Principles

1. **Hybrid Reasoning:** Combine symbolic proofs with neural pattern matching
2. **Bidirectional Integration:** Logic ‚Üî Embeddings ‚Üî Knowledge Graphs
3. **Confidence Fusion:** Merge symbolic proof scores with neural confidence
4. **End-to-End Pipeline:** Text ‚Üí Logic ‚Üí Proof ‚Üí Knowledge Graph ‚Üí Answer

---

## Phase-by-Phase Breakdown

### ‚úÖ Phase 1: TDFOL Foundation (Weeks 1-2) - COMPLETE

**Deliverables:**
- [x] Unified TDFOL core module (542 LOC)
- [x] TDFOL parser (509 LOC)
- [x] TDFOL prover with 10+ rules (542 LOC)
- [x] TDFOL converters (414 LOC)
- [x] Basic tests and verification

**Files Created:**
1. `logic/TDFOL/tdfol_core.py` - Formula representation
2. `logic/TDFOL/tdfol_parser.py` - String ‚Üí AST parser
3. `logic/TDFOL/tdfol_prover.py` - Theorem prover
4. `logic/TDFOL/tdfol_converter.py` - Format converters
5. `logic/TDFOL/README.md` - Documentation

**Verification:**
```python
from ipfs_datasets_py.logic.TDFOL import parse_tdfol, create_obligation
formula = parse_tdfol("P(x)")
obligation = create_obligation(formula)
assert obligation.to_string() == "O(P(x))"
```

---

### üîÑ Phase 2: Enhanced Prover Integration (Weeks 3-4) - IN PROGRESS

**Goals:**
1. Add 15+ temporal-deontic inference rules
2. Implement modal logic axioms (K, T, D, S4, S5)  
3. Create proof caching and optimization
4. Add comprehensive test coverage (50+ tests)

**Files to Create/Extend:**
```
logic/TDFOL/
‚îú‚îÄ‚îÄ tdfol_inference_rules.py        # 15+ new rules
‚îú‚îÄ‚îÄ tdfol_modal_axioms.py           # K, T, D, S4, S5 axioms
‚îú‚îÄ‚îÄ tdfol_proof_cache.py            # Proof caching
‚îî‚îÄ‚îÄ tdfol_optimization.py           # Proof search optimization

tests/unit_tests/logic/TDFOL/
‚îú‚îÄ‚îÄ test_tdfol_inference_rules.py   # Rule tests
‚îú‚îÄ‚îÄ test_tdfol_modal_axioms.py      # Axiom tests
‚îú‚îÄ‚îÄ test_tdfol_prover_advanced.py   # Advanced proving
‚îî‚îÄ‚îÄ test_tdfol_performance.py       # Performance benchmarks
```

**Inference Rules to Implement:**

**Temporal Logic (8 rules):**
1. K axiom: ‚ñ°(œÜ ‚Üí œà) ‚Üí (‚ñ°œÜ ‚Üí ‚ñ°œà)
2. T axiom: ‚ñ°œÜ ‚Üí œÜ
3. S4 axiom: ‚ñ°œÜ ‚Üí ‚ñ°‚ñ°œÜ
4. S5 axiom: ‚óäœÜ ‚Üí ‚ñ°‚óäœÜ
5. Temporal induction: œÜ ‚àß ‚ñ°(œÜ ‚Üí XœÜ) ‚Üí ‚ñ°œÜ
6. Until induction: (œÜ U œà) ‚Üí œà ‚à® (œÜ ‚àß X(œÜ U œà))
7. Since dual: (œÜ S œà) ‚Üî œà ‚à® (œÜ ‚àß Y(œÜ S œà))
8. Eventually expansion: ‚óäœÜ ‚Üî œÜ ‚à® X‚óäœÜ

**Deontic Logic (7 rules):**
1. D axiom: O(œÜ) ‚Üí P(œÜ)
2. Distribution: O(œÜ ‚Üí œà) ‚Üí (O(œÜ) ‚Üí O(œà))
3. Prohibition equivalence: F(œÜ) ‚Üî O(¬¨œÜ)
4. Permission negation: P(œÜ) ‚Üî ¬¨O(¬¨œÜ)
5. Obligation consistency: O(œÜ) ‚Üí ¬¨O(¬¨œÜ)
6. Permission introduction: œÜ ‚Üí P(œÜ)
7. Conditional obligation: O(œÜ|œà) ‚Üí (œà ‚Üí O(œÜ))

**Combined Temporal-Deontic (5 rules):**
1. Temporal obligation persistence: O(‚ñ°œÜ) ‚Üí ‚ñ°O(œÜ)
2. Deontic temporal introduction: O(œÜ) ‚Üí O(XœÜ)
3. Until obligation: O(œÜ U œà) ‚Üí ‚óäO(œà)
4. Always permission: P(‚ñ°œÜ) ‚Üí ‚ñ°P(œÜ)
5. Eventually forbidden: F(‚óäœÜ) ‚Üí ‚ñ°F(œÜ)

**Integration with CEC:**
```python
from ipfs_datasets_py.logic.CEC.native.prover_core import InferenceEngine
from ipfs_datasets_py.logic.TDFOL import TDFOLProver

# Extend TDFOL prover with CEC rules
tdfol_prover = TDFOLProver()
tdfol_prover.add_cec_rules(InferenceEngine().get_rules())  # 87 rules
```

**Success Criteria:**
- ‚úÖ 25+ total inference rules (10 TDFOL + 15 new)
- ‚úÖ Modal axioms K, T, D, S4, S5 implemented
- ‚úÖ Proof caching reduces search time by 50%+
- ‚úÖ 50+ comprehensive tests passing
- ‚úÖ Integration with CEC prover verified

---

### üìã Phase 3: Neural-Symbolic Bridge (Weeks 5-6)

**Goals:**
1. Create neurosymbolic reasoning coordinator
2. Implement embedding-enhanced theorem retrieval
3. Add neural pattern matching for formula similarity
4. Create hybrid confidence scoring (symbolic + neural)
5. Implement neural-guided proof search

**Architecture:**
```
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ           Neurosymbolic Reasoning Coordinator           ‚îÇ
‚îÇ                                                         ‚îÇ
‚îÇ  Symbolic Input         Neural Processing              ‚îÇ
‚îÇ  (TDFOL Formula)        (Embeddings)                   ‚îÇ
‚îÇ       ‚îÇ                       ‚îÇ                        ‚îÇ
‚îÇ       ‚îú‚îÄ‚îÄ‚ñ∫ Formula Embedding ‚îÄ‚î§                        ‚îÇ
‚îÇ       ‚îÇ    (768-dim vector)   ‚îÇ                        ‚îÇ
‚îÇ       ‚îÇ                       ‚îÇ                        ‚îÇ
‚îÇ       ‚îú‚îÄ‚îÄ‚ñ∫ Similar Theorems ‚óÑ‚îÄ‚î§ (FAISS search)        ‚îÇ
‚îÇ       ‚îÇ    (Top-K retrieval)  ‚îÇ                        ‚îÇ
‚îÇ       ‚îÇ                       ‚îÇ                        ‚îÇ
‚îÇ       ‚îú‚îÄ‚îÄ‚ñ∫ Neural Confidence ‚îÄ‚î§ (NN classifier)       ‚îÇ
‚îÇ       ‚îÇ    (0.0 - 1.0)        ‚îÇ                        ‚îÇ
‚îÇ       ‚îÇ                       ‚îÇ                        ‚îÇ
‚îÇ       ‚îî‚îÄ‚îÄ‚ñ∫ Hybrid Decision ‚óÑ‚îÄ‚îÄ‚îò                        ‚îÇ
‚îÇ            (Symbolic + Neural)                         ‚îÇ
‚îÇ                                                         ‚îÇ
‚îÇ  Output: Proof + Confidence + Evidence                 ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
```

**Files to Create:**
```
logic/neurosymbolic/
‚îú‚îÄ‚îÄ __init__.py
‚îú‚îÄ‚îÄ reasoning_coordinator.py       # Main coordinator (500 LOC)
‚îú‚îÄ‚îÄ neural_guided_search.py        # Neural-guided proving (400 LOC)
‚îú‚îÄ‚îÄ embedding_prover.py            # Embedding retrieval (300 LOC)
‚îú‚îÄ‚îÄ hybrid_confidence.py           # Confidence fusion (200 LOC)
‚îú‚îÄ‚îÄ formula_embedder.py            # Formula ‚Üí embedding (300 LOC)
‚îî‚îÄ‚îÄ pattern_matcher.py             # Neural pattern matching (250 LOC)

tests/unit_tests/logic/neurosymbolic/
‚îú‚îÄ‚îÄ test_reasoning_coordinator.py  # 20+ tests
‚îú‚îÄ‚îÄ test_neural_guided_search.py   # 15+ tests
‚îú‚îÄ‚îÄ test_embedding_prover.py       # 15+ tests
‚îî‚îÄ‚îÄ test_hybrid_confidence.py      # 10+ tests
```

**Key Components:**

**1. Formula Embedder:**
```python
class FormulaEmbedder:
    """Convert TDFOL formulas to embeddings."""
    
    def embed(self, formula: Formula) -> np.ndarray:
        """
        Embed formula into 768-dimensional space.
        
        Strategy:
        1. Convert formula to string representation
        2. Extract structural features (depth, operators, predicates)
        3. Use pre-trained model (e.g., Sentence-BERT)
        4. Combine linguistic and structural embeddings
        """
        # Linguistic embedding (80% weight)
        text = formula.to_string(pretty=True)
        linguistic_emb = self.encoder.encode(text)
        
        # Structural embedding (20% weight)
        structural_emb = self._structural_features(formula)
        
        # Weighted fusion
        return 0.8 * linguistic_emb + 0.2 * structural_emb
```

**2. Neural-Guided Search:**
```python
class NeuralGuidedSearch:
    """Guide proof search using neural networks."""
    
    def select_next_rule(
        self, 
        current_state: Formula,
        available_rules: List[InferenceRule]
    ) -> InferenceRule:
        """
        Select most promising inference rule.
        
        Strategy:
        1. Embed current formula state
        2. Embed each rule's pattern
        3. Compute similarity scores
        4. Return highest-scoring rule
        """
        state_emb = self.embedder.embed(current_state)
        
        scores = []
        for rule in available_rules:
            rule_emb = self.embedder.embed_rule(rule)
            similarity = cosine_similarity(state_emb, rule_emb)
            scores.append(similarity)
        
        best_idx = np.argmax(scores)
        return available_rules[best_idx]
```

**3. Hybrid Confidence:**
```python
class HybridConfidence:
    """Combine symbolic and neural confidence."""
    
    def score(
        self, 
        proof: ProofResult,
        formula: Formula
    ) -> float:
        """
        Compute hybrid confidence score.
        
        Combines:
        - Symbolic: proof length, rule quality
        - Neural: embedding similarity, pattern match
        """
        # Symbolic confidence (60% weight)
        symbolic = self._symbolic_confidence(proof)
        
        # Neural confidence (40% weight)
        neural = self._neural_confidence(formula, proof)
        
        # Weighted fusion
        return 0.6 * symbolic + 0.4 * neural
```

**Success Criteria:**
- ‚úÖ Formula embeddings capture semantic similarity
- ‚úÖ Neural-guided search reduces proof time by 30%+
- ‚úÖ Hybrid confidence correlates 0.85+ with human judgment
- ‚úÖ 60+ comprehensive tests passing
- ‚úÖ Integration with TDFOL prover verified

---

### üìã Phase 4: GraphRAG Integration (Weeks 7-8)

**Goals:**
1. Extend GraphRAG with logic-aware graph construction
2. Add entity extraction with logical type annotations
3. Implement theorem-augmented knowledge graph
4. Create logical consistency checking for graph edges
5. Add temporal reasoning over knowledge graphs

**Architecture:**
```
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ          Logic-Aware Knowledge Graph                    ‚îÇ
‚îÇ                                                         ‚îÇ
‚îÇ  Entities (typed)     Relations (verified)             ‚îÇ
‚îÇ  ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê        ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê                      ‚îÇ
‚îÇ  ‚îÇ Person   ‚îÇ        ‚îÇ employs  ‚îÇ ‚úì Consistent        ‚îÇ
‚îÇ  ‚îÇ (Agent)  ‚îÇ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚ñ∫‚îÇ (Action) ‚îÇ                      ‚îÇ
‚îÇ  ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò        ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò                      ‚îÇ
‚îÇ       ‚îÇ                                                 ‚îÇ
‚îÇ       ‚îÇ O(PayTax)    ‚Üê Theorem attached                ‚îÇ
‚îÇ       ‚îÇ                                                 ‚îÇ
‚îÇ  ‚îå‚îÄ‚îÄ‚îÄ‚ñº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê        ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê                      ‚îÇ
‚îÇ  ‚îÇ PayTax   ‚îÇ        ‚îÇ temporal ‚îÇ ‚úì Time-aware        ‚îÇ
‚îÇ  ‚îÇ (Action) ‚îÇ‚óÑ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÇ (always) ‚îÇ                      ‚îÇ
‚îÇ  ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò        ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò                      ‚îÇ
‚îÇ                                                         ‚îÇ
‚îÇ  + Logical consistency checking                        ‚îÇ
‚îÇ  + Theorem-based edge validation                       ‚îÇ
‚îÇ  + Temporal reasoning                                  ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
```

**Files to Create:**
```
graphrag/logic_integration/
‚îú‚îÄ‚îÄ __init__.py
‚îú‚îÄ‚îÄ logic_aware_graph.py           # Logic KG (600 LOC)
‚îú‚îÄ‚îÄ theorem_augmented_rag.py       # RAG + theorems (500 LOC)
‚îú‚îÄ‚îÄ temporal_graph_reasoning.py    # Temporal reasoning (400 LOC)
‚îú‚îÄ‚îÄ consistency_checker.py         # Consistency check (300 LOC)
‚îú‚îÄ‚îÄ entity_type_annotator.py       # Type annotation (200 LOC)
‚îî‚îÄ‚îÄ logical_query_engine.py        # Query with logic (400 LOC)

tests/unit_tests/graphrag/logic_integration/
‚îú‚îÄ‚îÄ test_logic_aware_graph.py      # 20+ tests
‚îú‚îÄ‚îÄ test_theorem_augmented_rag.py  # 15+ tests
‚îú‚îÄ‚îÄ test_temporal_graph_reasoning.py # 15+ tests
‚îî‚îÄ‚îÄ test_consistency_checker.py    # 10+ tests
```

**Key Components:**

**1. Logic-Aware Graph:**
```python
class LogicAwareKnowledgeGraph(KnowledgeGraph):
    """Knowledge graph with logical annotations."""
    
    def add_entity(
        self, 
        entity: str,
        entity_type: Sort,
        properties: Dict[str, Formula]
    ):
        """Add entity with logical type and properties."""
        # Type checking using TDFOL Sort system
        self.validate_type(entity, entity_type)
        
        # Attach logical properties
        for prop_name, formula in properties.items():
            self.add_property(entity, prop_name, formula)
    
    def add_relation(
        self,
        source: str,
        relation: str,
        target: str,
        theorem: Optional[Formula] = None
    ):
        """Add relation with optional theorem justification."""
        # Check consistency with existing theorems
        if not self.is_consistent(source, relation, target):
            raise InconsistencyError(...)
        
        # Attach theorem if provided
        if theorem:
            self.attach_theorem(source, relation, target, theorem)
```

**2. Theorem-Augmented RAG:**
```python
class TheoremAugmentedRAG:
    """RAG system enhanced with theorem proving."""
    
    def retrieve(
        self,
        query: str,
        top_k: int = 5,
        logical_reasoning: bool = True
    ) -> List[RetrievalResult]:
        """
        Retrieve documents with logical reasoning.
        
        Process:
        1. Parse query to TDFOL formula
        2. Retrieve similar theorems from KB
        3. Prove relevant implications
        4. Retrieve documents matching theorems
        5. Rank by relevance + logical confidence
        """
        # Parse query
        query_formula = self.parse_query(query)
        
        # Retrieve theorems
        theorems = self.kb.retrieve_similar(query_formula, top_k=20)
        
        # Prove implications
        relevant_theorems = []
        for theorem in theorems:
            if self.prover.prove_implies(query_formula, theorem):
                relevant_theorems.append(theorem)
        
        # Retrieve documents
        documents = self.doc_store.retrieve_by_theorems(
            relevant_theorems, top_k=top_k
        )
        
        return documents
```

**3. Temporal Graph Reasoning:**
```python
class TemporalGraphReasoner:
    """Reason about temporal properties in knowledge graphs."""
    
    def query_temporal(
        self,
        subject: str,
        relation: str,
        time_constraint: TemporalFormula
    ) -> List[Tuple[str, datetime]]:
        """
        Query graph with temporal constraints.
        
        Examples:
        - "Who was employed at time t?"
        - "What obligations were active in 2020?"
        - "Which permissions eventually expired?"
        """
        # Convert temporal formula to graph query
        query = self._temporal_to_query(time_constraint)
        
        # Execute on temporal graph
        results = self.graph.query_temporal(subject, relation, query)
        
        # Filter by temporal logic
        filtered = []
        for entity, timestamp in results:
            if self._satisfies_temporal(entity, timestamp, time_constraint):
                filtered.append((entity, timestamp))
        
        return filtered
```

**Success Criteria:**
- ‚úÖ Knowledge graph supports logical type annotations
- ‚úÖ Theorem-augmented RAG improves precision by 20%+
- ‚úÖ Temporal reasoning handles ‚ñ°, ‚óä, U operators
- ‚úÖ Consistency checker detects logical conflicts
- ‚úÖ 60+ comprehensive tests passing

---

### üìã Phase 5: End-to-End Pipeline (Weeks 9-10)

**Goals:**
1. Create unified NeurosymbolicGraphRAG class
2. Implement text ‚Üí TDFOL ‚Üí proof ‚Üí knowledge graph pipeline
3. Add interactive query interface with logical reasoning
4. Create visualization for proof trees + knowledge graphs
5. Add comprehensive examples and tutorials

**Architecture:**
```
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ       Neurosymbolic GraphRAG Pipeline                   ‚îÇ
‚îÇ                                                         ‚îÇ
‚îÇ  Input: Natural Language Query                         ‚îÇ
‚îÇ          ‚Üì                                              ‚îÇ
‚îÇ  Step 1: Parse to TDFOL                                ‚îÇ
‚îÇ          ‚Üì                                              ‚îÇ
‚îÇ  Step 2: Neural Embedding                              ‚îÇ
‚îÇ          ‚Üì                                              ‚îÇ
‚îÇ  Step 3: Retrieve Similar Theorems (Hybrid)           ‚îÇ
‚îÇ          ‚Üì                                              ‚îÇ
‚îÇ  Step 4: Theorem Proving (Symbolic)                    ‚îÇ
‚îÇ          ‚Üì                                              ‚îÇ
‚îÇ  Step 5: Knowledge Graph Query (Logic-Aware)          ‚îÇ
‚îÇ          ‚Üì                                              ‚îÇ
‚îÇ  Step 6: Generate Answer (Neural + Symbolic)          ‚îÇ
‚îÇ          ‚Üì                                              ‚îÇ
‚îÇ  Output: Answer + Proof + Evidence + Visualization    ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
```

**Files to Create:**
```
logic/integration/neurosymbolic_graphrag/
‚îú‚îÄ‚îÄ __init__.py
‚îú‚îÄ‚îÄ neurosymbolic_graphrag.py      # Main system (800 LOC)
‚îú‚îÄ‚îÄ query_engine.py                # Query interface (500 LOC)
‚îú‚îÄ‚îÄ pipeline.py                    # Processing pipeline (600 LOC)
‚îú‚îÄ‚îÄ visualizer.py                  # Visualization (400 LOC)
‚îú‚îÄ‚îÄ answer_generator.py            # Answer generation (300 LOC)
‚îî‚îÄ‚îÄ interactive_interface.py       # CLI/Web interface (400 LOC)

examples/neurosymbolic/
‚îú‚îÄ‚îÄ legal_reasoning_example.py     # Legal domain
‚îú‚îÄ‚îÄ medical_reasoning_example.py   # Medical domain
‚îú‚îÄ‚îÄ autonomous_systems_example.py  # Robotics
‚îî‚îÄ‚îÄ tutorial_notebook.ipynb        # Jupyter tutorial

tests/integration/neurosymbolic/
‚îú‚îÄ‚îÄ test_end_to_end_pipeline.py    # 20+ tests
‚îú‚îÄ‚îÄ test_query_engine.py           # 15+ tests
‚îî‚îÄ‚îÄ test_visualization.py          # 10+ tests
```

**Key Components:**

**1. Unified NeurosymbolicGraphRAG:**
```python
class NeurosymbolicGraphRAG:
    """Complete neurosymbolic reasoning system."""
    
    def __init__(
        self,
        tdfol_prover: TDFOLProver,
        knowledge_graph: LogicAwareKnowledgeGraph,
        embedder: FormulaEmbedder,
        reasoning_coordinator: ReasoningCoordinator
    ):
        self.prover = tdfol_prover
        self.kg = knowledge_graph
        self.embedder = embedder
        self.coordinator = reasoning_coordinator
    
    def query(
        self,
        question: str,
        reasoning_depth: str = "moderate"  # shallow, moderate, deep
    ) -> AnswerResult:
        """
        Answer question using neurosymbolic reasoning.
        
        Process:
        1. Parse question ‚Üí TDFOL
        2. Retrieve similar theorems (neural)
        3. Prove relevant theorems (symbolic)
        4. Query knowledge graph (hybrid)
        5. Generate answer (neural + symbolic)
        6. Provide proof + evidence
        """
        # Step 1: Parse
        query_formula = self.parse_question(question)
        
        # Step 2: Neural retrieval
        similar_theorems = self.retrieve_similar(query_formula, top_k=20)
        
        # Step 3: Symbolic proving
        proved_theorems = []
        for theorem in similar_theorems:
            proof = self.prover.prove(theorem)
            if proof.is_proved():
                proved_theorems.append((theorem, proof))
        
        # Step 4: Graph query
        graph_results = self.kg.query_with_theorems(
            query_formula, proved_theorems
        )
        
        # Step 5: Generate answer
        answer = self.coordinator.generate_answer(
            question, proved_theorems, graph_results
        )
        
        return AnswerResult(
            answer=answer.text,
            confidence=answer.confidence,
            proof_trees=[p for _, p in proved_theorems],
            evidence=graph_results,
            reasoning_trace=answer.trace
        )
```

**2. Interactive Query Interface:**
```python
class InteractiveInterface:
    """Interactive CLI/Web interface."""
    
    def run_cli(self):
        """Run command-line interface."""
        print("Neurosymbolic GraphRAG Query System")
        print("=" * 50)
        
        while True:
            query = input("\nQuery> ")
            if query.lower() in ['quit', 'exit']:
                break
            
            # Process query
            result = self.system.query(query)
            
            # Display results
            print(f"\nAnswer: {result.answer}")
            print(f"Confidence: {result.confidence:.2f}")
            print(f"\nProof Steps:")
            for i, step in enumerate(result.proof_trees[0].steps):
                print(f"  {i+1}. {step.justification}")
            
            # Option to visualize
            if input("\nVisualize proof tree? (y/n): ").lower() == 'y':
                self.visualizer.show_proof_tree(result.proof_trees[0])
```

**3. Visualization:**
```python
class ReasoningVisualizer:
    """Visualize proofs and knowledge graphs."""
    
    def visualize_proof_tree(
        self,
        proof: ProofResult,
        format: str = "mermaid"  # mermaid, graphviz, json
    ) -> str:
        """
        Generate proof tree visualization.
        
        Example output (Mermaid):
        ```mermaid
        graph TD
        A[Goal: Q] --> B[Modus Ponens]
        B --> C[Premise: P]
        B --> D[Premise: P‚ÜíQ]
        C --> E[Axiom 1]
        D --> F[Axiom 2]
        ```
        """
        if format == "mermaid":
            return self._generate_mermaid(proof)
        elif format == "graphviz":
            return self._generate_graphviz(proof)
        else:
            return json.dumps(proof.to_dict(), indent=2)
    
    def visualize_knowledge_graph(
        self,
        kg: LogicAwareKnowledgeGraph,
        highlight_entities: List[str] = None
    ) -> str:
        """Generate knowledge graph visualization."""
        # ... implementation ...
```

**Success Criteria:**
- ‚úÖ End-to-end pipeline processes queries in <2 seconds
- ‚úÖ Interactive interface provides real-time feedback
- ‚úÖ Visualizations are clear and informative
- ‚úÖ 5+ comprehensive examples for different domains
- ‚úÖ Tutorial covers all major features
- ‚úÖ 45+ integration tests passing

---

### üìã Phase 6: Testing & Documentation (Weeks 11-12)

**Goals:**
1. Add 100+ tests for TDFOL module
2. Add 50+ tests for neurosymbolic integration
3. Add 30+ tests for GraphRAG logic integration
4. Create comprehensive API documentation
5. Add usage examples and tutorials
6. Performance benchmarking and optimization

**Deliverables:**

**1. Test Suite:**
```
tests/
‚îú‚îÄ‚îÄ unit_tests/logic/TDFOL/                  # 100+ tests
‚îÇ   ‚îú‚îÄ‚îÄ test_tdfol_core.py
‚îÇ   ‚îú‚îÄ‚îÄ test_tdfol_parser.py
‚îÇ   ‚îú‚îÄ‚îÄ test_tdfol_prover.py
‚îÇ   ‚îú‚îÄ‚îÄ test_tdfol_converter.py
‚îÇ   ‚îú‚îÄ‚îÄ test_tdfol_inference_rules.py
‚îÇ   ‚îú‚îÄ‚îÄ test_tdfol_modal_axioms.py
‚îÇ   ‚îî‚îÄ‚îÄ test_tdfol_performance.py
‚îÇ
‚îú‚îÄ‚îÄ unit_tests/logic/neurosymbolic/          # 50+ tests
‚îÇ   ‚îú‚îÄ‚îÄ test_reasoning_coordinator.py
‚îÇ   ‚îú‚îÄ‚îÄ test_neural_guided_search.py
‚îÇ   ‚îú‚îÄ‚îÄ test_embedding_prover.py
‚îÇ   ‚îú‚îÄ‚îÄ test_hybrid_confidence.py
‚îÇ   ‚îî‚îÄ‚îÄ test_formula_embedder.py
‚îÇ
‚îú‚îÄ‚îÄ unit_tests/graphrag/logic_integration/   # 30+ tests
‚îÇ   ‚îú‚îÄ‚îÄ test_logic_aware_graph.py
‚îÇ   ‚îú‚îÄ‚îÄ test_theorem_augmented_rag.py
‚îÇ   ‚îî‚îÄ‚îÄ test_temporal_graph_reasoning.py
‚îÇ
‚îî‚îÄ‚îÄ integration/neurosymbolic/               # 50+ tests
    ‚îú‚îÄ‚îÄ test_end_to_end_pipeline.py
    ‚îú‚îÄ‚îÄ test_query_engine.py
    ‚îú‚îÄ‚îÄ test_legal_reasoning.py
    ‚îú‚îÄ‚îÄ test_medical_reasoning.py
    ‚îî‚îÄ‚îÄ test_performance_benchmarks.py
```

**2. Documentation:**
```
docs/neurosymbolic/
‚îú‚îÄ‚îÄ ARCHITECTURE.md           # Architecture overview
‚îú‚îÄ‚îÄ API_REFERENCE.md          # Complete API docs
‚îú‚îÄ‚îÄ TUTORIAL.md               # Step-by-step tutorial
‚îú‚îÄ‚îÄ EXAMPLES.md               # Usage examples
‚îú‚îÄ‚îÄ BENCHMARKS.md             # Performance results
‚îú‚îÄ‚îÄ TROUBLESHOOTING.md        # Common issues
‚îî‚îÄ‚îÄ ROADMAP.md                # Future plans
```

**3. Performance Benchmarks:**
```python
# Target Performance Metrics

# Formula Operations
- Creation: <0.01ms
- Parsing: <5ms (typical), <20ms (complex)
- Conversion: <1ms

# Theorem Proving
- Simple (axiom lookup): <1ms
- Medium (5-10 steps): <50ms
- Complex (20+ steps): <500ms
- With neural guidance: 30% faster

# Knowledge Graph
- Entity add: <1ms
- Relation add with check: <10ms
- Temporal query (1000 nodes): <100ms
- Consistency check (100 theorems): <200ms

# End-to-End Query
- Parse + embed: <10ms
- Retrieve theorems: <50ms
- Prove (3 theorems): <150ms
- Graph query: <100ms
- Generate answer: <50ms
- Total: <400ms (target: <500ms)

# Memory Usage
- Single formula: ~200 bytes
- Knowledge base (1000 formulas): ~200KB
- Embeddings (1000 formulas): ~3MB
- Knowledge graph (1000 nodes): ~5MB
- Total system: <50MB (target: <100MB)
```

**Success Criteria:**
- ‚úÖ 230+ total tests passing
- ‚úÖ >85% code coverage
- ‚úÖ All performance targets met
- ‚úÖ API documentation complete
- ‚úÖ Tutorial covers all major workflows
- ‚úÖ Zero critical bugs
- ‚úÖ Ready for production use

---

## Integration Points

### 1. CEC (Cognitive Event Calculus)

**Current:**
- CEC native prover: 87 inference rules
- Modal tableaux: K, S4, S5 support
- DCEC parsing and namespace management

**TDFOL Integration:**
- TDFOL prover uses CEC inference rules
- Bidirectional TDFOL ‚Üî DCEC conversion
- Modal axioms extend CEC modal tableaux

**Example:**
```python
from ipfs_datasets_py.logic.CEC.native import InferenceEngine
from ipfs_datasets_py.logic.TDFOL import TDFOLProver

# Create unified prover
prover = TDFOLProver()
prover.add_cec_rules(InferenceEngine().get_rules())

# Prove with combined rules (87 CEC + 25 TDFOL = 112 total)
result = prover.prove(goal)
```

### 2. GraphRAG

**Current:**
- Vector-based retrieval with FAISS
- Hybrid vector-graph search (60:40)
- Cross-document reasoning
- Knowledge graph construction

**TDFOL Integration:**
- Logic-aware graph construction
- Theorem-augmented retrieval
- Consistency checking with theorems
- Temporal reasoning over graphs

**Example:**
```python
from ipfs_datasets_py.graphrag.integrations import GraphRAGQueryEngine
from ipfs_datasets_py.logic.TDFOL import TDFOLProver

# Create logic-enhanced GraphRAG
engine = GraphRAGQueryEngine(
    logic_prover=TDFOLProver(),
    enable_logical_consistency=True,
    enable_temporal_reasoning=True
)

# Query with logical reasoning
result = engine.query(
    "What legal obligations apply to data processing?",
    logical_reasoning=True,
    temporal_scope=(start_date, end_date)
)
```

### 3. FOL and Deontic Modules

**Current:**
- `logic/fol/text_to_fol.py` - Text ‚Üí FOL conversion
- `logic/deontic/legal_text_to_deontic.py` - Legal text ‚Üí deontic
- Separate processing pipelines

**TDFOL Integration:**
- Unified TDFOL representation
- Single parser for all three logics
- Converters maintain compatibility

**Example:**
```python
from ipfs_datasets_py.logic.fol import convert_text_to_fol
from ipfs_datasets_py.logic.deontic import convert_legal_text_to_deontic
from ipfs_datasets_py.logic.TDFOL import parse_tdfol, tdfol_to_fol

# Legacy approach (separate)
fol_result = await convert_text_to_fol("All humans are mortal")
deontic_result = await convert_legal_text_to_deontic("Must pay tax")

# TDFOL approach (unified)
tdfol_formula = parse_tdfol("forall x. Human(x) -> O(PayTax(x))")
fol_formula = tdfol_to_fol(tdfol_formula)  # Extract FOL part
```

---

## Success Metrics

### Functional Metrics

**Phase 2:**
- ‚úÖ 25+ inference rules implemented
- ‚úÖ Modal axioms K, T, D, S4, S5 working
- ‚úÖ 50+ tests passing
- ‚úÖ Proof caching reduces time by 50%+

**Phase 3:**
- ‚úÖ Formula embeddings capture semantics
- ‚úÖ Neural guidance improves speed by 30%+
- ‚úÖ Hybrid confidence correlates 0.85+ with humans
- ‚úÖ 60+ tests passing

**Phase 4:**
- ‚úÖ Logic-aware KG supports type annotations
- ‚úÖ Theorem-augmented RAG improves precision by 20%+
- ‚úÖ Temporal reasoning handles ‚ñ°, ‚óä, U
- ‚úÖ 60+ tests passing

**Phase 5:**
- ‚úÖ End-to-end query processing <2 seconds
- ‚úÖ Interactive interface functional
- ‚úÖ 5+ domain examples working
- ‚úÖ 45+ integration tests passing

**Phase 6:**
- ‚úÖ 230+ total tests passing
- ‚úÖ >85% code coverage
- ‚úÖ All documentation complete
- ‚úÖ Performance targets met

### Performance Metrics

**Latency:**
- Simple query: <500ms
- Medium query: <2 seconds
- Complex query: <5 seconds

**Throughput:**
- 10 queries/second (simple)
- 2 queries/second (complex)

**Memory:**
- System footprint: <100MB
- Per-query overhead: <10MB

**Accuracy:**
- Theorem proving: >95% correct
- Neural guidance: >80% useful
- Hybrid confidence: 0.85+ correlation

---

## Risk Mitigation

### Technical Risks

**Risk 1: CEC Integration Complexity**
- *Mitigation:* Start with wrapper API, gradual integration
- *Fallback:* Use TDFOL prover standalone if needed

**Risk 2: Neural Component Performance**
- *Mitigation:* Caching, batch processing, model optimization
- *Fallback:* Symbolic-only mode without neural components

**Risk 3: GraphRAG Scalability**
- *Mitigation:* Incremental indexing, distributed graph store
- *Fallback:* Simplified graph with pruning

**Risk 4: Testing Coverage**
- *Mitigation:* Automated test generation, property-based testing
- *Fallback:* Focus on critical paths first

### Timeline Risks

**Risk: Phase Overrun**
- *Mitigation:* Weekly checkpoints, scope adjustment if needed
- *Fallback:* Defer non-critical features to future phases

**Risk: Integration Issues**
- *Mitigation:* Early integration testing, modular design
- *Fallback:* Fallback to standalone components

---

## Dependencies

### System Requirements

**Python:**
- Python 3.12+ (required)
- Type hints support

**Core Dependencies:**
- No external dependencies for TDFOL core
- CEC native prover (included)

**Optional Dependencies:**
- NumPy (for neural components)
- Sentence-Transformers (for embeddings)
- FAISS (for vector search)
- Transformers (for LLM integration)

**Development Dependencies:**
- pytest (testing)
- mypy (type checking)
- black (code formatting)

### External Systems

**Optional:**
- SPASS (automated theorem prover)
- TPTP library (test problems)
- Hugging Face models (embeddings)

---

## Timeline Summary

| Phase | Weeks | Status | Deliverables |
|-------|-------|--------|--------------|
| **1** | 1-2 | ‚úÖ Complete | TDFOL core, parser, prover, converter (2,007 LOC) |
| **2** | 3-4 | üîÑ Next | 25+ inference rules, modal axioms, tests (1,500 LOC) |
| **3** | 5-6 | üìã Planned | Neurosymbolic bridge, neural guidance (2,000 LOC) |
| **4** | 7-8 | üìã Planned | GraphRAG integration, logic-aware KG (2,400 LOC) |
| **5** | 9-10 | üìã Planned | End-to-end pipeline, examples (3,000 LOC) |
| **6** | 11-12 | üìã Planned | Testing, documentation, optimization (230+ tests) |

**Total:** 12 weeks, ~11,000 LOC production code, 230+ tests

---

## Conclusion

This plan provides a **comprehensive roadmap** for building a true neurosymbolic architecture that combines:

- ‚úÖ **Symbolic Logic:** TDFOL with 50+ inference rules
- ‚úÖ **Neural Networks:** Embeddings, LLM-based reasoning
- ‚úÖ **Knowledge Graphs:** Logic-aware GraphRAG
- ‚úÖ **Theorem Provers:** CEC + TDFOL + modal tableaux

**Current Status:** Phase 1 Complete (Weeks 1-2) ‚úÖ  
**Next Milestone:** Phase 2 (Weeks 3-4) - Enhanced Prover with 25+ rules üîÑ

The foundation is solid, and the path forward is clear. Each phase builds incrementally, with comprehensive testing and integration at every step.

---

**Version:** 1.0.0  
**Date:** February 12, 2026  
**Author:** GitHub Copilot Agent  
**Status:** Phase 1 Complete, Phase 2 In Progress

---

## üîó SymbolicAI Integration Strategy

**See:** [SYMBOLICAI_INTEGRATION_ANALYSIS.md](./SYMBOLICAI_INTEGRATION_ANALYSIS.md) for complete details

### Executive Summary

**Status:** SymbolicAI (ExtensityAI/symbolicai) is **already integrated** with 1,876 LOC

**Decision:** **EXTEND** existing integration (same pattern as caching layer) ‚úÖ

**Benefits:**
- Reuse 1,876 LOC of existing code
- Leverage mature LLM abstraction layer
- Use proven contract validation system
- 50% reduction in development time (6 weeks vs 10 weeks)

### Existing Integration

```
ipfs_datasets_py/logic/integration/
‚îú‚îÄ‚îÄ symbolic_fol_bridge.py          563 LOC - FOL bridge with semantic parsing
‚îú‚îÄ‚îÄ symbolic_contracts.py           763 LOC - Contract validation
‚îú‚îÄ‚îÄ symbolic_logic_primitives.py    550 LOC - Logic primitives
‚îî‚îÄ‚îÄ [5+ more files using SymbolicAI]

Total: 1,876+ LOC already production-ready
```

### What is SymbolicAI?

**Project:** https://github.com/ExtensityAI/symbolicai  
**Paper:** https://arxiv.org/abs/2402.00854

**Core Features:**
1. **Dual-Mode Symbol Objects**: Toggle between syntactic (fast) and semantic (LLM-powered)
2. **Expression Composition**: Chain symbolic + semantic operations
3. **LLM Abstraction**: Unified interface for OpenAI, Anthropic, Google, local models
4. **Contract System**: Formal validation with automatic retries
5. **Probabilistic Programming**: Causal inference and quality scoring

### Integration Architecture

```
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ                Neurosymbolic Architecture                  ‚îÇ
‚îÇ                                                            ‚îÇ
‚îÇ  ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê         ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê        ‚îÇ
‚îÇ  ‚îÇ  TDFOL Module   ‚îÇ‚óÑ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚ñ∫‚îÇ  SymbolicAI      ‚îÇ        ‚îÇ
‚îÇ  ‚îÇ  (New: 3K LOC)  ‚îÇ         ‚îÇ  (Existing: 1.8K)‚îÇ        ‚îÇ
‚îÇ  ‚îÇ                 ‚îÇ         ‚îÇ                  ‚îÇ        ‚îÇ
‚îÇ  ‚îÇ  ‚Ä¢ Parser       ‚îÇ         ‚îÇ  ‚Ä¢ Semantic      ‚îÇ        ‚îÇ
‚îÇ  ‚îÇ  ‚Ä¢ Prover       ‚îÇ         ‚îÇ  ‚Ä¢ LLM Bridge    ‚îÇ        ‚îÇ
‚îÇ  ‚îÇ  ‚Ä¢ 40 Rules     ‚îÇ         ‚îÇ  ‚Ä¢ Contracts     ‚îÇ        ‚îÇ
‚îÇ  ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò         ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò        ‚îÇ
‚îÇ           ‚îÇ                           ‚îÇ                   ‚îÇ
‚îÇ           ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò                   ‚îÇ
‚îÇ                       ‚ñº                                   ‚îÇ
‚îÇ            ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê                      ‚îÇ
‚îÇ            ‚îÇ  Integration Layer   ‚îÇ                      ‚îÇ
‚îÇ            ‚îÇ  (New: Week 3-8)     ‚îÇ                      ‚îÇ
‚îÇ            ‚îÇ                      ‚îÇ                      ‚îÇ
‚îÇ            ‚îÇ  ‚Ä¢ TDFOL Bridge      ‚îÇ                      ‚îÇ
‚îÇ            ‚îÇ  ‚Ä¢ Proof Guidance    ‚îÇ                      ‚îÇ
‚îÇ            ‚îÇ  ‚Ä¢ Formula Embeddings‚îÇ                      ‚îÇ
‚îÇ            ‚îÇ  ‚Ä¢ Graph Enhancement ‚îÇ                      ‚îÇ
‚îÇ            ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò                      ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
```

### Enhancement Plan (Weeks 3-8)

#### Week 3: TDFOL Bridge (400 LOC)
**File:** `logic/neurosymbolic/symai_tdfol_bridge.py`

**Features:**
- Natural language ‚Üí TDFOL formulas (not just strings)
- Semantic validation using SymbolicAI contracts
- Caching for TDFOL objects
- Fallback to pattern-based parsing

**Example:**
```python
from ipfs_datasets_py.logic.neurosymbolic import SymaiTDFOLBridge

bridge = SymaiTDFOLBridge()

# Parse with semantic understanding
formula = bridge.parse_with_semantics(
    "It is obligatory that always, agents must report violations"
)
# Returns: DeonticFormula(O, TemporalFormula(‚ñ°, ...))

# Not just a string!
print(formula.get_free_variables())  # {'agent', 'violation'}
```

#### Week 4-5: Neural-Guided Proof Search (500 LOC)
**File:** `logic/neurosymbolic/symai_proof_guide.py`

**Features:**
- LLM selects best inference rules
- Natural language explanation of proof steps
- Contract-based validation of strategies

**Example:**
```python
prover = TDFOLProver(use_neural_guidance=True)
result = prover.prove(goal)

# Explanation included
print(result.proof_steps[0].explanation)
# "Applied Modus Ponens because we have P and P‚ÜíQ, so we can derive Q"
```

#### Week 5-6: Semantic Formula Embeddings (300 LOC)
**File:** `logic/neurosymbolic/symai_formula_embedder.py`

**Features:**
- TDFOL formulas ‚Üí 768-dim embeddings
- Semantic similarity search
- Fuzzy matching for theorem retrieval

**Example:**
```python
embedder = SymaiFormulaEmbedder()

# Find similar theorems
similar = embedder.similar_formulas(
    query=parse_tdfol("O(P(x))"),
    formula_bank=knowledge_base.get_all_formulas(),
    top_k=5
)

for formula, similarity in similar:
    print(f"{similarity:.2f}: {formula}")
```

#### Week 6-7: Contract-Based Validation (200 LOC)
**Extend:** `symbolic_contracts.py`

**Features:**
- Validate deontic logic consistency
- Check temporal logic coherence
- Semantic validation via LLM

**Example:**
```python
contract = TDFOLContract()

# Validate deontic formula
valid = contract.validate_deontic(
    parse_dcec("(and (O P) (O (not P)))")  # Inconsistent!
)
# Returns: False (with explanation)
```

#### Week 7-8: GraphRAG Enhancement (600 LOC)
**File:** `graphrag/logic_integration/symai_graph_builder.py`

**Features:**
- Logical entity extraction with types
- Theorem-augmented knowledge graphs
- Semantic similarity-based edges

**Example:**
```python
builder = SymaiGraphBuilder()

# Extract logical entities
entities = builder.extract_logical_entities(
    "The contractor must submit reports. Failure to report is prohibited."
)
# Returns: [
#   Entity("contractor", type=Sort.AGENT),
#   Entity("submit_reports", type=Sort.ACTION),
#   Entity("failure_to_report", type=Sort.EVENT),
# ]

# Build theorem graph
graph = builder.build_theorem_graph(knowledge_base.theorems)
# Graph with semantic similarity edges
```

### Code Reuse Strategy

**Pattern:** Same as caching layer enhancement

1. **Don't Replace** - Extend existing code
2. **Backward Compatible** - Keep current API working
3. **Progressive Enhancement** - Add features incrementally
4. **Fallback Mechanisms** - Graceful degradation without LLM
5. **Performance First** - Cache aggressively

### Timeline Comparison

| Approach | Time | LOC | Risk |
|----------|------|-----|------|
| **Build from Scratch** | 10 weeks | 3,000 | High |
| **Extend SymbolicAI** ‚úÖ | 6 weeks | 2,000 | Low |

**Savings:** 4 weeks, 1,000 LOC, lower risk

### Dependencies

**Current:**
```python
extras_require = {
    'symbolic': ['symbolicai>=0.13.1'],
}
```

**Enhanced (Week 3):**
```python
extras_require = {
    'neurosymbolic': [
        'symbolicai>=0.13.1',
        'sentence-transformers>=2.0.0',
        'faiss-cpu>=1.7.0',
    ],
}
```

### Success Metrics

**Technical:**
- ‚úÖ TDFOL formulas from natural language (90%+ accuracy)
- ‚úÖ Proof search 30%+ faster with neural guidance
- ‚úÖ Formula similarity 0.85+ correlation with human judgment

**Integration:**
- ‚úÖ Seamless with existing symbolic_fol_bridge.py
- ‚úÖ <5% performance overhead
- ‚úÖ 80%+ LLM calls cached

**Code:**
- ‚úÖ <2,000 LOC new code (reuse 1,876 LOC existing)
- ‚úÖ 90%+ test coverage
- ‚úÖ Full type hints

### Testing Strategy

**Unit Tests (100+):**
- TDFOL bridge: 30 tests
- Proof guidance: 25 tests
- Formula embeddings: 20 tests
- Contracts: 15 tests
- Graph builder: 20 tests

**Integration Tests (30+):**
- End-to-end text ‚Üí TDFOL ‚Üí proof
- GraphRAG with theorems
- Multi-step reasoning pipelines

**Performance Tests:**
- Caching effectiveness: >80% hit rate
- LLM latency: <500ms with cache
- Proof speedup: >30% improvement

### Risk Mitigation

**Risk 1: SymbolicAI Dependency**
- *Mitigation:* Fallback to pattern-based parsing
- *Tested:* Already in production with fallback

**Risk 2: LLM API Costs**
- *Mitigation:* Aggressive caching (existing layer)
- *Tested:* Current cache reduces 80%+ calls

**Risk 3: LLM Reliability**
- *Mitigation:* Contract validation + multi-engine fallback
- *Tested:* SymbolicAI has built-in retry logic

**Risk 4: Integration Complexity**
- *Mitigation:* Incremental rollout, keep current API
- *Tested:* Same pattern as caching layer (proven)

---

## üìù Updated Implementation Plan with SymbolicAI

### Phase 1: Foundation ‚úÖ (Weeks 1-2) - COMPLETE
- [x] Unified TDFOL module
- [x] 40 inference rules
- [x] DCEC parser
- [x] Documentation

### Phase 2: Enhanced Prover ‚úÖ (Weeks 3-4) - SUBSTANTIALLY COMPLETE
- [x] 40 comprehensive rules
- [x] DCEC string parser
- [x] Integration with prover
- [ ] **Week 3: SymbolicAI TDFOL Bridge** ‚Üê NEXT
- [ ] Proof caching and optimization
- [ ] Full test coverage

### Phase 3: Neural-Symbolic Bridge (Weeks 5-6) - UPDATED
- [ ] **Week 4-5: SymaiProofGuide** - Neural-guided proof search
- [ ] **Week 5-6: SymaiFormulaEmbedder** - Semantic embeddings
- [ ] Hybrid confidence scoring
- [ ] Pattern matching

### Phase 4: GraphRAG Integration (Weeks 7-8) - UPDATED
- [ ] **Week 6-7: TDFOLContract** - Contract-based validation
- [ ] **Week 7-8: SymaiGraphBuilder** - Logic-aware graphs
- [ ] Theorem-augmented RAG
- [ ] Temporal graph reasoning

### Phase 5: End-to-End Pipeline (Weeks 9-10)
- [ ] Unified NeurosymbolicGraphRAG class
- [ ] Complete text ‚Üí TDFOL ‚Üí proof ‚Üí graph pipeline
- [ ] Interactive query interface
- [ ] Visualization tools

### Phase 6: Testing & Documentation (Weeks 11-12)
- [ ] 230+ comprehensive tests
- [ ] Performance benchmarking
- [ ] Complete documentation
- [ ] Production deployment

**Total Timeline:** 12 weeks (unchanged)  
**Code Reuse:** 1,876 LOC (SymbolicAI) + 3,069 LOC (TDFOL) = 4,945 LOC foundation  
**New Code:** ~2,000 LOC (integration layer)

---

**See Also:**
- [SYMBOLICAI_INTEGRATION_ANALYSIS.md](./SYMBOLICAI_INTEGRATION_ANALYSIS.md) - Complete analysis
- [README.md](./ipfs_datasets_py/logic/TDFOL/README.md) - TDFOL documentation
- [symbolic_fol_bridge.py](./ipfs_datasets_py/logic/integration/symbolic_fol_bridge.py) - Current integration

**Status:** Ready for Phase 2 completion (Week 3 - SymbolicAI TDFOL Bridge) ‚úÖ

