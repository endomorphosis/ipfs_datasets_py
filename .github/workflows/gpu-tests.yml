name: GPU-Enabled Tests

on:
  push:
    branches: [ main, develop ]
    paths:
      - 'ipfs_datasets_py/**'
      - 'tests/**'
      - '.github/workflows/gpu-tests.yml'
  pull_request:
    branches: [ main, develop ]
  workflow_dispatch:
    inputs:
      test_suite:
        description: 'Test suite to run'
        required: true
        default: 'gpu'
        type: choice
        options:
          - gpu
          - cpu
          - all

env:
  PYTHON_VERSION: '3.10'

jobs:
  # GPU tests on self-hosted runner
  gpu-tests:
    runs-on: [self-hosted, linux, x64, gpu]
    # Skip if runner not available
    if: ${{ !cancelled() }}
    continue-on-error: false
    timeout-minutes: 60
    
    steps:
      - name: Checkout repository
        uses: actions/checkout@v4
      
      - name: System Information
        run: |
          echo "## 🎮 GPU Runner Information" >> $GITHUB_STEP_SUMMARY
          echo "" >> $GITHUB_STEP_SUMMARY
          echo "### Hardware" >> $GITHUB_STEP_SUMMARY
          echo "\`\`\`" >> $GITHUB_STEP_SUMMARY
          nvidia-smi --query-gpu=name,driver_version,memory.total --format=csv,noheader >> $GITHUB_STEP_SUMMARY
          echo "\`\`\`" >> $GITHUB_STEP_SUMMARY
          echo "" >> $GITHUB_STEP_SUMMARY
          echo "### CUDA" >> $GITHUB_STEP_SUMMARY
          echo "- CUDA Version: $(nvidia-smi | grep "CUDA Version" | awk '{print $9}')" >> $GITHUB_STEP_SUMMARY
          echo "- GPU Count: $(nvidia-smi --list-gpus | wc -l)" >> $GITHUB_STEP_SUMMARY
          echo "" >> $GITHUB_STEP_SUMMARY
      
      - name: Set up Python
        uses: actions/setup-python@v5
        with:
          python-version: ${{ env.PYTHON_VERSION }}
          cache: 'pip'
      
      - name: Install dependencies
        run: |
          python -m pip install --upgrade pip
          pip install -e .[test]
          pip install torch torchvision torchaudio --index-url https://download.pytorch.org/whl/cu118
          pip install pytest pytest-cov pytest-xdist
      
      - name: Verify GPU Access
        run: |
          python -c "
          import torch
          print(f'PyTorch version: {torch.__version__}')
          print(f'CUDA available: {torch.cuda.is_available()}')
          print(f'CUDA version: {torch.version.cuda}')
          print(f'GPU count: {torch.cuda.device_count()}')
          if torch.cuda.is_available():
              for i in range(torch.cuda.device_count()):
                  print(f'GPU {i}: {torch.cuda.get_device_name(i)}')
                  print(f'  Memory: {torch.cuda.get_device_properties(i).total_memory / 1024**3:.2f} GB')
          "
      
      - name: Test MCP GPU Tools
        run: |
          echo "🔧 Testing MCP GPU-enabled tools..."
          python -c "
          import torch
          from ipfs_datasets_py.mcp_server.tools import get_all_tools
          
          print(f'CUDA Available: {torch.cuda.is_available()}')
          if torch.cuda.is_available():
              print(f'GPU Memory: {torch.cuda.get_device_properties(0).total_memory / 1024**3:.2f} GB')
          
          # Test GPU-specific MCP tools
          tools = get_all_tools()
          gpu_tools = [name for name in tools.keys() if 'gpu' in name.lower() or 'cuda' in name.lower()]
          print(f'GPU-enabled MCP tools: {gpu_tools}')
          "
      
      - name: Run GPU Tests
        run: |
          # Create GPU test results directory
          mkdir -p gpu_test_results
          
          # Run comprehensive GPU tests
          pytest tests/ -v \
            -m "gpu" \
            --cov=ipfs_datasets_py \
            --cov-report=xml:gpu_test_results/coverage.xml \
            --cov-report=term \
            --junit-xml=gpu_test_results/gpu-test-results.xml \
            --tb=short \
            --maxfail=5 \
            || echo "Some GPU tests failed, but continuing..."
          
          # Test specific GPU functionalities
          python -c "
          import sys
          import torch
          import numpy as np
          from pathlib import Path
          
          # Test GPU memory allocation
          try:
              if torch.cuda.is_available():
                  # Test memory allocation
                  x = torch.randn(1000, 1000, device='cuda')
                  y = torch.randn(1000, 1000, device='cuda')
                  z = torch.matmul(x, y)
                  print('✅ GPU matrix operations successful')
                  
                  # Test memory cleanup
                  del x, y, z
                  torch.cuda.empty_cache()
                  print('✅ GPU memory cleanup successful')
              else:
                  print('⚠️ CUDA not available, skipping GPU-specific tests')
          except Exception as e:
              print(f'❌ GPU test failed: {e}')
              sys.exit(1)
          "
        env:
          CUDA_VISIBLE_DEVICES: "0,1"
          PYTORCH_CUDA_ALLOC_CONF: "max_split_size_mb:512"
          CUDA_LAUNCH_BLOCKING: "1"
      
      - name: Upload GPU Test Results
        uses: actions/upload-artifact@v4
        if: always()
        with:
          name: gpu-test-results
          path: |
            gpu-test-results.xml
            coverage.xml
      
      - name: GPU Memory Summary
        if: always()
        run: |
          echo "## 📊 GPU Memory Usage" >> $GITHUB_STEP_SUMMARY
          echo "\`\`\`" >> $GITHUB_STEP_SUMMARY
          nvidia-smi --query-gpu=index,name,memory.used,memory.total --format=csv >> $GITHUB_STEP_SUMMARY
          echo "\`\`\`" >> $GITHUB_STEP_SUMMARY

  # CPU-only tests on GitHub-hosted runner
  cpu-tests:
    runs-on: ubuntu-latest
    
    steps:
      - name: Checkout repository
        uses: actions/checkout@v4
      
      - name: Set up Python
        uses: actions/setup-python@v5
        with:
          python-version: ${{ env.PYTHON_VERSION }}
          cache: 'pip'
      
      - name: Install dependencies
        run: |
          python -m pip install --upgrade pip
          pip install -e .[test]
          pip install pytest pytest-cov
      
      - name: Run CPU Tests
        run: |
          pytest tests/ -v \
            -m "not gpu" \
            --cov=ipfs_datasets_py \
            --cov-report=xml \
            --cov-report=term \
            --junit-xml=cpu-test-results.xml
      
      - name: Upload CPU Test Results
        uses: actions/upload-artifact@v4
        if: always()
        with:
          name: cpu-test-results
          path: |
            cpu-test-results.xml
            coverage.xml

  # Docker tests with GPU support
  gpu-docker-tests:
    runs-on: [self-hosted, linux, x64, gpu]
    if: ${{ !cancelled() }}
    continue-on-error: true
    
    steps:
      - name: Checkout repository
        uses: actions/checkout@v4
      
      - name: Build GPU Docker Image
        run: |
          docker build -t ipfs-datasets-py:gpu-test \
            -f Dockerfile.gpu \
            --build-arg CUDA_VERSION=12.0.0 \
            --build-arg PYTHON_VERSION=3.10 \
            .
      
      - name: Test GPU in Docker
        run: |
          docker run --rm --gpus all ipfs-datasets-py:gpu-test python -c "
          import torch
          assert torch.cuda.is_available(), 'CUDA not available in container'
          print(f'✅ GPU accessible in Docker')
          print(f'GPU Count: {torch.cuda.device_count()}')
          for i in range(torch.cuda.device_count()):
              print(f'GPU {i}: {torch.cuda.get_device_name(i)}')
          "
      
      - name: Run Tests in GPU Container
        run: |
          docker run --rm --gpus all \
            -v $(pwd)/tests:/app/tests \
            -v $(pwd)/test_results:/app/test_results \
            ipfs-datasets-py:gpu-test \
            pytest /app/tests -v -m "gpu" \
              --junit-xml=/app/test_results/docker-gpu-tests.xml \
              || echo "Some Docker GPU tests failed"
      
      - name: Cleanup
        if: always()
        run: |
          docker rmi ipfs-datasets-py:gpu-test || true

  # Test summary
  test-summary:
    runs-on: ubuntu-latest
    needs: [gpu-tests, cpu-tests, gpu-docker-tests]
    if: always()
    
    steps:
      - name: Generate Test Summary
        run: |
          echo "## 🧪 Test Results Summary" >> $GITHUB_STEP_SUMMARY
          echo "" >> $GITHUB_STEP_SUMMARY
          echo "| Test Suite | Runner | Status |" >> $GITHUB_STEP_SUMMARY
          echo "|------------|--------|--------|" >> $GITHUB_STEP_SUMMARY
          echo "| GPU Tests | Self-Hosted (RTX 3090) | ${{ needs.gpu-tests.result }} |" >> $GITHUB_STEP_SUMMARY
          echo "| CPU Tests | GitHub-Hosted | ${{ needs.cpu-tests.result }} |" >> $GITHUB_STEP_SUMMARY
          echo "| Docker GPU Tests | Self-Hosted (RTX 3090) | ${{ needs.gpu-docker-tests.result }} |" >> $GITHUB_STEP_SUMMARY
          echo "" >> $GITHUB_STEP_SUMMARY
          
          if [ "${{ needs.gpu-tests.result }}" = "success" ]; then
            echo "✅ GPU tests passed on self-hosted runner" >> $GITHUB_STEP_SUMMARY
          elif [ "${{ needs.gpu-tests.result }}" = "skipped" ]; then
            echo "⏭️ GPU tests skipped (runner not available)" >> $GITHUB_STEP_SUMMARY
            echo "" >> $GITHUB_STEP_SUMMARY
            echo "To enable GPU tests:" >> $GITHUB_STEP_SUMMARY
            echo "1. Set up a self-hosted runner with GPU" >> $GITHUB_STEP_SUMMARY
            echo "2. See [GPU_RUNNER_SETUP.md](./GPU_RUNNER_SETUP.md)" >> $GITHUB_STEP_SUMMARY
          else
            echo "❌ GPU tests failed or had errors" >> $GITHUB_STEP_SUMMARY
          fi
