name: MCP Dashboard Automated Tests

on:
  push:
    branches: [main, develop]
    paths:
      - 'ipfs_datasets_py/mcp_dashboard/**'
      - 'ipfs_datasets_py/mcp_server/**'
      - 'validate_mcp_dashboard.py'
      - 'mcp_dashboard_tests.py'
      - '.github/workflows/mcp-dashboard-tests.yml'
  pull_request:
    branches: [main, develop]
  workflow_dispatch:
    inputs:
      test_mode:
        description: 'Test mode to run'
        required: true
        default: 'comprehensive'
        type: choice
        options:
          - smoke
          - comprehensive
          - performance
          - accessibility
  schedule:
    # Run dashboard tests daily at 2 AM UTC
    - cron: '0 2 * * *'

permissions:
  contents: read
  actions: read

env:
  PYTHON_VERSION: '3.12'
  MCP_DASHBOARD_PORT: 8899
  MCP_DASHBOARD_HOST: 127.0.0.1

jobs:
  # Basic smoke tests for all platforms (containerized)
  dashboard-smoke-tests:
    runs-on: [self-hosted, linux, x64]
    container:
      image: python:3.12-slim
      options: --user root
    strategy:
      matrix:
        python-version: ['3.12']
      fail-fast: false
    
    steps:
      - name: Checkout repository
        uses: actions/checkout@v5        with:
          submodules: false
      
      - name: Set up Docker Buildx
        uses: docker/setup-buildx-action@v3
      
      - name: Build MCP Test Docker Image (Python ${{ matrix.python-version }})
        run: |
          # Build with specific Python version
          docker build \
            --build-arg PYTHON_VERSION=${{ matrix.python-version }} \
            -f docker/Dockerfile.mcp-tests \
            -t ipfs-datasets-mcp-tests:py${{ matrix.python-version }} .
      
      - name: Run Dashboard Validation in Container
        id: validation
        run: |
          echo "ðŸ§ª Running MCP Dashboard validation in container..."
          mkdir -p test-results
          
          # Run validation in container
          docker run --rm \
            -v $(pwd)/test-results:/app/test-results \
            ipfs-datasets-mcp-tests:py${{ matrix.python-version }} \
            python validate_mcp_dashboard.py > test-results/validation-py${{ matrix.python-version }}.log 2>&1 || echo "validation_result=failed" >> $GITHUB_OUTPUT
          
          # Check if validation succeeded
          if [ -f test-results/validation-py${{ matrix.python-version }}.log ]; then
            if grep -q "success\|passed\|âœ…" test-results/validation-py${{ matrix.python-version }}.log; then
              echo "validation_result=success" >> $GITHUB_OUTPUT
            fi
          fi
        continue-on-error: true
      
      - name: Smoke Test Summary
        run: |
          echo "## ðŸ’¨ Smoke Test Results (Containerized)" >> $GITHUB_STEP_SUMMARY
          echo "" >> $GITHUB_STEP_SUMMARY
          echo "- **Runner**: ubuntu-latest (Docker)" >> $GITHUB_STEP_SUMMARY
          echo "- **Python**: ${{ matrix.python-version }}" >> $GITHUB_STEP_SUMMARY
          echo "- **Validation**: ${{ steps.validation.outputs.validation_result || 'failed' }}" >> $GITHUB_STEP_SUMMARY
          echo "- **Security**: Tests run in isolated Docker container" >> $GITHUB_STEP_SUMMARY
      
      - name: Upload Validation Logs
        uses: actions/upload-artifact@v4
        if: always()
        with:
          name: smoke-test-logs-py${{ matrix.python-version }}
          path: test-results/

  # Comprehensive dashboard testing
  dashboard-comprehensive-tests:
    runs-on: [self-hosted, linux, x64]
    container:
      image: python:3.12-slim
      options: --user root
    needs: dashboard-smoke-tests
    if: ${{ !cancelled() && (github.event.inputs.test_mode == 'comprehensive' || github.event_name != 'workflow_dispatch') }}
    
    steps:
      - name: Checkout repository
        uses: actions/checkout@v5        with:
          submodules: false
      
      - name: Set up Docker Buildx
        uses: docker/setup-buildx-action@v3
      
      - name: Build MCP Test Docker Image
        run: |
          docker build -f docker/Dockerfile.mcp-tests -t ipfs-datasets-mcp-tests:latest .
      
      - name: Run MCP Dashboard Tests in Docker
        run: |
          mkdir -p test-results
          # Run the container with dashboard service
          docker run --rm -d \
            --name mcp-dashboard-test \
            -p 8899:8899 \
            -v $(pwd)/test-results:/app/test-results \
            ipfs-datasets-mcp-tests:latest \
            python -m ipfs_datasets_py.mcp_dashboard
          
          # Wait for dashboard to be ready
          for i in {1..30}; do
            if curl -f http://127.0.0.1:8899/api/health 2>/dev/null; then
              echo "âœ… Dashboard is ready!"
              break
            fi
            echo "â³ Waiting for dashboard... ($i/30)"
            sleep 2
          done
          
          # Run tests against the dashboard
          docker exec mcp-dashboard-test pytest tests/ -v \
            --cov=ipfs_datasets_py \
            --cov-report=xml:/app/test-results/coverage.xml \
            --junit-xml=/app/test-results/mcp-test-results.xml || true
          
          # Stop the container
          docker stop mcp-dashboard-test
      
      - name: Upload Test Results
        uses: actions/upload-artifact@v4
        if: always()
        with:
          name: mcp-comprehensive-test-results
          path: test-results/

  # Endpoint and browser testing with live dashboard (containerized)
  dashboard-endpoint-tests:
    runs-on: [self-hosted, linux, x64]
    container:
      image: python:3.12-slim
      options: --user root
    needs: dashboard-smoke-tests
    if: ${{ !cancelled() && (github.event.inputs.test_mode == 'comprehensive' || github.event_name != 'workflow_dispatch') }}
    
    steps:
      - name: Checkout repository
        uses: actions/checkout@v5        with:
          submodules: false
      
      - name: Set up Docker Buildx
        uses: docker/setup-buildx-action@v3
      
      - name: Build MCP Test Docker Image
        run: |
          docker build -f docker/Dockerfile.mcp-tests -t ipfs-datasets-mcp-tests:latest .
      
      - name: Run Endpoint Tests in Container
        run: |
          mkdir -p test-results
          echo "ðŸš€ Starting MCP Dashboard in container for endpoint tests..."
          
          # Start dashboard in container
          docker run --rm -d \
            --name mcp-endpoint-test \
            -p 8899:8899 \
            -v $(pwd)/test-results:/app/test-results \
            ipfs-datasets-mcp-tests:latest \
            python -m ipfs_datasets_py.mcp_dashboard
          
          # Wait for dashboard to be ready
          echo "â³ Waiting for dashboard..."
          for i in {1..60}; do
            if curl -f http://127.0.0.1:8899/api/health 2>/dev/null; then
              echo "âœ… Dashboard is ready!"
              break
            fi
            if [ $i -eq 60 ]; then
              echo "âŒ Dashboard failed to start"
              docker logs mcp-endpoint-test
              docker stop mcp-endpoint-test || true
              exit 1
            fi
            sleep 2
          done
          
          # Run endpoint tests in separate container
          docker run --rm \
            --network="host" \
            -v $(pwd)/test-results:/app/test-results \
            ipfs-datasets-mcp-tests:latest \
            bash -c 'python -c "import requests; import json; response = requests.get(\"http://127.0.0.1:8899/api/mcp/tools\", timeout=15); tools = response.json() if response.status_code == 200 else []; print(f\"âœ… Loaded {len(tools)} MCP tools\") if tools else print(\"âš ï¸ No tools loaded\")"' || true
          
          # Stop dashboard container
          docker stop mcp-endpoint-test || true
      
      - name: Run Browser Tests in Container
        run: |
          echo "ðŸŒ Running browser-based tests in container..."
          
          # Start dashboard in container for browser tests
          docker run --rm -d \
            --name mcp-browser-test \
            -p 8899:8899 \
            -v $(pwd)/test-results:/app/test-results \
            ipfs-datasets-mcp-tests:latest \
            python -m ipfs_datasets_py.mcp_dashboard
          
          # Wait for dashboard
          sleep 10
          
          # Run browser tests
          docker run --rm \
            --network="host" \
            -v $(pwd)/test-results:/app/test-results \
            -v $(pwd)/.github/scripts:/scripts \
            ipfs-datasets-mcp-tests:latest \
            python /scripts/test_browser.py || true
          
          # Stop dashboard container
          docker stop mcp-browser-test || true
      
      - name: Upload Test Results
        uses: actions/upload-artifact@v4
        if: always()
        with:
          name: dashboard-endpoint-test-results
          path: test-results/

  # Self-hosted runner tests (x86_64 and ARM64)
  dashboard-self-hosted-tests:
    runs-on: ${{ matrix.runner }}
    needs: dashboard-smoke-tests
    if: ${{ !cancelled() }}
    continue-on-error: true
    strategy:
      matrix:
        runner: 
          - [self-hosted, linux, x64]
          - [self-hosted, linux, arm64]
      fail-fast: false
    
    steps:
      - name: Checkout repository
        uses: actions/checkout@v5        with:
          submodules: false
      
      - name: System Info
        run: |
          echo "ðŸ–¥ï¸ Testing on $(uname -m) architecture"
          echo "OS: $(cat /etc/os-release | grep PRETTY_NAME | cut -d= -f2)"
          echo "Python: $(python3 --version)"
      
      - name: Set up environment
        run: |
          python3 -m venv .venv-dashboard-test
          source .venv-dashboard-test/bin/activate
          pip install --upgrade pip
          pip install -e .[test]
          pip install flask mcp
      
      - name: Run Architecture-Specific Tests
        run: |
          source .venv-dashboard-test/bin/activate
          echo "ðŸ§ª Running dashboard tests on $(uname -m)..."
          
          # Run validation with architecture info
          python -c "
          import platform
          import subprocess
          import sys
          
          print(f'Architecture: {platform.machine()}')
          print(f'Platform: {platform.platform()}')
          
          # Run validation
          result = subprocess.run([sys.executable, 'validate_mcp_dashboard.py'], 
                                capture_output=True, text=True)
          
          print('STDOUT:', result.stdout)
          if result.stderr:
              print('STDERR:', result.stderr)
          
          if result.returncode == 0:
              print(f'âœ… Dashboard validation passed on {platform.machine()}')
          else:
              print(f'âŒ Dashboard validation failed on {platform.machine()}')
              sys.exit(1)
          "

  # Generate comprehensive test report
  dashboard-test-summary:
    runs-on: [self-hosted, linux, x64]
    container:
      image: python:3.12-slim
      options: --user root
    needs: [dashboard-smoke-tests, dashboard-comprehensive-tests, dashboard-self-hosted-tests]
    if: always()
    
    steps:
      - name: Download Test Results
        uses: actions/download-artifact@v4
        with:
          name: dashboard-comprehensive-test-results
          path: test_results/
        continue-on-error: true
      
      - name: Generate Test Summary
        run: |
          echo "## ðŸŽ›ï¸ MCP Dashboard Test Results" >> $GITHUB_STEP_SUMMARY
          echo "" >> $GITHUB_STEP_SUMMARY
          echo "### Test Suite Results" >> $GITHUB_STEP_SUMMARY
          echo "" >> $GITHUB_STEP_SUMMARY
          echo "| Test Suite | Status | Details |" >> $GITHUB_STEP_SUMMARY
          echo "|------------|--------|---------|" >> $GITHUB_STEP_SUMMARY
          echo "| Smoke Tests | ${{ needs.dashboard-smoke-tests.result }} | Basic functionality across Python versions |" >> $GITHUB_STEP_SUMMARY
          echo "| Comprehensive Tests | ${{ needs.dashboard-comprehensive-tests.result }} | Full endpoint and UI testing |" >> $GITHUB_STEP_SUMMARY
          echo "| Self-Hosted Tests | ${{ needs.dashboard-self-hosted-tests.result }} | Multi-architecture validation |" >> $GITHUB_STEP_SUMMARY
          echo "" >> $GITHUB_STEP_SUMMARY
          
          # Add recommendations
          echo "### Recommendations" >> $GITHUB_STEP_SUMMARY
          echo "" >> $GITHUB_STEP_SUMMARY
          
          if [ "${{ needs.dashboard-smoke-tests.result }}" = "success" ]; then
            echo "âœ… Basic dashboard functionality is working correctly" >> $GITHUB_STEP_SUMMARY
          else
            echo "âš ï¸ Smoke tests failed - check basic setup and dependencies" >> $GITHUB_STEP_SUMMARY
          fi
          
          if [ "${{ needs.dashboard-comprehensive-tests.result }}" = "success" ]; then
            echo "âœ… Comprehensive testing passed - dashboard is production ready" >> $GITHUB_STEP_SUMMARY
          else
            echo "âš ï¸ Comprehensive tests failed - review endpoint functionality" >> $GITHUB_STEP_SUMMARY
          fi
          
          if [ "${{ needs.dashboard-self-hosted-tests.result }}" = "success" ]; then
            echo "âœ… Multi-architecture support confirmed" >> $GITHUB_STEP_SUMMARY
          else
            echo "âš ï¸ Self-hosted runner tests had issues - may be expected if runners not available" >> $GITHUB_STEP_SUMMARY
          fi
          
          echo "" >> $GITHUB_STEP_SUMMARY
          echo "### Next Steps" >> $GITHUB_STEP_SUMMARY
          echo "- Monitor dashboard performance in production" >> $GITHUB_STEP_SUMMARY
          echo "- Set up automated monitoring and alerting" >> $GITHUB_STEP_SUMMARY
          echo "- Consider adding more comprehensive integration tests" >> $GITHUB_STEP_SUMMARY
