name: MCP Dashboard Automated Tests
true:
  push:
    branches:
    - main
    - develop
    paths:
    - ipfs_datasets_py/mcp_dashboard/**
    - ipfs_datasets_py/mcp_server/**
    - validate_mcp_dashboard.py
    - mcp_dashboard_tests.py
    - .github/workflows/mcp-dashboard-tests.yml
  pull_request:
    branches:
    - main
    - develop
  workflow_dispatch:
    inputs:
      test_mode:
        description: Test mode to run
        required: true
        default: comprehensive
        type: choice
        options:
        - smoke
        - comprehensive
        - performance
        - accessibility
  schedule:
  - cron: 0 2 * * *
concurrency:
  group: ${{ github.workflow }}-${{ github.ref }}
  cancel-in-progress: true
permissions:
  contents: read
  actions: read
env:
  PYTHON_VERSION: '3.12'
  MCP_DASHBOARD_PORT: 8899
  MCP_DASHBOARD_HOST: 127.0.0.1
jobs:
  dashboard-smoke-tests:
    runs-on:
    - self-hosted
    - linux
    - x64
    timeout-minutes: 45
    container:
      image: python:3.12-slim
      options: --user root
    strategy:
      matrix:
        python-version:
        - '3.12'
      fail-fast: false
    steps:
    - name: Checkout repository
      uses: actions/checkout@v5
      with:
        fetch-depth: 1
        submodules: false
    - name: Set up Docker Buildx
      uses: docker/setup-buildx-action@v3
    - name: Build MCP Test Docker Image (Python ${{ matrix.python-version }})
      run: "# Build with specific Python version\ndocker build \\\n  --build-arg PYTHON_VERSION=${{ matrix.python-version\
        \ }} \\\n  -f docker/Dockerfile.mcp-tests \\\n  -t ipfs-datasets-mcp-tests:py${{ matrix.python-version }} .\n"
    - name: Run Dashboard Validation in Container
      id: validation
      run: "echo \"\U0001F9EA Running MCP Dashboard validation in container...\"\nmkdir -p test-results\n\n# Run validation\
        \ in container\ndocker run --rm \\\n  -v $(pwd)/test-results:/app/test-results \\\n  ipfs-datasets-mcp-tests:py${{\
        \ matrix.python-version }} \\\n  python validate_mcp_dashboard.py > test-results/validation-py${{ matrix.python-version\
        \ }}.log 2>&1 || echo \"validation_result=failed\" >> $GITHUB_OUTPUT\n\n# Check if validation succeeded\nif [ -f test-results/validation-py${{\
        \ matrix.python-version }}.log ]; then\n  if grep -q \"success\\|passed\\|\u2705\" test-results/validation-py${{ matrix.python-version\
        \ }}.log; then\n    echo \"validation_result=success\" >> $GITHUB_OUTPUT\n  fi\nfi\n"
      continue-on-error: true
    - name: Smoke Test Summary
      run: "echo \"## \U0001F4A8 Smoke Test Results (Containerized)\" >> $GITHUB_STEP_SUMMARY\necho \"\" >> $GITHUB_STEP_SUMMARY\n\
        echo \"- **Runner**: ubuntu-latest (Docker)\" >> $GITHUB_STEP_SUMMARY\necho \"- **Python**: ${{ matrix.python-version\
        \ }}\" >> $GITHUB_STEP_SUMMARY\necho \"- **Validation**: ${{ steps.validation.outputs.validation_result || 'failed'\
        \ }}\" >> $GITHUB_STEP_SUMMARY\necho \"- **Security**: Tests run in isolated Docker container\" >> $GITHUB_STEP_SUMMARY\n"
    - name: Upload Validation Logs
      uses: actions/upload-artifact@v4
      if: always()
      with:
        name: smoke-test-logs-py${{ matrix.python-version }}
        path: test-results/
  dashboard-comprehensive-tests:
    runs-on:
    - self-hosted
    - linux
    - x64
    timeout-minutes: 45
    container:
      image: python:3.12-slim
      options: --user root
    needs: dashboard-smoke-tests
    if: ${{ !cancelled() && (github.event.inputs.test_mode == 'comprehensive' || github.event_name != 'workflow_dispatch')
      }}
    steps:
    - name: Checkout repository
      uses: actions/checkout@v5
      with:
        fetch-depth: 1
        submodules: false
    - name: Set up Docker Buildx
      uses: docker/setup-buildx-action@v3
    - name: Build MCP Test Docker Image
      run: 'docker build -f docker/Dockerfile.mcp-tests -t ipfs-datasets-mcp-tests:latest .

        '
    - name: Run MCP Dashboard Tests in Docker
      run: "mkdir -p test-results\n# Run the container with dashboard service\ndocker run --rm -d \\\n  --name mcp-dashboard-test\
        \ \\\n  -p 8899:8899 \\\n  -v $(pwd)/test-results:/app/test-results \\\n  ipfs-datasets-mcp-tests:latest \\\n  python\
        \ -m ipfs_datasets_py.mcp_dashboard\n\n# Wait for dashboard to be ready\nfor i in {1..30}; do\n  if curl -f http://127.0.0.1:8899/api/health\
        \ 2>/dev/null; then\n    echo \"\u2705 Dashboard is ready!\"\n    break\n  fi\n  echo \"\u23F3 Waiting for dashboard...\
        \ ($i/30)\"\n  sleep 2\ndone\n\n# Run tests against the dashboard\ndocker exec mcp-dashboard-test pytest tests/ -v\
        \ \\\n  --cov=ipfs_datasets_py \\\n  --cov-report=xml:/app/test-results/coverage.xml \\\n  --junit-xml=/app/test-results/mcp-test-results.xml\
        \ || true\n\n# Stop the container\ndocker stop mcp-dashboard-test\n"
    - name: Upload Test Results
      uses: actions/upload-artifact@v4
      if: always()
      with:
        name: mcp-comprehensive-test-results
        path: test-results/
  dashboard-endpoint-tests:
    runs-on:
    - self-hosted
    - linux
    - x64
    timeout-minutes: 45
    container:
      image: python:3.12-slim
      options: --user root
    needs: dashboard-smoke-tests
    if: ${{ !cancelled() && (github.event.inputs.test_mode == 'comprehensive' || github.event_name != 'workflow_dispatch')
      }}
    steps:
    - name: Checkout repository
      uses: actions/checkout@v5
      with:
        fetch-depth: 1
        submodules: false
    - name: Set up Docker Buildx
      uses: docker/setup-buildx-action@v3
    - name: Build MCP Test Docker Image
      run: 'docker build -f docker/Dockerfile.mcp-tests -t ipfs-datasets-mcp-tests:latest .

        '
    - name: Run Endpoint Tests in Container
      run: "mkdir -p test-results\necho \"\U0001F680 Starting MCP Dashboard in container for endpoint tests...\"\n\n# Start\
        \ dashboard in container\ndocker run --rm -d \\\n  --name mcp-endpoint-test \\\n  -p 8899:8899 \\\n  -v $(pwd)/test-results:/app/test-results\
        \ \\\n  ipfs-datasets-mcp-tests:latest \\\n  python -m ipfs_datasets_py.mcp_dashboard\n\n# Wait for dashboard to be\
        \ ready\necho \"\u23F3 Waiting for dashboard...\"\nfor i in {1..60}; do\n  if curl -f http://127.0.0.1:8899/api/health\
        \ 2>/dev/null; then\n    echo \"\u2705 Dashboard is ready!\"\n    break\n  fi\n  if [ $i -eq 60 ]; then\n    echo\
        \ \"\u274C Dashboard failed to start\"\n    docker logs mcp-endpoint-test\n    docker stop mcp-endpoint-test || true\n\
        \    exit 1\n  fi\n  sleep 2\ndone\n\n# Run endpoint tests in separate container\ndocker run --rm \\\n  --network=\"\
        host\" \\\n  -v $(pwd)/test-results:/app/test-results \\\n  ipfs-datasets-mcp-tests:latest \\\n  bash -c 'python -c\
        \ \"import requests; import json; response = requests.get(\\\"http://127.0.0.1:8899/api/mcp/tools\\\", timeout=15);\
        \ tools = response.json() if response.status_code == 200 else []; print(f\\\"\u2705 Loaded {len(tools)} MCP tools\\\
        \") if tools else print(\\\"\u26A0\uFE0F No tools loaded\\\")\"' || true\n\n# Stop dashboard container\ndocker stop\
        \ mcp-endpoint-test || true\n"
    - name: Run Browser Tests in Container
      run: "echo \"\U0001F310 Running browser-based tests in container...\"\n\n# Start dashboard in container for browser\
        \ tests\ndocker run --rm -d \\\n  --name mcp-browser-test \\\n  -p 8899:8899 \\\n  -v $(pwd)/test-results:/app/test-results\
        \ \\\n  ipfs-datasets-mcp-tests:latest \\\n  python -m ipfs_datasets_py.mcp_dashboard\n\n# Wait for dashboard\nsleep\
        \ 10\n\n# Run browser tests\ndocker run --rm \\\n  --network=\"host\" \\\n  -v $(pwd)/test-results:/app/test-results\
        \ \\\n  -v $(pwd)/.github/scripts:/scripts \\\n  ipfs-datasets-mcp-tests:latest \\\n  python /scripts/test_browser.py\
        \ || true\n\n# Stop dashboard container\ndocker stop mcp-browser-test || true\n"
    - name: Upload Test Results
      uses: actions/upload-artifact@v4
      if: always()
      with:
        name: dashboard-endpoint-test-results
        path: test-results/
  dashboard-self-hosted-tests:
    runs-on: ${{ matrix.runner }}
    timeout-minutes: 30
    needs: dashboard-smoke-tests
    if: ${{ !cancelled() }}
    continue-on-error: true
    strategy:
      matrix:
        runner:
        - - self-hosted
          - linux
          - x64
        - - self-hosted
          - linux
          - arm64
      fail-fast: false
    steps:
    - name: Checkout repository
      uses: actions/checkout@v5
      with:
        fetch-depth: 1
        submodules: false
    - name: System Info
      run: "echo \"\U0001F5A5\uFE0F Testing on $(uname -m) architecture\"\necho \"OS: $(cat /etc/os-release | grep PRETTY_NAME\
        \ | cut -d= -f2)\"\necho \"Python: $(python3 --version)\"\n"
    - name: Set up environment
      run: 'python3 -m venv .venv-dashboard-test

        source .venv-dashboard-test/bin/activate

        pip install --upgrade pip

        pip install -e .[test]

        pip install flask mcp

        '
    - name: Run Architecture-Specific Tests
      run: "source .venv-dashboard-test/bin/activate\necho \"\U0001F9EA Running dashboard tests on $(uname -m)...\"\n\n# Run\
        \ validation with architecture info\npython -c \"\nimport platform\nimport subprocess\nimport sys\n\nprint(f'Architecture:\
        \ {platform.machine()}')\nprint(f'Platform: {platform.platform()}')\n\n# Run validation\nresult = subprocess.run([sys.executable,\
        \ 'validate_mcp_dashboard.py'], \n                      capture_output=True, text=True)\n\nprint('STDOUT:', result.stdout)\n\
        if result.stderr:\n    print('STDERR:', result.stderr)\n\nif result.returncode == 0:\n    print(f'\u2705 Dashboard\
        \ validation passed on {platform.machine()}')\nelse:\n    print(f'\u274C Dashboard validation failed on {platform.machine()}')\n\
        \    sys.exit(1)\n\"\n"
  dashboard-test-summary:
    runs-on:
    - self-hosted
    - linux
    - x64
    timeout-minutes: 45
    container:
      image: python:3.12-slim
      options: --user root
    needs:
    - dashboard-smoke-tests
    - dashboard-comprehensive-tests
    - dashboard-self-hosted-tests
    if: always()
    steps:
    - name: Download Test Results
      uses: actions/download-artifact@v4
      with:
        name: dashboard-comprehensive-test-results
        path: test_results/
      continue-on-error: true
    - name: Generate Test Summary
      run: "echo \"## \U0001F39B\uFE0F MCP Dashboard Test Results\" >> $GITHUB_STEP_SUMMARY\necho \"\" >> $GITHUB_STEP_SUMMARY\n\
        echo \"### Test Suite Results\" >> $GITHUB_STEP_SUMMARY\necho \"\" >> $GITHUB_STEP_SUMMARY\necho \"| Test Suite |\
        \ Status | Details |\" >> $GITHUB_STEP_SUMMARY\necho \"|------------|--------|---------|\" >> $GITHUB_STEP_SUMMARY\n\
        echo \"| Smoke Tests | ${{ needs.dashboard-smoke-tests.result }} | Basic functionality across Python versions |\"\
        \ >> $GITHUB_STEP_SUMMARY\necho \"| Comprehensive Tests | ${{ needs.dashboard-comprehensive-tests.result }} | Full\
        \ endpoint and UI testing |\" >> $GITHUB_STEP_SUMMARY\necho \"| Self-Hosted Tests | ${{ needs.dashboard-self-hosted-tests.result\
        \ }} | Multi-architecture validation |\" >> $GITHUB_STEP_SUMMARY\necho \"\" >> $GITHUB_STEP_SUMMARY\n\n# Add recommendations\n\
        echo \"### Recommendations\" >> $GITHUB_STEP_SUMMARY\necho \"\" >> $GITHUB_STEP_SUMMARY\n\nif [ \"${{ needs.dashboard-smoke-tests.result\
        \ }}\" = \"success\" ]; then\n  echo \"\u2705 Basic dashboard functionality is working correctly\" >> $GITHUB_STEP_SUMMARY\n\
        else\n  echo \"\u26A0\uFE0F Smoke tests failed - check basic setup and dependencies\" >> $GITHUB_STEP_SUMMARY\nfi\n\
        \nif [ \"${{ needs.dashboard-comprehensive-tests.result }}\" = \"success\" ]; then\n  echo \"\u2705 Comprehensive\
        \ testing passed - dashboard is production ready\" >> $GITHUB_STEP_SUMMARY\nelse\n  echo \"\u26A0\uFE0F Comprehensive\
        \ tests failed - review endpoint functionality\" >> $GITHUB_STEP_SUMMARY\nfi\n\nif [ \"${{ needs.dashboard-self-hosted-tests.result\
        \ }}\" = \"success\" ]; then\n  echo \"\u2705 Multi-architecture support confirmed\" >> $GITHUB_STEP_SUMMARY\nelse\n\
        \  echo \"\u26A0\uFE0F Self-hosted runner tests had issues - may be expected if runners not available\" >> $GITHUB_STEP_SUMMARY\n\
        fi\n\necho \"\" >> $GITHUB_STEP_SUMMARY\necho \"### Next Steps\" >> $GITHUB_STEP_SUMMARY\necho \"- Monitor dashboard\
        \ performance in production\" >> $GITHUB_STEP_SUMMARY\necho \"- Set up automated monitoring and alerting\" >> $GITHUB_STEP_SUMMARY\n\
        echo \"- Consider adding more comprehensive integration tests\" >> $GITHUB_STEP_SUMMARY\n"
