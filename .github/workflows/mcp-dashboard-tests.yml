name: MCP Dashboard Automated Tests

on:
  push:
    branches: [ main, develop ]
    paths:
      - 'ipfs_datasets_py/mcp_dashboard/**'
      - 'ipfs_datasets_py/mcp_server/**'
      - 'validate_mcp_dashboard.py'
      - 'mcp_dashboard_tests.py'
      - '.github/workflows/mcp-dashboard-tests.yml'
  pull_request:
    branches: [ main, develop ]
  workflow_dispatch:
    inputs:
      test_mode:
        description: 'Test mode to run'
        required: true
        default: 'comprehensive'
        type: choice
        options:
          - smoke
          - comprehensive
          - performance
          - accessibility
  schedule:
    # Run dashboard tests daily at 2 AM UTC
    - cron: '0 2 * * *'

env:
  PYTHON_VERSION: '3.12'
  MCP_DASHBOARD_PORT: 8899
  MCP_DASHBOARD_HOST: 127.0.0.1

jobs:
  # Basic smoke tests for all platforms
  dashboard-smoke-tests:
    runs-on: ${{ matrix.os }}
    strategy:
      matrix:
        os: [ubuntu-latest]
        python-version: ['3.10', '3.11', '3.12']
      fail-fast: false
    
    steps:
      - name: Checkout repository
        uses: actions/checkout@v4
      with:
        submodules: false
      
      - name: Set up Python ${{ matrix.python-version }}
        uses: actions/setup-python@v5
        with:
          python-version: ${{ matrix.python-version }}
          cache: 'pip'
      
      - name: Install dependencies
        run: |
          python -m pip install --upgrade pip
          pip install -e .[test]
          pip install flask mcp pytest playwright
      
      - name: Install Playwright browsers
        run: |
          playwright install chromium --with-deps
      
      - name: Run Dashboard Validation
        id: validation
        run: |
          echo "ðŸ§ª Running MCP Dashboard validation..."
          python validate_mcp_dashboard.py
          echo "validation_result=success" >> $GITHUB_OUTPUT
        continue-on-error: true
      
      - name: Smoke Test Summary
        run: |
          echo "## ðŸ’¨ Smoke Test Results" >> $GITHUB_STEP_SUMMARY
          echo "" >> $GITHUB_STEP_SUMMARY
          echo "- **OS**: ${{ matrix.os }}" >> $GITHUB_STEP_SUMMARY
          echo "- **Python**: ${{ matrix.python-version }}" >> $GITHUB_STEP_SUMMARY
          echo "- **Validation**: ${{ steps.validation.outputs.validation_result || 'failed' }}" >> $GITHUB_STEP_SUMMARY

  # Comprehensive dashboard testing
  dashboard-comprehensive-tests:
    runs-on: ubuntu-latest
    needs: dashboard-smoke-tests
    if: ${{ !cancelled() && (github.event.inputs.test_mode == 'comprehensive' || github.event_name != 'workflow_dispatch') }}
    
    steps:
      - name: Checkout repository
        uses: actions/checkout@v4
        with:
          submodules: false
      
      - name: Set up Docker Buildx
        uses: docker/setup-buildx-action@v3
      
      - name: Build MCP Test Docker Image
        run: |
          docker build -f Dockerfile.mcp-tests -t ipfs-datasets-mcp-tests:latest .
      
      - name: Run MCP Dashboard Tests in Docker
        run: |
          mkdir -p test-results
          # Run the container with dashboard service
          docker run --rm -d \
            --name mcp-dashboard-test \
            -p 8899:8899 \
            -v $(pwd)/test-results:/app/test-results \
            ipfs-datasets-mcp-tests:latest \
            python -m ipfs_datasets_py.mcp_dashboard
          
          # Wait for dashboard to be ready
          for i in {1..30}; do
            if curl -f http://127.0.0.1:8899/api/health 2>/dev/null; then
              echo "âœ… Dashboard is ready!"
              break
            fi
            echo "â³ Waiting for dashboard... ($i/30)"
            sleep 2
          done
          
          # Run tests against the dashboard
          docker exec mcp-dashboard-test pytest tests/ -v \
            --cov=ipfs_datasets_py \
            --cov-report=xml:/app/test-results/coverage.xml \
            --junit-xml=/app/test-results/mcp-test-results.xml || true
          
          # Stop the container
          docker stop mcp-dashboard-test
      
      - name: Upload Test Results
        uses: actions/upload-artifact@v4
        if: always()
        with:
          name: mcp-comprehensive-test-results
          path: test-results/
                      'success': response.status_code == 200,
                      'name': name
                  }
                  print(f'âœ… {name}: {response.status_code}')
              except Exception as e:
                  results[endpoint] = {
                      'status': 'error',
                      'success': False,
                      'error': str(e),
                      'name': name
                  }
                  print(f'âŒ {name}: {e}')
          
          # Save results
          with open('endpoint_test_results.json', 'w') as f:
              json.dump(results, f, indent=2)
          
          # Check if critical endpoints work
          critical_endpoints = ['/api/mcp/status', '/']
          failures = [ep for ep in critical_endpoints if not results.get(ep, {}).get('success', False)]
          
          if failures:
              print(f'âŒ Critical endpoints failed: {failures}')
              sys.exit(1)
          else:
              print('âœ… All critical endpoints working')
          "
      
      - name: Test MCP Tools Loading
        run: |
          echo "ðŸ”§ Testing MCP tools loading..."
          python -c "
          import requests
          import json
          
          try:
              response = requests.get('http://127.0.0.1:8899/api/mcp/tools', timeout=15)
              if response.status_code == 200:
                  tools = response.json()
                  print(f'âœ… Loaded {len(tools)} MCP tools')
                  
                  # Test some tools
                  sample_tools = list(tools.keys())[:5]
                  for tool in sample_tools:
                      print(f'  - {tool}: {tools[tool].get(\"description\", \"No description\")}')
              else:
                  print(f'âš ï¸ Tools API returned {response.status_code}')
          except Exception as e:
              print(f'âŒ Error testing tools: {e}')
          "
      
      - name: Run Playwright Browser Tests
        run: |
          echo "ðŸŒ Running browser-based tests..."
          python -c "
          from playwright.sync_api import sync_playwright
          import json
          import time
          
          def test_dashboard_ui():
              with sync_playwright() as p:
                  # Test multiple browsers
                  browsers = [p.chromium, p.firefox]
                  results = {}
                  
                  for browser_type in browsers:
                      browser_name = browser_type.name
                      print(f'Testing with {browser_name}...')
                      
                      try:
                          browser = browser_type.launch(headless=True)
                          page = browser.new_page()
                          
                          # Test main dashboard page
                          page.goto('http://127.0.0.1:8899', wait_until='networkidle')
                          
                          # Check if page loaded
                          title = page.title()
                          print(f'  Page title: {title}')
                          
                          # Look for key elements
                          elements = {
                              'header': page.locator('h1, .header, [role=\"banner\"]').count(),
                              'navigation': page.locator('nav, .nav, .navigation').count(),
                              'content': page.locator('main, .main, .content').count(),
                              'forms': page.locator('form').count(),
                              'buttons': page.locator('button').count()
                          }
                          
                          results[browser_name] = {
                              'title': title,
                              'elements': elements,
                              'success': True,
                              'load_time': time.time()
                          }
                          
                          print(f'  âœ… {browser_name} test passed')
                          browser.close()
                          
                      except Exception as e:
                          results[browser_name] = {
                              'success': False,
                              'error': str(e)
                          }
                          print(f'  âŒ {browser_name} test failed: {e}')
                          try:
                              browser.close()
                          except:
                              pass
                  
                  # Save results
                  with open('browser_test_results.json', 'w') as f:
                      json.dump(results, f, indent=2)
                  
                  return results
          
          test_results = test_dashboard_ui()
          successful_browsers = sum(1 for r in test_results.values() if r.get('success', False))
          print(f'Browser tests: {successful_browsers}/{len(test_results)} successful')
          "
      
      - name: Performance Tests
        if: ${{ github.event.inputs.test_mode == 'performance' || github.event_name == 'schedule' }}
        run: |
          echo "âš¡ Running performance tests..."
          python -c "
          import requests
          import time
          import statistics
          
          def measure_endpoint_performance():
              endpoints = [
                  'http://127.0.0.1:8899/api/mcp/status',
                  'http://127.0.0.1:8899/',
                  'http://127.0.0.1:8899/api/mcp/tools'
              ]
              
              results = {}
              for endpoint in endpoints:
                  times = []
                  for i in range(10):  # 10 requests per endpoint
                      start = time.time()
                      try:
                          response = requests.get(endpoint, timeout=5)
                          end = time.time()
                          if response.status_code == 200:
                              times.append(end - start)
                      except:
                          pass
                  
                  if times:
                      results[endpoint] = {
                          'avg_response_time': statistics.mean(times),
                          'min_response_time': min(times),
                          'max_response_time': max(times),
                          'requests_tested': len(times)
                      }
                      print(f'{endpoint}: avg={statistics.mean(times):.3f}s, min={min(times):.3f}s, max={max(times):.3f}s')
                  
              return results
          
          perf_results = measure_endpoint_performance()
          print('Performance test completed')
          "
      
      - name: Stop Dashboard
        if: always()
        run: |
          if [ ! -z \"$MCP_DASHBOARD_PID\" ]; then
            echo \"ðŸ›‘ Stopping MCP Dashboard (PID: $MCP_DASHBOARD_PID)...\"
            kill $MCP_DASHBOARD_PID || true
            sleep 2
          fi
          
          # Cleanup any remaining processes
          pkill -f \"mcp_dashboard\" || true
          pkill -f \"ipfs_datasets_py.mcp_dashboard\" || true
      
      - name: Upload Test Results
        uses: actions/upload-artifact@v4
        if: always()
        with:
          name: dashboard-comprehensive-test-results
          path: |
            endpoint_test_results.json
            browser_test_results.json
            *.log

  # Self-hosted runner tests (x86_64 and ARM64)
  dashboard-self-hosted-tests:
    runs-on: ${{ matrix.runner }}
    needs: dashboard-smoke-tests
    if: ${{ !cancelled() }}
    continue-on-error: true
    strategy:
      matrix:
        runner: 
          - [self-hosted, linux, x64]
          - [self-hosted, linux, arm64]
      fail-fast: false
    
    steps:
      - name: Checkout repository
        uses: actions/checkout@v4
      with:
        submodules: false
      
      - name: System Info
        run: |
          echo "ðŸ–¥ï¸ Testing on $(uname -m) architecture"
          echo "OS: $(cat /etc/os-release | grep PRETTY_NAME | cut -d= -f2)"
          echo "Python: $(python3 --version)"
      
      - name: Set up environment
        run: |
          python3 -m venv .venv-dashboard-test
          source .venv-dashboard-test/bin/activate
          pip install --upgrade pip
          pip install -e .[test]
          pip install flask mcp
      
      - name: Run Architecture-Specific Tests
        run: |
          source .venv-dashboard-test/bin/activate
          echo "ðŸ§ª Running dashboard tests on $(uname -m)..."
          
          # Run validation with architecture info
          python -c "
          import platform
          import subprocess
          import sys
          
          print(f'Architecture: {platform.machine()}')
          print(f'Platform: {platform.platform()}')
          
          # Run validation
          result = subprocess.run([sys.executable, 'validate_mcp_dashboard.py'], 
                                capture_output=True, text=True)
          
          print('STDOUT:', result.stdout)
          if result.stderr:
              print('STDERR:', result.stderr)
          
          if result.returncode == 0:
              print(f'âœ… Dashboard validation passed on {platform.machine()}')
          else:
              print(f'âŒ Dashboard validation failed on {platform.machine()}')
              sys.exit(1)
          "

  # Generate comprehensive test report
  dashboard-test-summary:
    runs-on: ubuntu-latest
    needs: [dashboard-smoke-tests, dashboard-comprehensive-tests, dashboard-self-hosted-tests]
    if: always()
    
    steps:
      - name: Download Test Results
        uses: actions/download-artifact@v4
        with:
          name: dashboard-comprehensive-test-results
          path: test_results/
        continue-on-error: true
      
      - name: Generate Test Summary
        run: |
          echo "## ðŸŽ›ï¸ MCP Dashboard Test Results" >> $GITHUB_STEP_SUMMARY
          echo "" >> $GITHUB_STEP_SUMMARY
          echo "### Test Suite Results" >> $GITHUB_STEP_SUMMARY
          echo "" >> $GITHUB_STEP_SUMMARY
          echo "| Test Suite | Status | Details |" >> $GITHUB_STEP_SUMMARY
          echo "|------------|--------|---------|" >> $GITHUB_STEP_SUMMARY
          echo "| Smoke Tests | ${{ needs.dashboard-smoke-tests.result }} | Basic functionality across Python versions |" >> $GITHUB_STEP_SUMMARY
          echo "| Comprehensive Tests | ${{ needs.dashboard-comprehensive-tests.result }} | Full endpoint and UI testing |" >> $GITHUB_STEP_SUMMARY
          echo "| Self-Hosted Tests | ${{ needs.dashboard-self-hosted-tests.result }} | Multi-architecture validation |" >> $GITHUB_STEP_SUMMARY
          echo "" >> $GITHUB_STEP_SUMMARY
          
          # Add recommendations
          echo "### Recommendations" >> $GITHUB_STEP_SUMMARY
          echo "" >> $GITHUB_STEP_SUMMARY
          
          if [ "${{ needs.dashboard-smoke-tests.result }}" = "success" ]; then
            echo "âœ… Basic dashboard functionality is working correctly" >> $GITHUB_STEP_SUMMARY
          else
            echo "âš ï¸ Smoke tests failed - check basic setup and dependencies" >> $GITHUB_STEP_SUMMARY
          fi
          
          if [ "${{ needs.dashboard-comprehensive-tests.result }}" = "success" ]; then
            echo "âœ… Comprehensive testing passed - dashboard is production ready" >> $GITHUB_STEP_SUMMARY
          else
            echo "âš ï¸ Comprehensive tests failed - review endpoint functionality" >> $GITHUB_STEP_SUMMARY
          fi
          
          if [ "${{ needs.dashboard-self-hosted-tests.result }}" = "success" ]; then
            echo "âœ… Multi-architecture support confirmed" >> $GITHUB_STEP_SUMMARY
          else
            echo "âš ï¸ Self-hosted runner tests had issues - may be expected if runners not available" >> $GITHUB_STEP_SUMMARY
          fi
          
          echo "" >> $GITHUB_STEP_SUMMARY
          echo "### Next Steps" >> $GITHUB_STEP_SUMMARY
          echo "- Monitor dashboard performance in production" >> $GITHUB_STEP_SUMMARY
          echo "- Set up automated monitoring and alerting" >> $GITHUB_STEP_SUMMARY
          echo "- Consider adding more comprehensive integration tests" >> $GITHUB_STEP_SUMMARY