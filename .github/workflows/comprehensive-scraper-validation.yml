name: Comprehensive Scraper Validation with HuggingFace Schema Check
true:
  workflow_dispatch:
    inputs:
      domain:
        description: Domain to test (all, caselaw, finance, medicine, software)
        required: true
        default: all
        type: choice
        options:
        - all
        - caselaw
        - finance
        - medicine
        - software
  schedule:
  - cron: 0 3 * * 0
  push:
    branches:
    - main
    - develop
    paths:
    - ipfs_datasets_py/mcp_server/tools/**/*scraper*.py
    - tests/scraper_tests/**
    - comprehensive_scraper_validation.py
concurrency:
  group: ${{ github.workflow }}-${{ github.ref }}
  cancel-in-progress: true
permissions:
  contents: read
  issues: write
  pull-requests: write
env:
  PYTHON_VERSION: '3.12'
jobs:
  comprehensive-validation:
    name: Validate Scrapers on Self-Hosted Runner
    runs-on:
    - self-hosted
    - linux
    - x64
    timeout-minutes: 45
    container:
      image: python:3.12-slim
      options: --user root
    steps:
    - name: Configure git for container
      run: 'git config --global --add safe.directory ''*''

        '
    - name: Install system dependencies
      run: 'apt-get update

        apt-get install -y git curl

        rm -rf /var/lib/apt/lists/*

        '
    - name: Checkout repository
      uses: actions/checkout@v5
      with:
        fetch-depth: 0
        token: ${{ secrets.GITHUB_TOKEN }}
    - name: Install Python dependencies
      run: 'python --version

        pip install --upgrade pip

        pip install -r requirements.txt || echo "No requirements.txt, installing minimal deps"

        pip install pytest pytest-asyncio pytest-timeout

        pip install datasets  # HuggingFace datasets library

        '
    - name: Run Comprehensive Validation
      id: validation
      run: "echo \"\U0001F9EA Running comprehensive scraper validation...\"\necho \"Validating: Execution, Schema, HuggingFace\
        \ compatibility\"\necho \"\"\n\npython comprehensive_scraper_validation.py > validation_output.txt 2>&1 || true\n\n\
        cat validation_output.txt\n\n# Check if validation passed\nif grep -q \"ALL VALIDATIONS PASSED\" validation_output.txt;\
        \ then\n  echo \"validation_status=passed\" >> $GITHUB_OUTPUT\nelse\n  echo \"validation_status=failed\" >> $GITHUB_OUTPUT\n\
        fi\n"
    - name: Parse Results
      if: always()
      run: "echo \"## \U0001F4CA Comprehensive Validation Results\" >> $GITHUB_STEP_SUMMARY\necho \"\" >> $GITHUB_STEP_SUMMARY\n\
        \nif [ -f \"validation_results/comprehensive_validation_report.json\" ]; then\n  python3 << 'PYTHON'\nimport json\n\
        import sys\n\nwith open('validation_results/comprehensive_validation_report.json') as f:\n    report = json.load(f)\n\
        \nprint(f\"**Total Scrapers**: {report['total_scrapers']}\")\nprint(f\"**Passed**: \u2705 {report['passed']}\")\n\
        print(f\"**Failed**: \u274C {report['failed']}\")\nprint(\"\")\nprint(\"### Detailed Results\")\nprint(\"\")\n\nfor\
        \ result in report['results']:\n    status = \"\u2705\" if (result['execution_success'] and result['schema_valid']\
        \ and result['hf_compatible'] and result['quality_score'] >= 50) else \"\u274C\"\n    print(f\"{status} **{result['scraper_name']}**\
        \ ({result['domain']})\")\n    print(f\"  - Records: {result['record_count']}\")\n    print(f\"  - Quality Score:\
        \ {result['quality_score']:.1f}/100\")\n    print(f\"  - Execution: {'\u2705' if result['execution_success'] else\
        \ '\u274C'}\")\n    print(f\"  - Schema Valid: {'\u2705' if result['schema_valid'] else '\u274C'}\")\n    print(f\"\
        \  - HF Compatible: {'\u2705' if result['hf_compatible'] else '\u274C'}\")\n    \n    if result['schema_issues']:\n\
        \        print(f\"  - Schema Issues: {', '.join(result['schema_issues'])}\")\n    if result['hf_issues']:\n      \
        \  print(f\"  - HF Issues: {', '.join(result['hf_issues'])}\")\n    if result['error']:\n        print(f\"  - Error:\
        \ {result['error']}\")\n    print(\"\")\nPYTHON\nfi >> $GITHUB_STEP_SUMMARY\n"
    - name: Test HuggingFace Dataset Creation
      if: always()
      run: "echo \"## \U0001F917 HuggingFace Dataset Compatibility Test\" >> $GITHUB_STEP_SUMMARY\necho \"\" >> $GITHUB_STEP_SUMMARY\n\
        \npython3 << 'PYTHON'\nimport json\nimport sys\n\ntry:\n    from datasets import Dataset\n    \n    # Try to load\
        \ sample data and create dataset\n    with open('validation_results/comprehensive_validation_report.json') as f:\n\
        \        report = json.load(f)\n    \n    for result in report['results']:\n        if result['sample_records']:\n\
        \            try:\n                ds = Dataset.from_list(result['sample_records'])\n                print(f\"\u2705\
        \ {result['scraper_name']}: Created HF dataset with {len(ds)} records\")\n                print(f\"   Schema: {ds.features}\"\
        )\n            except Exception as e:\n                print(f\"\u274C {result['scraper_name']}: Failed to create\
        \ HF dataset: {str(e)}\")\nexcept ImportError:\n    print(\"\u26A0\uFE0F  HuggingFace datasets library not available\"\
        )\nexcept FileNotFoundError:\n    print(\"\u26A0\uFE0F  No validation results found\")\nexcept Exception as e:\n \
        \   print(f\"\u274C Error: {str(e)}\")\nPYTHON\n"
    - name: Upload Validation Results
      if: always()
      uses: actions/upload-artifact@v4
      with:
        name: comprehensive-validation-results
        path: 'validation_results/

          validation_output.txt

          '
        retention-days: 30
    - name: Create Issue and Draft PR on Failure
      id: create_issue_pr
      if: steps.validation.outputs.validation_status == 'failed'
      env:
        GH_TOKEN: ${{ secrets.GITHUB_TOKEN }}
      run: "# Read validation summary\nif [ -f \"validation_results/validation_summary.txt\" ]; then\n  SUMMARY=$(cat validation_results/validation_summary.txt)\n\
        else\n  SUMMARY=\"Validation summary not available\"\nfi\n\n# Create issue body\ncat > /tmp/issue_body.md << 'ISSUE_EOF'\n\
        ## \U0001F6A8 Scraper Validation Failed\n\n**Workflow Run**: [${{ github.run_number }}](${{ github.server_url }}/${{\
        \ github.repository }}/actions/runs/${{ github.run_id }})\n\n### Summary\n\n```\nISSUE_EOF\n\necho \"$SUMMARY\" >>\
        \ /tmp/issue_body.md\n\ncat >> /tmp/issue_body.md << 'ISSUE_EOF'\n```\n\n### Action Required\n\n1. Review the validation\
        \ report artifacts\n2. Fix scrapers that are failing validation\n3. Ensure data schemas match HuggingFace dataset\
        \ requirements\n4. Re-run validation to confirm fixes\n\n### Related Files\n\n- `comprehensive_scraper_validation.py`:\
        \ Validation script\n- `validation_results/`: Detailed results (download artifacts)\n\n### Detailed Report\n\nISSUE_EOF\n\
        \n# Add detailed report if available\nif [ -f \"validation_results/comprehensive_validation_report.json\" ]; then\n\
        \  echo '```json' >> /tmp/issue_body.md\n  head -100 validation_results/comprehensive_validation_report.json >> /tmp/issue_body.md\n\
        \  echo '```' >> /tmp/issue_body.md\nfi\n\ncat >> /tmp/issue_body.md << 'ISSUE_EOF'\n\n---\n\n\U0001F916 **Auto-Healing**:\
        \ A draft PR will be created automatically by GitHub Copilot.\nISSUE_EOF\n\n# Check if issue already exists\nEXISTING_ISSUE=$(gh\
        \ issue list --label \"scraper-validation,automated\" --state open --json number,title --jq '.[] | select(.title |\
        \ contains(\"Scraper Validation Failed\")) | .number' | head -1)\n\nif [ -n \"$EXISTING_ISSUE\" ]; then\n  echo \"\
        Updating existing issue #$EXISTING_ISSUE\"\n  gh issue comment \"$EXISTING_ISSUE\" --body-file /tmp/issue_body.md\n\
        \  ISSUE_NUMBER=\"$EXISTING_ISSUE\"\nelse\n  echo \"Creating new issue\"\n  ISSUE_URL=$(gh issue create \\\n    --title\
        \ \"\U0001F6A8 Scraper Validation Failed - Action Required\" \\\n    --body-file /tmp/issue_body.md \\\n    --label\
        \ \"scraper-validation,automated,bug\")\n  ISSUE_NUMBER=$(echo \"$ISSUE_URL\" | grep -oP '\\d+$')\n  echo \"Created\
        \ issue #$ISSUE_NUMBER\"\nfi\n\necho \"issue_number=$ISSUE_NUMBER\" >> $GITHUB_OUTPUT\n\n# Create a new branch for\
        \ the fix\nBRANCH_NAME=\"autofix/scraper-validation-$(date +%Y%m%d-%H%M%S)\"\ngit config user.name \"github-actions[bot]\"\
        \ngit config user.email \"41898282+github-actions[bot]@users.noreply.github.com\"\ngit checkout -b \"$BRANCH_NAME\"\
        \n\n# Create a placeholder commit to enable PR creation\necho \"# Scraper Validation Auto-Fix\" > /tmp/AUTOFIX_README.md\n\
        echo \"\" >> /tmp/AUTOFIX_README.md\necho \"This branch was created automatically to fix scraper validation failures.\"\
        \ >> /tmp/AUTOFIX_README.md\necho \"Issue: #$ISSUE_NUMBER\" >> /tmp/AUTOFIX_README.md\necho \"Run ID: ${{ github.run_id\
        \ }}\" >> /tmp/AUTOFIX_README.md\ncp /tmp/AUTOFIX_README.md .\ngit add AUTOFIX_README.md\ngit commit -m \"autofix:\
        \ Initialize branch for scraper validation fixes\n\nThis commit creates a branch for automated fixes to address scraper\n\
        validation failures.\n\nRelated Issue: #$ISSUE_NUMBER\nWorkflow Run: ${{ github.run_id }}\"\n\ngit push origin \"\
        $BRANCH_NAME\"\necho \"branch_name=$BRANCH_NAME\" >> $GITHUB_OUTPUT\n\n# Create draft PR\ncat > /tmp/pr_body.md <<\
        \ 'PR_EOF'\n## \U0001F916 Automated Fix for Scraper Validation Failures\n\nThis PR addresses the scraper validation\
        \ failures identified in issue #ISSUE_NUMBER.\n\n### Related Issue\n\nFixes #ISSUE_NUMBER\n\n### Failure Summary\n\
        \n```\nPR_EOF\n\necho \"$SUMMARY\" >> /tmp/pr_body.md\n\ncat >> /tmp/pr_body.md << 'PR_EOF'\n```\n\n### What This\
        \ PR Does\n\nThis is a draft PR that will be automatically updated by GitHub Copilot to fix the validation failures.\n\
        \nGitHub Copilot will be invoked using the VERIFIED working method (draft PR + @copilot trigger).\nSee COPILOT_INVOCATION_GUIDE.md\
        \ for details on the invocation method.\n\nCopilot will implement fixes for:\n\n1. Schema validation issues for scrapers\n\
        2. Missing required fields (title, text) in scraper output\n3. HuggingFace dataset compatibility\n4. Data quality\
        \ improvements\n\n### Next Steps\n\n- [ ] GitHub Copilot will analyze the failures\n- [ ] Copilot will implement fixes\n\
        - [ ] Automated tests will verify the fixes\n- [ ] Manual review and approval\n\n---\n\n\U0001F916 **Auto-generated\
        \ by**: Comprehensive Scraper Validation Workflow\n**Run**: [${{ github.run_number }}](${{ github.server_url }}/${{\
        \ github.repository }}/actions/runs/${{ github.run_id }})\nPR_EOF\n\n# Replace ISSUE_NUMBER placeholder\nsed -i \"\
        s/#ISSUE_NUMBER/#$ISSUE_NUMBER/g\" /tmp/pr_body.md\n\nPR_URL=$(gh pr create \\\n  --draft \\\n  --title \"\U0001F527\
        \ Fix scraper validation failures (Issue #$ISSUE_NUMBER)\" \\\n  --body-file /tmp/pr_body.md \\\n  --base main \\\n\
        \  --head \"$BRANCH_NAME\" \\\n  --label \"automated-fix,scraper-validation,copilot-ready\")\n\necho \"pr_url=$PR_URL\"\
        \ >> $GITHUB_OUTPUT\nPR_NUMBER=$(echo \"$PR_URL\" | grep -oP '\\d+$')\necho \"pr_number=$PR_NUMBER\" >> $GITHUB_OUTPUT\n\
        \n# Invoke Copilot using the VERIFIED working method (draft PR + @copilot trigger)\n# See COPILOT_INVOCATION_GUIDE.md\
        \ for details\necho \"Invoking GitHub Copilot on PR #$PR_NUMBER using verified dual method...\"\n\n# Build task description\
        \ with validation context\nTASK_DESC=\"Please analyze the scraper validation failures and implement fixes. Focus on:\
        \ (1) Schema validation issues, (2) Missing required fields (title, text), (3) HuggingFace compatibility, (4) Data\
        \ quality improvements.\"\n\n# Check if validation report exists and include summary\nif [ -f \"validation_results/comprehensive_validation_report.json\"\
        \ ]; then\n  echo \"Including validation report context...\"\n  REPORT_SUMMARY=$(python3 -c \"import json; r=json.load(open('validation_results/comprehensive_validation_report.json'));\
        \ print(f'Validation: {r.get(\\\"total_scrapers\\\",\\\"?\\\")}} scrapers, {r.get(\\\"failed_scrapers\\\",\\\"?\\\"\
        )} failed')\" 2>/dev/null || echo \"\")\n  TASK_DESC=\"$TASK_DESC $REPORT_SUMMARY\"\nfi\n\n# Invoke using invoke_copilot_on_pr.py\
        \ (uses draft PR + @copilot trigger method)\npython3 scripts/invoke_copilot_on_pr.py \\\n  --pr \"$PR_NUMBER\" \\\n\
        \  --task \"$TASK_DESC\" \\\n  --repo ${{ github.repository }} || echo \"\u26A0\uFE0F  Copilot invocation failed,\
        \ check logs\"\n\necho \"\u2705 Created issue #$ISSUE_NUMBER and draft PR #$PR_NUMBER\"\necho \"\" >> $GITHUB_STEP_SUMMARY\n\
        echo \"## \U0001F916 Auto-Healing Initiated\" >> $GITHUB_STEP_SUMMARY\necho \"\" >> $GITHUB_STEP_SUMMARY\necho \"\
        - **Issue**: #$ISSUE_NUMBER\" >> $GITHUB_STEP_SUMMARY\necho \"- **Draft PR**: #$PR_NUMBER\" >> $GITHUB_STEP_SUMMARY\n\
        echo \"- **Branch**: \\`$BRANCH_NAME\\`\" >> $GITHUB_STEP_SUMMARY\necho \"- **Status**: GitHub Copilot invoked using\
        \ verified dual method (draft PR + @copilot trigger)\" >> $GITHUB_STEP_SUMMARY\necho \"- **Method**: See COPILOT_INVOCATION_GUIDE.md\
        \ for details\" >> $GITHUB_STEP_SUMMARY\n"
    - name: Final Summary
      if: always()
      run: "echo \"\" >> $GITHUB_STEP_SUMMARY\necho \"---\" >> $GITHUB_STEP_SUMMARY\necho \"\" >> $GITHUB_STEP_SUMMARY\necho\
        \ \"### Validation Complete\" >> $GITHUB_STEP_SUMMARY\necho \"\" >> $GITHUB_STEP_SUMMARY\necho \"- **Status**: ${{\
        \ steps.validation.outputs.validation_status }}\" >> $GITHUB_STEP_SUMMARY\necho \"- **Platform**: Self-hosted runner\
        \ with Docker\" >> $GITHUB_STEP_SUMMARY\necho \"- **Container**: python:3.12-slim\" >> $GITHUB_STEP_SUMMARY\necho\
        \ \"- **HuggingFace**: Datasets library used for validation\" >> $GITHUB_STEP_SUMMARY\necho \"\" >> $GITHUB_STEP_SUMMARY\n\
        \nif [ \"${{ steps.validation.outputs.validation_status }}\" = \"passed\" ]; then\n  echo \"\u2705 **All scrapers\
        \ validated successfully**\" >> $GITHUB_STEP_SUMMARY\n  echo \"\" >> $GITHUB_STEP_SUMMARY\n  echo \"Scrapers are:\"\
        \ >> $GITHUB_STEP_SUMMARY\n  echo \"- Executing correctly\" >> $GITHUB_STEP_SUMMARY\n  echo \"- Producing valid data\"\
        \ >> $GITHUB_STEP_SUMMARY\n  echo \"- Compatible with HuggingFace datasets\" >> $GITHUB_STEP_SUMMARY\n  echo \"- Meeting\
        \ schema requirements\" >> $GITHUB_STEP_SUMMARY\nelse\n  echo \"\u274C **Some validations failed**\" >> $GITHUB_STEP_SUMMARY\n\
        \  echo \"\" >> $GITHUB_STEP_SUMMARY\n  echo \"Please review the artifacts and fix failing scrapers.\" >> $GITHUB_STEP_SUMMARY\n\
        fi\n"
