name: Comprehensive Scraper Validation with HuggingFace Schema Check

on:
  workflow_dispatch:
    inputs:
      domain:
        description: 'Domain to test (all, caselaw, finance, medicine, software)'
        required: true
        default: 'all'
        type: choice
        options:
          - all
          - caselaw
          - finance
          - medicine
          - software
  schedule:
    # Run weekly on Sundays at 3 AM UTC
    - cron: '0 3 * * 0'
  push:
    branches: [ main, develop ]
    paths:
      - 'ipfs_datasets_py/mcp_server/tools/**/*scraper*.py'
      - 'tests/scraper_tests/**'
      - 'comprehensive_scraper_validation.py'

permissions:
  contents: read
  issues: write
  pull-requests: write

env:
  PYTHON_VERSION: '3.12'

jobs:
  comprehensive-validation:
    name: Validate Scrapers on Self-Hosted Runner
    runs-on: [self-hosted, linux, x64]
    
    container:
      image: python:3.12-slim
      options: --user root
    
    steps:
      - name: Configure git for container
        run: |
          git config --global --add safe.directory '*'
      
      - name: Install system dependencies
        run: |
          apt-get update
          apt-get install -y git curl
          rm -rf /var/lib/apt/lists/*
      
      - name: Checkout repository
        uses: actions/checkout@v4
        with:
          fetch-depth: 0  # Full history for branch creation
          token: ${{ secrets.GITHUB_TOKEN }}
      
      - name: Install Python dependencies
        run: |
          python --version
          pip install --upgrade pip
          pip install -r requirements.txt || echo "No requirements.txt, installing minimal deps"
          pip install pytest pytest-asyncio pytest-timeout
          pip install datasets  # HuggingFace datasets library
      
      - name: Run Comprehensive Validation
        id: validation
        run: |
          echo "🧪 Running comprehensive scraper validation..."
          echo "Validating: Execution, Schema, HuggingFace compatibility"
          echo ""
          
          python comprehensive_scraper_validation.py > validation_output.txt 2>&1 || true
          
          cat validation_output.txt
          
          # Check if validation passed
          if grep -q "ALL VALIDATIONS PASSED" validation_output.txt; then
            echo "validation_status=passed" >> $GITHUB_OUTPUT
          else
            echo "validation_status=failed" >> $GITHUB_OUTPUT
          fi
      
      - name: Parse Results
        if: always()
        run: |
          echo "## 📊 Comprehensive Validation Results" >> $GITHUB_STEP_SUMMARY
          echo "" >> $GITHUB_STEP_SUMMARY
          
          if [ -f "validation_results/comprehensive_validation_report.json" ]; then
            python3 << 'PYTHON'
          import json
          import sys
          
          with open('validation_results/comprehensive_validation_report.json') as f:
              report = json.load(f)
          
          print(f"**Total Scrapers**: {report['total_scrapers']}")
          print(f"**Passed**: ✅ {report['passed']}")
          print(f"**Failed**: ❌ {report['failed']}")
          print("")
          print("### Detailed Results")
          print("")
          
          for result in report['results']:
              status = "✅" if (result['execution_success'] and result['schema_valid'] and result['hf_compatible'] and result['quality_score'] >= 50) else "❌"
              print(f"{status} **{result['scraper_name']}** ({result['domain']})")
              print(f"  - Records: {result['record_count']}")
              print(f"  - Quality Score: {result['quality_score']:.1f}/100")
              print(f"  - Execution: {'✅' if result['execution_success'] else '❌'}")
              print(f"  - Schema Valid: {'✅' if result['schema_valid'] else '❌'}")
              print(f"  - HF Compatible: {'✅' if result['hf_compatible'] else '❌'}")
              
              if result['schema_issues']:
                  print(f"  - Schema Issues: {', '.join(result['schema_issues'])}")
              if result['hf_issues']:
                  print(f"  - HF Issues: {', '.join(result['hf_issues'])}")
              if result['error']:
                  print(f"  - Error: {result['error']}")
              print("")
          PYTHON
          fi >> $GITHUB_STEP_SUMMARY
      
      - name: Test HuggingFace Dataset Creation
        if: always()
        run: |
          echo "## 🤗 HuggingFace Dataset Compatibility Test" >> $GITHUB_STEP_SUMMARY
          echo "" >> $GITHUB_STEP_SUMMARY
          
          python3 << 'PYTHON'
          import json
          import sys
          
          try:
              from datasets import Dataset
              
              # Try to load sample data and create dataset
              with open('validation_results/comprehensive_validation_report.json') as f:
                  report = json.load(f)
              
              for result in report['results']:
                  if result['sample_records']:
                      try:
                          ds = Dataset.from_list(result['sample_records'])
                          print(f"✅ {result['scraper_name']}: Created HF dataset with {len(ds)} records")
                          print(f"   Schema: {ds.features}")
                      except Exception as e:
                          print(f"❌ {result['scraper_name']}: Failed to create HF dataset: {str(e)}")
          except ImportError:
              print("⚠️  HuggingFace datasets library not available")
          except FileNotFoundError:
              print("⚠️  No validation results found")
          except Exception as e:
              print(f"❌ Error: {str(e)}")
          PYTHON
          
      - name: Upload Validation Results
        if: always()
        uses: actions/upload-artifact@v4
        with:
          name: comprehensive-validation-results
          path: |
            validation_results/
            validation_output.txt
          retention-days: 30
      
      - name: Create Issue and Draft PR on Failure
        id: create_issue_pr
        if: steps.validation.outputs.validation_status == 'failed'
        env:
          GH_TOKEN: ${{ secrets.GITHUB_TOKEN }}
        run: |
          # Read validation summary
          if [ -f "validation_results/validation_summary.txt" ]; then
            SUMMARY=$(cat validation_results/validation_summary.txt)
          else
            SUMMARY="Validation summary not available"
          fi
          
          # Create issue body
          cat > /tmp/issue_body.md << 'ISSUE_EOF'
          ## 🚨 Scraper Validation Failed
          
          **Workflow Run**: [${{ github.run_number }}](${{ github.server_url }}/${{ github.repository }}/actions/runs/${{ github.run_id }})
          
          ### Summary
          
          ```
          ISSUE_EOF
          
          echo "$SUMMARY" >> /tmp/issue_body.md
          
          cat >> /tmp/issue_body.md << 'ISSUE_EOF'
          ```
          
          ### Action Required
          
          1. Review the validation report artifacts
          2. Fix scrapers that are failing validation
          3. Ensure data schemas match HuggingFace dataset requirements
          4. Re-run validation to confirm fixes
          
          ### Related Files
          
          - `comprehensive_scraper_validation.py`: Validation script
          - `validation_results/`: Detailed results (download artifacts)
          
          ### Detailed Report
          
          ISSUE_EOF
          
          # Add detailed report if available
          if [ -f "validation_results/comprehensive_validation_report.json" ]; then
            echo '```json' >> /tmp/issue_body.md
            head -100 validation_results/comprehensive_validation_report.json >> /tmp/issue_body.md
            echo '```' >> /tmp/issue_body.md
          fi
          
          cat >> /tmp/issue_body.md << 'ISSUE_EOF'
          
          ---
          
          🤖 **Auto-Healing**: A draft PR will be created automatically by GitHub Copilot.
          ISSUE_EOF
          
          # Check if issue already exists
          EXISTING_ISSUE=$(gh issue list --label "scraper-validation,automated" --state open --json number,title --jq '.[] | select(.title | contains("Scraper Validation Failed")) | .number' | head -1)
          
          if [ -n "$EXISTING_ISSUE" ]; then
            echo "Updating existing issue #$EXISTING_ISSUE"
            gh issue comment "$EXISTING_ISSUE" --body-file /tmp/issue_body.md
            ISSUE_NUMBER="$EXISTING_ISSUE"
          else
            echo "Creating new issue"
            ISSUE_URL=$(gh issue create \
              --title "🚨 Scraper Validation Failed - Action Required" \
              --body-file /tmp/issue_body.md \
              --label "scraper-validation,automated,bug")
            ISSUE_NUMBER=$(echo "$ISSUE_URL" | grep -oP '\d+$')
            echo "Created issue #$ISSUE_NUMBER"
          fi
          
          echo "issue_number=$ISSUE_NUMBER" >> $GITHUB_OUTPUT
          
          # Create a new branch for the fix
          BRANCH_NAME="autofix/scraper-validation-$(date +%Y%m%d-%H%M%S)"
          git config user.name "github-actions[bot]"
          git config user.email "41898282+github-actions[bot]@users.noreply.github.com"
          git checkout -b "$BRANCH_NAME"
          
          # Create a placeholder commit to enable PR creation
          echo "# Scraper Validation Auto-Fix" > /tmp/AUTOFIX_README.md
          echo "" >> /tmp/AUTOFIX_README.md
          echo "This branch was created automatically to fix scraper validation failures." >> /tmp/AUTOFIX_README.md
          echo "Issue: #$ISSUE_NUMBER" >> /tmp/AUTOFIX_README.md
          echo "Run ID: ${{ github.run_id }}" >> /tmp/AUTOFIX_README.md
          cp /tmp/AUTOFIX_README.md .
          git add AUTOFIX_README.md
          git commit -m "autofix: Initialize branch for scraper validation fixes

          This commit creates a branch for automated fixes to address scraper
          validation failures.
          
          Related Issue: #$ISSUE_NUMBER
          Workflow Run: ${{ github.run_id }}"
          
          git push origin "$BRANCH_NAME"
          echo "branch_name=$BRANCH_NAME" >> $GITHUB_OUTPUT
          
          # Create draft PR
          cat > /tmp/pr_body.md << 'PR_EOF'
          ## 🤖 Automated Fix for Scraper Validation Failures
          
          This PR addresses the scraper validation failures identified in issue #ISSUE_NUMBER.
          
          ### Related Issue
          
          Fixes #ISSUE_NUMBER
          
          ### Failure Summary
          
          ```
          PR_EOF
          
          echo "$SUMMARY" >> /tmp/pr_body.md
          
          cat >> /tmp/pr_body.md << 'PR_EOF'
          ```
          
          ### What This PR Does
          
          This is a draft PR that will be automatically updated by GitHub Copilot to fix the validation failures.
          
          @copilot Please review the validation failures in issue #ISSUE_NUMBER and implement fixes for:
          
          1. Schema validation issues for scrapers
          2. Missing required fields (title, text) in scraper output
          3. HuggingFace dataset compatibility
          4. Data quality improvements
          
          ### Next Steps
          
          - [ ] GitHub Copilot will analyze the failures
          - [ ] Copilot will implement fixes
          - [ ] Automated tests will verify the fixes
          - [ ] Manual review and approval
          
          ---
          
          🤖 **Auto-generated by**: Comprehensive Scraper Validation Workflow
          **Run**: [${{ github.run_number }}](${{ github.server_url }}/${{ github.repository }}/actions/runs/${{ github.run_id }})
          PR_EOF
          
          # Replace ISSUE_NUMBER placeholder
          sed -i "s/#ISSUE_NUMBER/#$ISSUE_NUMBER/g" /tmp/pr_body.md
          
          PR_URL=$(gh pr create \
            --draft \
            --title "🔧 Fix scraper validation failures (Issue #$ISSUE_NUMBER)" \
            --body-file /tmp/pr_body.md \
            --base main \
            --head "$BRANCH_NAME" \
            --label "automated-fix,scraper-validation,copilot-ready")
          
          echo "pr_url=$PR_URL" >> $GITHUB_OUTPUT
          PR_NUMBER=$(echo "$PR_URL" | grep -oP '\d+$')
          echo "pr_number=$PR_NUMBER" >> $GITHUB_OUTPUT
          
          # Add a comment to mention Copilot and provide context
          gh pr comment "$PR_NUMBER" --body "@copilot /fix

          Please analyze the scraper validation failures and implement fixes. Focus on:
          
          1. Ensure all scrapers produce data with required fields: \`title\` and \`text\`
          2. Fix schema validation issues
          3. Ensure HuggingFace dataset compatibility
          4. Improve data quality scores
          
          See the full validation report in the workflow artifacts."
          
          echo "✅ Created issue #$ISSUE_NUMBER and draft PR #$PR_NUMBER"
          echo "" >> $GITHUB_STEP_SUMMARY
          echo "## 🤖 Auto-Healing Initiated" >> $GITHUB_STEP_SUMMARY
          echo "" >> $GITHUB_STEP_SUMMARY
          echo "- **Issue**: #$ISSUE_NUMBER" >> $GITHUB_STEP_SUMMARY
          echo "- **Draft PR**: #$PR_NUMBER" >> $GITHUB_STEP_SUMMARY
          echo "- **Branch**: \`$BRANCH_NAME\`" >> $GITHUB_STEP_SUMMARY
          echo "- **Status**: GitHub Copilot has been notified and will implement fixes" >> $GITHUB_STEP_SUMMARY
      
      - name: Final Summary
        if: always()
        run: |
          echo "" >> $GITHUB_STEP_SUMMARY
          echo "---" >> $GITHUB_STEP_SUMMARY
          echo "" >> $GITHUB_STEP_SUMMARY
          echo "### Validation Complete" >> $GITHUB_STEP_SUMMARY
          echo "" >> $GITHUB_STEP_SUMMARY
          echo "- **Status**: ${{ steps.validation.outputs.validation_status }}" >> $GITHUB_STEP_SUMMARY
          echo "- **Platform**: Self-hosted runner with Docker" >> $GITHUB_STEP_SUMMARY
          echo "- **Container**: python:3.12-slim" >> $GITHUB_STEP_SUMMARY
          echo "- **HuggingFace**: Datasets library used for validation" >> $GITHUB_STEP_SUMMARY
          echo "" >> $GITHUB_STEP_SUMMARY
          
          if [ "${{ steps.validation.outputs.validation_status }}" = "passed" ]; then
            echo "✅ **All scrapers validated successfully**" >> $GITHUB_STEP_SUMMARY
            echo "" >> $GITHUB_STEP_SUMMARY
            echo "Scrapers are:" >> $GITHUB_STEP_SUMMARY
            echo "- Executing correctly" >> $GITHUB_STEP_SUMMARY
            echo "- Producing valid data" >> $GITHUB_STEP_SUMMARY
            echo "- Compatible with HuggingFace datasets" >> $GITHUB_STEP_SUMMARY
            echo "- Meeting schema requirements" >> $GITHUB_STEP_SUMMARY
          else
            echo "❌ **Some validations failed**" >> $GITHUB_STEP_SUMMARY
            echo "" >> $GITHUB_STEP_SUMMARY
            echo "Please review the artifacts and fix failing scrapers." >> $GITHUB_STEP_SUMMARY
          fi
