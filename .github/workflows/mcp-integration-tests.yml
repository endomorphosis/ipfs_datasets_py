name: MCP Endpoints Integration Tests
true:
  push:
    branches:
    - main
    - develop
    paths:
    - ipfs_datasets_py/mcp_server/**
    - tests/integration/**
    - .github/workflows/mcp-integration-tests.yml
  pull_request:
    branches:
    - main
    - develop
  workflow_dispatch:
    inputs:
      test_scope:
        description: Scope of integration tests to run
        required: true
        default: comprehensive
        type: choice
        options:
        - basic
        - comprehensive
        - performance
        - tools-only
  schedule:
  - cron: 0 3 * * *
permissions:
  contents: read
  packages: write
  actions: read
concurrency:
  group: ${{ github.workflow }}-${{ github.ref }}
  cancel-in-progress: true
env:
  PYTHON_VERSION: '3.12'
  MCP_DASHBOARD_PORT: 8899
jobs:
  check-runner:
    timeout-minutes: 20
    uses: ./.github/workflows/templates/check-runner-availability.yml
    with:
      runner_labels: self-hosted,linux,x64
      skip_if_unavailable: true
  basic-integration-tests:
    needs:
    - check-runner
    if: ${{ needs.check-runner.outputs.should_run == 'true' }}
    runs-on:
    - self-hosted
    - linux
    - x64
    timeout-minutes: 30
    container:
      image: python:3.12-slim
      options: --user root
    steps:
    - name: Checkout repository
      uses: actions/checkout@v5
      with:
        fetch-depth: 1
        submodules: false
    - name: Set up Docker Buildx
      uses: docker/setup-buildx-action@v3
    - name: Build MCP Test Docker Image
      run: 'docker build -f docker/Dockerfile.mcp-tests -t ipfs-datasets-mcp-tests:latest .

        '
    - name: Run Basic Integration Tests in Docker
      run: "mkdir -p test-results\n# Run the container with dashboard service\ndocker run --rm -d \\\n  --name mcp-integration-test\
        \ \\\n  -p 8899:8899 \\\n  -v $(pwd)/test-results:/app/test-results \\\n  ipfs-datasets-mcp-tests:latest \\\n  python\
        \ -m ipfs_datasets_py.mcp_dashboard\n\n# Wait for dashboard to be ready\nfor i in {1..60}; do\n  if curl -f http://127.0.0.1:8899/api/mcp/status\
        \ 2>/dev/null; then\n    echo \"\u2705 Dashboard is ready!\"\n    break\n  fi\n  echo \"\u23F3 Waiting for dashboard...\
        \ ($i/60)\"\n  sleep 2\ndone\n\n# Run basic integration tests\ndocker exec mcp-integration-test pytest tests/integration/test_mcp_endpoints.py\
        \ -v \\\n  --junit-xml=/app/test-results/basic-integration-results.xml || true\n\n# Stop the container\ndocker stop\
        \ mcp-integration-test\n"
    - name: Upload Test Results
      uses: actions/upload-artifact@v4
      if: always()
      with:
        name: basic-integration-test-results
        path: test-results/
  comprehensive-integration-tests:
    needs:
    - check-runner
    - basic-integration-tests
    if: ${{ needs.check-runner.outputs.should_run == 'true' && !cancelled() && (github.event.inputs.test_scope == 'comprehensive'
      || github.event_name != 'workflow_dispatch') }}
    runs-on:
    - self-hosted
    - linux
    - x64
    timeout-minutes: 30
    container:
      image: python:3.12-slim
      options: --user root
    steps:
    - name: Checkout repository
      uses: actions/checkout@v5
      with:
        fetch-depth: 1
        submodules: false
    - name: Set up Docker Buildx
      uses: docker/setup-buildx-action@v3
    - name: Build MCP Test Docker Image
      run: 'docker build -f docker/Dockerfile.mcp-tests -t ipfs-datasets-mcp-tests:latest .

        '
    - name: Run Comprehensive Integration Tests in Docker
      run: "mkdir -p test-results\n# Run the container with dashboard service\ndocker run --rm -d \\\n  --name mcp-comprehensive-test\
        \ \\\n  -p 8899:8899 \\\n  -v $(pwd)/test-results:/app/test-results \\\n  ipfs-datasets-mcp-tests:latest \\\n  python\
        \ -m ipfs_datasets_py.mcp_dashboard\n\n# Wait for dashboard to be ready\nfor i in {1..60}; do\n  if curl -f http://127.0.0.1:8899/api/mcp/status\
        \ 2>/dev/null; then\n    echo \"\u2705 Dashboard is ready!\"\n    break\n  fi\n  echo \"\u23F3 Waiting for dashboard...\
        \ ($i/60)\"\n  sleep 2\ndone\n\n# Run comprehensive integration tests\ndocker exec mcp-comprehensive-test pytest tests/integration/\
        \ -v \\\n  --cov=ipfs_datasets_py \\\n  --cov-report=xml:/app/test-results/coverage.xml \\\n  --junit-xml=/app/test-results/comprehensive-integration-results.xml\
        \ || true\n\n# Stop the container\ndocker stop mcp-comprehensive-test\n"
      timeout-minutes: 30
    - name: Upload Comprehensive Test Results
      uses: actions/upload-artifact@v4
      if: always()
      with:
        name: comprehensive-integration-results
        path: test-results/
  tools-integration-tests:
    needs:
    - check-runner
    - basic-integration-tests
    if: ${{ needs.check-runner.outputs.should_run == 'true' && !cancelled() && (github.event.inputs.test_scope == 'tools-only'
      || github.event.inputs.test_scope == 'comprehensive') }}
    runs-on:
    - self-hosted
    - linux
    - x64
    timeout-minutes: 30
    container:
      image: python:3.12-slim
      options: --user root
    steps:
    - name: Checkout repository
      uses: actions/checkout@v5
      with:
        fetch-depth: 1
        submodules: false
    - name: Set up Docker Buildx
      uses: docker/setup-buildx-action@v3
    - name: Build MCP Test Docker Image
      run: 'docker build -f docker/Dockerfile.mcp-tests -t ipfs-datasets-mcp-tests:latest .

        '
    - name: Test MCP Tools Loading in Container
      run: "mkdir -p test-results\necho \"\U0001F527 Testing MCP tools loading in container...\"\n\ndocker run --rm \\\n \
        \ -v $(pwd)/test-results:/app/test-results \\\n  -v $(pwd)/.github/scripts:/scripts \\\n  ipfs-datasets-mcp-tests:latest\
        \ \\\n  python /scripts/test_tools_loading.py\n"
    - name: Test MCP Tools via API in Container
      run: "echo \"\U0001F310 Testing MCP tools via API in container...\"\n\n# Start dashboard\ndocker run --rm -d \\\n  --name\
        \ mcp-tools-test \\\n  -p 8899:8899 \\\n  ipfs-datasets-mcp-tests:latest \\\n  python -m ipfs_datasets_py.mcp_dashboard\n\
        \n# Wait for dashboard\nsleep 15\n\n# Test API\ndocker run --rm \\\n  --network=\"host\" \\\n  -v $(pwd)/.github/scripts:/scripts\
        \ \\\n  ipfs-datasets-mcp-tests:latest \\\n  python /scripts/test_tools_api.py || true\n\n# Stop dashboard\ndocker\
        \ stop mcp-tools-test || true\n"
  performance-integration-tests:
    needs:
    - check-runner
    - basic-integration-tests
    if: ${{ needs.check-runner.outputs.should_run == 'true' && !cancelled() && (github.event.inputs.test_scope == 'performance'
      || github.event_name == 'schedule') }}
    runs-on:
    - self-hosted
    - linux
    - x64
    timeout-minutes: 30
    container:
      image: python:3.12-slim
      options: --user root
    steps:
    - name: Checkout repository
      uses: actions/checkout@v5
      with:
        fetch-depth: 1
        submodules: false
    - name: Set up Docker Buildx
      uses: docker/setup-buildx-action@v3
    - name: Build MCP Test Docker Image
      run: 'docker build -f docker/Dockerfile.mcp-tests -t ipfs-datasets-mcp-tests:latest .

        '
    - name: Run Performance Tests in Container
      run: "mkdir -p test-results\necho \"\u26A1 Running performance tests in container...\"\n\n# Start dashboard\ndocker\
        \ run --rm -d \\\n  --name mcp-perf-test \\\n  -p 8899:8899 \\\n  -v $(pwd)/test-results:/app/test-results \\\n  ipfs-datasets-mcp-tests:latest\
        \ \\\n  python -m ipfs_datasets_py.mcp_dashboard\n\n# Wait for dashboard\nsleep 15\n\n# Run performance tests\ndocker\
        \ run --rm \\\n  --network=\"host\" \\\n  -v $(pwd)/test-results:/app/test-results \\\n  -v $(pwd)/.github/scripts:/scripts\
        \ \\\n  ipfs-datasets-mcp-tests:latest \\\n  python /scripts/test_performance.py || true\n\n# Stop dashboard\ndocker\
        \ stop mcp-perf-test || true\n"
    - name: Upload Performance Results
      uses: actions/upload-artifact@v4
      if: always()
      with:
        name: performance-test-results
        path: test-results/
        cache: pip
    - name: Install dependencies
      run: 'python -m pip install --upgrade pip

        pip install -e .[test,all]

        pip install pytest requests numpy psutil

        '
    - name: Start MCP Dashboard for Performance Tests
      run: "echo \"\U0001F680 Starting MCP Dashboard for performance tests...\"\npython -m ipfs_datasets_py.mcp_dashboard\
        \ &\nMCP_PID=$!\necho \"MCP_DASHBOARD_PID=$MCP_PID\" >> $GITHUB_ENV\n\n# Wait for dashboard\nfor i in {1..60}; do\n\
        \  if curl -f http://127.0.0.1:8899/api/mcp/status; then\n    echo \"\u2705 Dashboard ready for performance tests!\"\
        \n    break\n  fi\n  sleep 2\ndone\n"
    - name: Run Performance Tests
      run: "echo \"\u26A1 Running MCP performance tests...\"\npython -c \"\nimport requests\nimport time\nimport statistics\n\
        import psutil\nimport json\nimport sys\n\ndef measure_performance():\n    endpoints = [\n        'http://127.0.0.1:8899/api/mcp/status',\n\
        \        'http://127.0.0.1:8899/api/mcp/tools',\n        'http://127.0.0.1:8899/api/health'\n    ]\n    \n    results\
        \ = {}\n    \n    for endpoint in endpoints:\n        print(f'\U0001F4CA Testing {endpoint}...')\n        times =\
        \ []\n        errors = 0\n        \n        # Warm up\n        try:\n            requests.get(endpoint, timeout=5)\n\
        \        except:\n            pass\n        \n        # Measure performance\n        for i in range(20):  # 20 requests\n\
        \            try:\n                start = time.time()\n                response = requests.get(endpoint, timeout=10)\n\
        \                end = time.time()\n                \n                if response.status_code == 200:\n          \
        \          times.append(end - start)\n                else:\n                    errors += 1\n            except Exception\
        \ as e:\n                errors += 1\n        \n        if times:\n            results[endpoint] = {\n           \
        \     'avg_time': statistics.mean(times),\n                'min_time': min(times),\n                'max_time': max(times),\n\
        \                'median_time': statistics.median(times),\n                'successful_requests': len(times),\n  \
        \              'failed_requests': errors,\n                'success_rate': len(times) / 20\n            }\n      \
        \      \n            print(f'  \u2705 Avg: {statistics.mean(times):.3f}s')\n            print(f'  \U0001F4C8 Min/Max:\
        \ {min(times):.3f}s / {max(times):.3f}s')\n            print(f'  \U0001F3AF Success rate: {len(times)/20*100:.1f}%')\n\
        \        else:\n            results[endpoint] = {'error': 'All requests failed'}\n            print(f'  \u274C All\
        \ requests failed')\n    \n    # System resource usage\n    process = psutil.Process()\n    memory_info = process.memory_info()\n\
        \    \n    results['system_metrics'] = {\n        'memory_rss_mb': memory_info.rss / 1024 / 1024,\n        'memory_vms_mb':\
        \ memory_info.vms / 1024 / 1024,\n        'cpu_percent': process.cpu_percent(),\n        'num_threads': process.num_threads()\n\
        \    }\n    \n    print(f'\U0001F4BE Memory RSS: {memory_info.rss / 1024 / 1024:.1f} MB')\n    print(f'\U0001F9F5\
        \ Threads: {process.num_threads()}')\n    \n    # Save results\n    with open('performance_results.json', 'w') as\
        \ f:\n        json.dump(results, f, indent=2)\n    \n    return results\n\nperf_results = measure_performance()\n\n\
        # Check performance thresholds\nfailed_endpoints = []\nfor endpoint, data in perf_results.items():\n    if endpoint\
        \ == 'system_metrics':\n        continue\n        \n    if isinstance(data, dict) and 'avg_time' in data:\n      \
        \  if data['avg_time'] > 5.0:  # 5 second threshold\n            failed_endpoints.append(f'{endpoint} (avg: {data[\\\
        \"avg_time\\\"]:.3f}s)')\n        if data['success_rate'] < 0.8:  # 80% success rate threshold\n            failed_endpoints.append(f'{endpoint}\
        \ (success: {data[\\\"success_rate\\\"]*100:.1f}%)')\n\nif failed_endpoints:\n    print(f'\u26A0\uFE0F Performance\
        \ issues detected: {failed_endpoints}')\n    # Don't fail the build for performance issues, just warn\nelse:\n   \
        \ print('\u2705 All performance tests passed')\n\"\n"
    - name: Upload Performance Results
      uses: actions/upload-artifact@v4
      if: always()
      with:
        name: performance-test-results
        path: performance_results.json
    - name: Stop Dashboard
      if: always()
      run: "if [ ! -z \"$MCP_DASHBOARD_PID\" ]; then\n  kill $MCP_DASHBOARD_PID || true\nfi\npkill -f \"mcp_dashboard\" ||\
        \ true\n"
  multi-arch-integration-tests:
    needs:
    - check-runner
    - basic-integration-tests
    if: ${{ needs.check-runner.outputs.should_run == 'true' && !cancelled() }}
    runs-on: ${{ matrix.runner }}
    timeout-minutes: 30
    continue-on-error: true
    strategy:
      matrix:
        runner:
        - - self-hosted
          - linux
          - x64
        - - self-hosted
          - linux
          - arm64
      fail-fast: false
    steps:
    - name: Checkout repository
      uses: actions/checkout@v5
      with:
        fetch-depth: 1
        submodules: false
    - name: Architecture Info
      run: "echo \"\U0001F3D7\uFE0F Testing on $(uname -m) architecture\"\necho \"OS: $(cat /etc/os-release | grep PRETTY_NAME\
        \ | cut -d= -f2)\"\necho \"Python: $(python3 --version)\"\n"
    - name: Set up environment
      run: 'python3 -m venv .venv-integration

        source .venv-integration/bin/activate

        pip install --upgrade pip

        pip install -e .[test]

        pip install requests pytest pytest-asyncio

        '
    - name: Run Basic Integration Tests
      run: "source .venv-integration/bin/activate\necho \"\U0001F9EA Running basic integration tests on $(uname -m)...\"\n\
        \n# Start dashboard\npython -m ipfs_datasets_py.mcp_dashboard &\nMCP_PID=$!\n\n# Wait for dashboard\nfor i in {1..60};\
        \ do\n  if curl -f http://127.0.0.1:8899/api/mcp/status; then\n    echo \"\u2705 Dashboard ready on $(uname -m)!\"\
        \n    break\n  fi\n  sleep 2\ndone\n\n# Run tests\ncd tests/integration\npython -m pytest test_mcp_endpoints.py::TestMCPIntegration::test_basic_endpoints_only\
        \ -v\n\n# Cleanup\nkill $MCP_PID || true\npkill -f \"mcp_dashboard\" || true\n"
  integration-test-summary:
    needs:
    - check-runner
    - basic-integration-tests
    - comprehensive-integration-tests
    - tools-integration-tests
    - performance-integration-tests
    - multi-arch-integration-tests
    if: always()
    runs-on: ubuntu-latest
    timeout-minutes: 30
    steps:
    - name: Download Test Results
      uses: actions/download-artifact@v4
      with:
        name: comprehensive-integration-results
        path: test_results/
      continue-on-error: true
    - name: Download Performance Results
      uses: actions/download-artifact@v4
      with:
        name: performance-test-results
        path: test_results/
      continue-on-error: true
    - name: Generate Integration Test Summary
      run: "echo \"## \U0001F517 MCP Integration Test Results\" >> $GITHUB_STEP_SUMMARY\necho \"\" >> $GITHUB_STEP_SUMMARY\n\
        echo \"### Test Suite Results\" >> $GITHUB_STEP_SUMMARY\necho \"\" >> $GITHUB_STEP_SUMMARY\necho \"| Test Suite |\
        \ Status | Description |\" >> $GITHUB_STEP_SUMMARY\necho \"|------------|--------|-------------|\" >> $GITHUB_STEP_SUMMARY\n\
        echo \"| Basic Integration | ${{ needs.basic-integration-tests.result }} | Core endpoint and tools availability |\"\
        \ >> $GITHUB_STEP_SUMMARY\necho \"| Comprehensive Integration | ${{ needs.comprehensive-integration-tests.result }}\
        \ | Full MCP functionality testing |\" >> $GITHUB_STEP_SUMMARY\necho \"| Tools Integration | ${{ needs.tools-integration-tests.result\
        \ }} | MCP tools loading and execution |\" >> $GITHUB_STEP_SUMMARY\necho \"| Performance Tests | ${{ needs.performance-integration-tests.result\
        \ }} | API performance and resource usage |\" >> $GITHUB_STEP_SUMMARY\necho \"| Multi-Architecture | ${{ needs.multi-arch-integration-tests.result\
        \ }} | x86_64 and ARM64 compatibility |\" >> $GITHUB_STEP_SUMMARY\necho \"\" >> $GITHUB_STEP_SUMMARY\n\n# Add success/failure\
        \ analysis\necho \"### Analysis\" >> $GITHUB_STEP_SUMMARY\necho \"\" >> $GITHUB_STEP_SUMMARY\n\nif [ \"${{ needs.basic-integration-tests.result\
        \ }}\" = \"success\" ]; then\n  echo \"\u2705 **Basic Integration**: Core MCP functionality is working correctly\"\
        \ >> $GITHUB_STEP_SUMMARY\nelse\n  echo \"\u274C **Basic Integration**: Critical issues with core MCP functionality\"\
        \ >> $GITHUB_STEP_SUMMARY\nfi\n\nif [ \"${{ needs.comprehensive-integration-tests.result }}\" = \"success\" ]; then\n\
        \  echo \"\u2705 **Comprehensive**: All MCP endpoints and tools are functional\" >> $GITHUB_STEP_SUMMARY\nelse\n \
        \ echo \"\u26A0\uFE0F **Comprehensive**: Some MCP endpoints or tools have issues\" >> $GITHUB_STEP_SUMMARY\nfi\n\n\
        if [ \"${{ needs.tools-integration-tests.result }}\" = \"success\" ]; then\n  echo \"\u2705 **Tools**: MCP tools loading\
        \ and basic execution working\" >> $GITHUB_STEP_SUMMARY\nelse\n  echo \"\u26A0\uFE0F **Tools**: Issues with MCP tools\
        \ loading or execution\" >> $GITHUB_STEP_SUMMARY\nfi\n\necho \"\" >> $GITHUB_STEP_SUMMARY\necho \"### Recommendations\"\
        \ >> $GITHUB_STEP_SUMMARY\necho \"- Review test artifacts for detailed results\" >> $GITHUB_STEP_SUMMARY\necho \"\
        - Monitor MCP dashboard performance in production\" >> $GITHUB_STEP_SUMMARY\necho \"- Set up continuous monitoring\
        \ for endpoint availability\" >> $GITHUB_STEP_SUMMARY\necho \"- Consider adding more specific tool integration tests\"\
        \ >> $GITHUB_STEP_SUMMARY\n"
