name: MCP Endpoints Integration Tests

on:
  push:
    branches: [ main, develop ]
    paths:
      - 'ipfs_datasets_py/mcp_server/**'
      - 'tests/integration/**'
      - '.github/workflows/mcp-integration-tests.yml'
  pull_request:
    branches: [ main, develop ]
  workflow_dispatch:
    inputs:
      test_scope:
        description: 'Scope of integration tests to run'
        required: true
        default: 'comprehensive'
        type: choice
        options:
          - basic
          - comprehensive
          - performance
          - tools-only
  schedule:
    # Run comprehensive integration tests daily at 3 AM UTC
    - cron: '0 3 * * *'

env:
  PYTHON_VERSION: '3.12'
  MCP_DASHBOARD_PORT: 8899

jobs:
  # Basic integration tests (containerized)
  basic-integration-tests:
    runs-on: ubuntu-latest
    
    steps:
      - name: Checkout repository
        uses: actions/checkout@v4
        with:
          submodules: false
      
      - name: Set up Docker Buildx
        uses: docker/setup-buildx-action@v3
      
      - name: Build MCP Test Docker Image
        run: |
          docker build -f Dockerfile.mcp-tests -t ipfs-datasets-mcp-tests:latest .
      
      - name: Run Basic Integration Tests in Docker
        run: |
          mkdir -p test-results
          # Run the container with dashboard service
          docker run --rm -d \
            --name mcp-integration-test \
            -p 8899:8899 \
            -v $(pwd)/test-results:/app/test-results \
            ipfs-datasets-mcp-tests:latest \
            python -m ipfs_datasets_py.mcp_dashboard
          
          # Wait for dashboard to be ready
          for i in {1..60}; do
            if curl -f http://127.0.0.1:8899/api/mcp/status 2>/dev/null; then
              echo "✅ Dashboard is ready!"
              break
            fi
            echo "⏳ Waiting for dashboard... ($i/60)"
            sleep 2
          done
          
          # Run basic integration tests
          docker exec mcp-integration-test pytest tests/integration/test_mcp_endpoints.py -v \
            --junit-xml=/app/test-results/basic-integration-results.xml || true
          
          # Stop the container
          docker stop mcp-integration-test
      
      - name: Upload Test Results
        uses: actions/upload-artifact@v4
        if: always()
        with:
          name: basic-integration-test-results
          path: test-results/

  # Comprehensive integration tests (containerized)
  comprehensive-integration-tests:
    runs-on: ubuntu-latest
    needs: basic-integration-tests
    if: ${{ !cancelled() && (github.event.inputs.test_scope == 'comprehensive' || github.event_name != 'workflow_dispatch') }}
    
    steps:
      - name: Checkout repository
        uses: actions/checkout@v4
        with:
          submodules: false
      
      - name: Set up Docker Buildx
        uses: docker/setup-buildx-action@v3
      
      - name: Build MCP Test Docker Image
        run: |
          docker build -f Dockerfile.mcp-tests -t ipfs-datasets-mcp-tests:latest .
      
      - name: Run Comprehensive Integration Tests in Docker
        run: |
          mkdir -p test-results
          # Run the container with dashboard service
          docker run --rm -d \
            --name mcp-comprehensive-test \
            -p 8899:8899 \
            -v $(pwd)/test-results:/app/test-results \
            ipfs-datasets-mcp-tests:latest \
            python -m ipfs_datasets_py.mcp_dashboard
          
          # Wait for dashboard to be ready
          for i in {1..60}; do
            if curl -f http://127.0.0.1:8899/api/mcp/status 2>/dev/null; then
              echo "✅ Dashboard is ready!"
              break
            fi
            echo "⏳ Waiting for dashboard... ($i/60)"
            sleep 2
          done
          
          # Run comprehensive integration tests
          docker exec mcp-comprehensive-test pytest tests/integration/ -v \
            --cov=ipfs_datasets_py \
            --cov-report=xml:/app/test-results/coverage.xml \
            --junit-xml=/app/test-results/comprehensive-integration-results.xml || true
          
          # Stop the container
          docker stop mcp-comprehensive-test
        timeout-minutes: 30
      
      - name: Upload Comprehensive Test Results
        uses: actions/upload-artifact@v4
        if: always()
        with:
          name: comprehensive-integration-results
          path: test-results/

  # Tools-specific tests
  tools-integration-tests:
    runs-on: ubuntu-latest
    needs: basic-integration-tests
    if: ${{ !cancelled() && (github.event.inputs.test_scope == 'tools-only' || github.event.inputs.test_scope == 'comprehensive') }}
    
    steps:
      - name: Checkout repository
        uses: actions/checkout@v4
        with:
          submodules: false
      
      - name: Set up Python
        uses: actions/setup-python@v5
        with:
          python-version: ${{ env.PYTHON_VERSION }}
          cache: 'pip'
      
      - name: Install dependencies
        run: |
          python -m pip install --upgrade pip
          pip install -e .[test,all]
          pip install pytest pytest-asyncio requests numpy faiss-cpu
      
      - name: Test MCP Tools Loading (Direct)
        run: |
          echo "🔧 Testing direct MCP tools loading..."
          python -c "
          import sys
          import traceback
          
          try:
              from ipfs_datasets_py.mcp_server.tools import get_all_tools
              tools = get_all_tools()
              
              print(f'✅ Successfully loaded {len(tools)} MCP tools')
              
              # Test tools by category
              categories = {}
              for tool_name, tool_info in tools.items():
                  category = tool_info.get('category', 'uncategorized')
                  if category not in categories:
                      categories[category] = []
                  categories[category].append(tool_name)
              
              print('📊 Tools by category:')
              for category, tool_list in categories.items():
                  print(f'  {category}: {len(tool_list)} tools')
              
              # Test a few sample tools
              sample_tools = list(tools.keys())[:5]
              print(f'🔍 Sample tools: {sample_tools}')
              
          except Exception as e:
              print(f'❌ Error loading MCP tools: {e}')
              traceback.print_exc()
              sys.exit(1)
          "
      
      - name: Start MCP Dashboard for Tools Tests
        run: |
          echo "🚀 Starting MCP Dashboard for tools tests..."
          python -m ipfs_datasets_py.mcp_dashboard &
          MCP_PID=$!
          echo "MCP_DASHBOARD_PID=$MCP_PID" >> $GITHUB_ENV
          
          # Wait for dashboard
          for i in {1..60}; do
            if curl -f http://127.0.0.1:8899/api/mcp/status; then
              echo "✅ Dashboard ready for tools tests!"
              break
            fi
            sleep 2
          done
      
      - name: Test MCP Tools via API
        run: |
          echo "🌐 Testing MCP tools via API..."
          python -c "
          import requests
          import json
          import sys
          
          try:
              # Get tools from API
              response = requests.get('http://127.0.0.1:8899/api/mcp/tools', timeout=30)
              if response.status_code != 200:
                  print(f'❌ Tools API returned {response.status_code}')
                  sys.exit(1)
              
              tools = response.json()
              print(f'✅ API returned {len(tools)} tools')
              
              # Test tools structure
              for tool_name, tool_info in list(tools.items())[:5]:
                  print(f'🔧 {tool_name}:')
                  print(f'  - Description: {tool_info.get(\"description\", \"N/A\")}')
                  print(f'  - Category: {tool_info.get(\"category\", \"N/A\")}')
                  print(f'  - Parameters: {len(tool_info.get(\"parameters\", {}))}')
              
              # Test status endpoint
              status_response = requests.get('http://127.0.0.1:8899/api/mcp/status', timeout=10)
              if status_response.status_code == 200:
                  status_data = status_response.json()
                  print(f'📊 Dashboard status: {status_data.get(\"status\", \"unknown\")}')
                  print(f'📊 Tools available: {status_data.get(\"tools_available\", 0)}')
              
          except Exception as e:
              print(f'❌ Error testing tools API: {e}')
              import traceback
              traceback.print_exc()
              sys.exit(1)
          "
      
      - name: Test Sample Tool Execution
        run: |
          echo "⚡ Testing sample tool execution..."
          python -c "
          import asyncio
          import sys
          import traceback
          
          async def test_tools():
              try:
                  from ipfs_datasets_py.mcp_server.tools import get_all_tools
                  tools = get_all_tools()
                  
                  # Find safe tools to test
                  safe_tools = []
                  for tool_name, tool_info in tools.items():
                      # Look for tools that are likely safe to test
                      if any(keyword in tool_name.lower() for keyword in ['list', 'info', 'status', 'get']):
                          safe_tools.append((tool_name, tool_info))
                  
                  print(f'🧪 Found {len(safe_tools)} potentially safe tools to test')
                  
                  # Test up to 3 safe tools
                  for tool_name, tool_info in safe_tools[:3]:
                      try:
                          print(f'🔍 Testing tool: {tool_name}')
                          tool_func = tool_info.get('function')
                          
                          if tool_func:
                              # Try with minimal or no arguments
                              if asyncio.iscoroutinefunction(tool_func):
                                  result = await tool_func()
                              else:
                                  result = tool_func()
                              
                              print(f'  ✅ {tool_name} executed successfully')
                              print(f'  📊 Result type: {type(result).__name__}')
                          
                      except Exception as e:
                          print(f'  ⚠️ {tool_name} failed (expected): {type(e).__name__}')
                  
                  print('✅ Tool execution tests completed')
                  
              except Exception as e:
                  print(f'❌ Error in tool execution tests: {e}')
                  traceback.print_exc()
                  sys.exit(1)
          
          asyncio.run(test_tools())
          "
      
      - name: Stop Dashboard
        if: always()
        run: |
          if [ ! -z "$MCP_DASHBOARD_PID" ]; then
            kill $MCP_DASHBOARD_PID || true
          fi
          pkill -f "mcp_dashboard" || true

  # Performance tests
  performance-integration-tests:
    runs-on: ubuntu-latest
    needs: basic-integration-tests
    if: ${{ !cancelled() && (github.event.inputs.test_scope == 'performance' || github.event_name == 'schedule') }}
    
    steps:
      - name: Checkout repository
        uses: actions/checkout@v4
        with:
          submodules: false
      
      - name: Set up Python
        uses: actions/setup-python@v5
        with:
          python-version: ${{ env.PYTHON_VERSION }}
          cache: 'pip'
      
      - name: Install dependencies
        run: |
          python -m pip install --upgrade pip
          pip install -e .[test,all]
          pip install pytest requests numpy psutil
      
      - name: Start MCP Dashboard for Performance Tests
        run: |
          echo "🚀 Starting MCP Dashboard for performance tests..."
          python -m ipfs_datasets_py.mcp_dashboard &
          MCP_PID=$!
          echo "MCP_DASHBOARD_PID=$MCP_PID" >> $GITHUB_ENV
          
          # Wait for dashboard
          for i in {1..60}; do
            if curl -f http://127.0.0.1:8899/api/mcp/status; then
              echo "✅ Dashboard ready for performance tests!"
              break
            fi
            sleep 2
          done
      
      - name: Run Performance Tests
        run: |
          echo "⚡ Running MCP performance tests..."
          python -c "
          import requests
          import time
          import statistics
          import psutil
          import json
          import sys
          
          def measure_performance():
              endpoints = [
                  'http://127.0.0.1:8899/api/mcp/status',
                  'http://127.0.0.1:8899/api/mcp/tools',
                  'http://127.0.0.1:8899/api/health'
              ]
              
              results = {}
              
              for endpoint in endpoints:
                  print(f'📊 Testing {endpoint}...')
                  times = []
                  errors = 0
                  
                  # Warm up
                  try:
                      requests.get(endpoint, timeout=5)
                  except:
                      pass
                  
                  # Measure performance
                  for i in range(20):  # 20 requests
                      try:
                          start = time.time()
                          response = requests.get(endpoint, timeout=10)
                          end = time.time()
                          
                          if response.status_code == 200:
                              times.append(end - start)
                          else:
                              errors += 1
                      except Exception as e:
                          errors += 1
                  
                  if times:
                      results[endpoint] = {
                          'avg_time': statistics.mean(times),
                          'min_time': min(times),
                          'max_time': max(times),
                          'median_time': statistics.median(times),
                          'successful_requests': len(times),
                          'failed_requests': errors,
                          'success_rate': len(times) / 20
                      }
                      
                      print(f'  ✅ Avg: {statistics.mean(times):.3f}s')
                      print(f'  📈 Min/Max: {min(times):.3f}s / {max(times):.3f}s')
                      print(f'  🎯 Success rate: {len(times)/20*100:.1f}%')
                  else:
                      results[endpoint] = {'error': 'All requests failed'}
                      print(f'  ❌ All requests failed')
              
              # System resource usage
              process = psutil.Process()
              memory_info = process.memory_info()
              
              results['system_metrics'] = {
                  'memory_rss_mb': memory_info.rss / 1024 / 1024,
                  'memory_vms_mb': memory_info.vms / 1024 / 1024,
                  'cpu_percent': process.cpu_percent(),
                  'num_threads': process.num_threads()
              }
              
              print(f'💾 Memory RSS: {memory_info.rss / 1024 / 1024:.1f} MB')
              print(f'🧵 Threads: {process.num_threads()}')
              
              # Save results
              with open('performance_results.json', 'w') as f:
                  json.dump(results, f, indent=2)
              
              return results
          
          perf_results = measure_performance()
          
          # Check performance thresholds
          failed_endpoints = []
          for endpoint, data in perf_results.items():
              if endpoint == 'system_metrics':
                  continue
                  
              if isinstance(data, dict) and 'avg_time' in data:
                  if data['avg_time'] > 5.0:  # 5 second threshold
                      failed_endpoints.append(f'{endpoint} (avg: {data[\"avg_time\"]:.3f}s)')
                  if data['success_rate'] < 0.8:  # 80% success rate threshold
                      failed_endpoints.append(f'{endpoint} (success: {data[\"success_rate\"]*100:.1f}%)')
          
          if failed_endpoints:
              print(f'⚠️ Performance issues detected: {failed_endpoints}')
              # Don't fail the build for performance issues, just warn
          else:
              print('✅ All performance tests passed')
          "
      
      - name: Upload Performance Results
        uses: actions/upload-artifact@v4
        if: always()
        with:
          name: performance-test-results
          path: performance_results.json
      
      - name: Stop Dashboard
        if: always()
        run: |
          if [ ! -z "$MCP_DASHBOARD_PID" ]; then
            kill $MCP_DASHBOARD_PID || true
          fi
          pkill -f "mcp_dashboard" || true

  # Multi-architecture integration tests
  multi-arch-integration-tests:
    runs-on: ${{ matrix.runner }}
    needs: basic-integration-tests
    if: ${{ !cancelled() }}
    continue-on-error: true
    strategy:
      matrix:
        runner:
          - [self-hosted, linux, x64]
          - [self-hosted, linux, arm64]
      fail-fast: false
    
    steps:
      - name: Checkout repository
        uses: actions/checkout@v4
        with:
          submodules: false
      
      - name: Architecture Info
        run: |
          echo "🏗️ Testing on $(uname -m) architecture"
          echo "OS: $(cat /etc/os-release | grep PRETTY_NAME | cut -d= -f2)"
          echo "Python: $(python3 --version)"
      
      - name: Set up environment
        run: |
          python3 -m venv .venv-integration
          source .venv-integration/bin/activate
          pip install --upgrade pip
          pip install -e .[test]
          pip install requests pytest pytest-asyncio
      
      - name: Run Basic Integration Tests
        run: |
          source .venv-integration/bin/activate
          echo "🧪 Running basic integration tests on $(uname -m)..."
          
          # Start dashboard
          python -m ipfs_datasets_py.mcp_dashboard &
          MCP_PID=$!
          
          # Wait for dashboard
          for i in {1..60}; do
            if curl -f http://127.0.0.1:8899/api/mcp/status; then
              echo "✅ Dashboard ready on $(uname -m)!"
              break
            fi
            sleep 2
          done
          
          # Run tests
          cd tests/integration
          python -m pytest test_mcp_endpoints.py::TestMCPIntegration::test_basic_endpoints_only -v
          
          # Cleanup
          kill $MCP_PID || true
          pkill -f "mcp_dashboard" || true

  # Generate comprehensive test summary
  integration-test-summary:
    runs-on: ubuntu-latest
    needs: [basic-integration-tests, comprehensive-integration-tests, tools-integration-tests, performance-integration-tests, multi-arch-integration-tests]
    if: always()
    
    steps:
      - name: Download Test Results
        uses: actions/download-artifact@v4
        with:
          name: comprehensive-integration-results
          path: test_results/
        continue-on-error: true
      
      - name: Download Performance Results
        uses: actions/download-artifact@v4
        with:
          name: performance-test-results
          path: test_results/
        continue-on-error: true
      
      - name: Generate Integration Test Summary
        run: |
          echo "## 🔗 MCP Integration Test Results" >> $GITHUB_STEP_SUMMARY
          echo "" >> $GITHUB_STEP_SUMMARY
          echo "### Test Suite Results" >> $GITHUB_STEP_SUMMARY
          echo "" >> $GITHUB_STEP_SUMMARY
          echo "| Test Suite | Status | Description |" >> $GITHUB_STEP_SUMMARY
          echo "|------------|--------|-------------|" >> $GITHUB_STEP_SUMMARY
          echo "| Basic Integration | ${{ needs.basic-integration-tests.result }} | Core endpoint and tools availability |" >> $GITHUB_STEP_SUMMARY
          echo "| Comprehensive Integration | ${{ needs.comprehensive-integration-tests.result }} | Full MCP functionality testing |" >> $GITHUB_STEP_SUMMARY
          echo "| Tools Integration | ${{ needs.tools-integration-tests.result }} | MCP tools loading and execution |" >> $GITHUB_STEP_SUMMARY
          echo "| Performance Tests | ${{ needs.performance-integration-tests.result }} | API performance and resource usage |" >> $GITHUB_STEP_SUMMARY
          echo "| Multi-Architecture | ${{ needs.multi-arch-integration-tests.result }} | x86_64 and ARM64 compatibility |" >> $GITHUB_STEP_SUMMARY
          echo "" >> $GITHUB_STEP_SUMMARY
          
          # Add success/failure analysis
          echo "### Analysis" >> $GITHUB_STEP_SUMMARY
          echo "" >> $GITHUB_STEP_SUMMARY
          
          if [ "${{ needs.basic-integration-tests.result }}" = "success" ]; then
            echo "✅ **Basic Integration**: Core MCP functionality is working correctly" >> $GITHUB_STEP_SUMMARY
          else
            echo "❌ **Basic Integration**: Critical issues with core MCP functionality" >> $GITHUB_STEP_SUMMARY
          fi
          
          if [ "${{ needs.comprehensive-integration-tests.result }}" = "success" ]; then
            echo "✅ **Comprehensive**: All MCP endpoints and tools are functional" >> $GITHUB_STEP_SUMMARY
          else
            echo "⚠️ **Comprehensive**: Some MCP endpoints or tools have issues" >> $GITHUB_STEP_SUMMARY
          fi
          
          if [ "${{ needs.tools-integration-tests.result }}" = "success" ]; then
            echo "✅ **Tools**: MCP tools loading and basic execution working" >> $GITHUB_STEP_SUMMARY
          else
            echo "⚠️ **Tools**: Issues with MCP tools loading or execution" >> $GITHUB_STEP_SUMMARY
          fi
          
          echo "" >> $GITHUB_STEP_SUMMARY
          echo "### Recommendations" >> $GITHUB_STEP_SUMMARY
          echo "- Review test artifacts for detailed results" >> $GITHUB_STEP_SUMMARY
          echo "- Monitor MCP dashboard performance in production" >> $GITHUB_STEP_SUMMARY
          echo "- Set up continuous monitoring for endpoint availability" >> $GITHUB_STEP_SUMMARY
          echo "- Consider adding more specific tool integration tests" >> $GITHUB_STEP_SUMMARY