"""
Test Generator Tool

Generates unittest test files from JSON specifications.
Migrated from claudes_toolbox with enhanced IPFS and dataset integration.

Features:
- Generate unittest or pytest test files
- Support for parametrized tests and fixtures
- Dataset-specific test generation
- IPFS integration for storing test specifications
- Template-based generation with Jinja2
"""

import json
import asyncio
from typing import Dict, List, Optional, Any, Union
from pathlib import Path
from datetime import datetime
import tempfile

try:
    import jinja2
    JINJA2_AVAILABLE = True
except ImportError:
    JINJA2_AVAILABLE = False
    jinja2 = None

from .base_tool import BaseDevelopmentTool, DevelopmentToolValidationError, DevelopmentToolExecutionError
from .config import get_config


class TestGeneratorTool(BaseDevelopmentTool):
    """
    Tool for generating test files from JSON specifications.

    Supports both unittest and pytest frameworks with configurable templates.
    Enhanced with dataset-specific test patterns and IPFS integration.
    """

    def __init__(self):
        super().__init__(
            name="test_generator",
            description="Generate unittest or pytest test files from JSON specifications",
            category="testing"
        )
        self.config = get_config().test_generator

        if not JINJA2_AVAILABLE:
            raise DevelopmentToolExecutionError("jinja2 is required for test generation")

        # Initialize Jinja2 environment
        self.jinja_env = jinja2.Environment(
            loader=jinja2.BaseLoader(),
            trim_blocks=True,
            lstrip_blocks=True
        )

    def _get_unittest_template(self) -> str:
        """Get the unittest template string."""
        return '''"""
{{ description }}

Generated by IPFS Datasets Test Generator
Date: {{ timestamp }}
Test Framework: unittest
"""

import unittest
{% if imports %}
{% for import in imports %}
{{ import }}
{% endfor %}
{% endif %}
from pathlib import Path
import sys

# Add project root to Python path
project_root = Path(__file__).parent.parent
if str(project_root) not in sys.path:
    sys.path.insert(0, str(project_root))

{% if dataset_imports %}
# Dataset-specific imports
{% for import in dataset_imports %}
{{ import }}
{% endfor %}
{% endif %}


class {{ class_name }}(unittest.TestCase):
    """{{ description }}"""

    {% if fixtures %}
    def setUp(self):
        """Set up test fixtures."""
        {% for fixture in fixtures %}
        self.{{ fixture.name }} = {{ fixture.value }}
        {% endfor %}
        {% if dataset_fixtures %}
        # Dataset fixtures
        {% for fixture in dataset_fixtures %}
        self.{{ fixture.name }} = {{ fixture.value }}
        {% endfor %}
        {% endif %}

    def tearDown(self):
        """Clean up after tests."""
        pass

    {% endif %}
    {% for test in tests %}
    def test_{{ test.name }}(self):
        """{{ test.description }}"""
        {% if test.setup %}
        # Test setup
        {% for setup_line in test.setup %}
        {{ setup_line }}
        {% endfor %}
        {% endif %}

        {% if test.is_dataset_test %}
        # Dataset-specific test logic
        {% if test.dataset_operations %}
        {% for operation in test.dataset_operations %}
        {{ operation }}
        {% endfor %}
        {% endif %}
        {% endif %}

        # Test execution
        {% if test.parametrized %}
        test_cases = {{ test.test_cases }}
        for case in test_cases:
            with self.subTest(case=case):
                {% for assertion in test.assertions %}
                {{ assertion.replace('case', 'case') }}
                {% endfor %}
        {% else %}
        {% for assertion in test.assertions %}
        {{ assertion }}
        {% endfor %}
        {% endif %}

        {% if test.cleanup %}
        # Test cleanup
        {% for cleanup_line in test.cleanup %}
        {{ cleanup_line }}
        {% endfor %}
        {% endif %}

    {% endfor %}

if __name__ == '__main__':
    unittest.main()
'''

    def _get_pytest_template(self) -> str:
        """Get the pytest template string."""
        return '''"""
{{ description }}

Generated by IPFS Datasets Test Generator
Date: {{ timestamp }}
Test Framework: pytest
"""

import pytest
{% if imports %}
{% for import in imports %}
{{ import }}
{% endfor %}
{% endif %}
from pathlib import Path
import sys

# Add project root to Python path
project_root = Path(__file__).parent.parent
if str(project_root) not in sys.path:
    sys.path.insert(0, str(project_root))

{% if dataset_imports %}
# Dataset-specific imports
{% for import in dataset_imports %}
{{ import }}
{% endfor %}
{% endif %}

{% if fixtures %}
# Test fixtures
{% for fixture in fixtures %}
@pytest.fixture
def {{ fixture.name }}():
    """{{ fixture.description or 'Test fixture' }}"""
    return {{ fixture.value }}

{% endfor %}
{% endif %}

{% if dataset_fixtures %}
# Dataset fixtures
{% for fixture in dataset_fixtures %}
@pytest.fixture
def {{ fixture.name }}():
    """{{ fixture.description or 'Dataset fixture' }}"""
    return {{ fixture.value }}

{% endfor %}
{% endif %}

{% for test in tests %}
{% if test.parametrized %}
@pytest.mark.parametrize("{{ test.param_names }}", {{ test.test_cases }})
{% endif %}
def test_{{ test.name }}({% if test.parametrized %}{{ test.param_names }}{% endif %}{% if fixtures %}{% for fixture in fixtures %}, {{ fixture.name }}{% endfor %}{% endif %}):
    """{{ test.description }}"""
    {% if test.setup %}
    # Test setup
    {% for setup_line in test.setup %}
    {{ setup_line }}
    {% endfor %}
    {% endif %}

    {% if test.is_dataset_test %}
    # Dataset-specific test logic
    {% if test.dataset_operations %}
    {% for operation in test.dataset_operations %}
    {{ operation }}
    {% endfor %}
    {% endif %}
    {% endif %}

    # Test execution
    {% for assertion in test.assertions %}
    {{ assertion }}
    {% endfor %}

    {% if test.cleanup %}
    # Test cleanup
    {% for cleanup_line in test.cleanup %}
    {{ cleanup_line }}
    {% endfor %}
    {% endif %}

{% endfor %}
'''

    def _parse_test_specification(self, test_spec: Union[str, Dict[str, Any]]) -> Dict[str, Any]:
        """
        Parse test specification from JSON string or dict.

        Args:
            test_spec: Test specification as JSON string or dict

        Returns:
            Parsed test specification dictionary
        """
        if isinstance(test_spec, str):
            try:
                spec = json.loads(test_spec)
            except json.JSONDecodeError as e:
                raise DevelopmentToolValidationError(f"Invalid JSON specification: {e}")
        else:
            spec = test_spec

        # Validate required fields
        required_fields = ['tests']
        for field in required_fields:
            if field not in spec:
                raise DevelopmentToolValidationError(f"Missing required field: {field}")

        # Set defaults
        spec.setdefault('imports', [])
        spec.setdefault('fixtures', [])
        spec.setdefault('harness', self.config.harness)

        return spec

    def _generate_class_name(self, name: str) -> str:
        """Generate a valid class name from test name."""
        # Convert to PascalCase and ensure it starts with Test
        class_name = ''.join(word.capitalize() for word in name.replace('_', ' ').split())
        if not class_name.startswith('Test'):
            class_name = 'Test' + class_name
        return class_name

    def _enhance_with_dataset_features(self, spec: Dict[str, Any]) -> Dict[str, Any]:
        """
        Enhance test specification with dataset-specific features.

        Args:
            spec: Base test specification

        Returns:
            Enhanced specification with dataset features
        """
        # Add dataset imports if not present
        dataset_imports = spec.get('dataset_imports', [])
        if not dataset_imports:
            dataset_imports = [
                "# Dataset imports would go here",
                "# from ipfs_datasets_py import IPFSDatasets"
            ]
        spec['dataset_imports'] = dataset_imports

        # Add dataset fixtures
        dataset_fixtures = spec.get('dataset_fixtures', [])
        if not dataset_fixtures:
            dataset_fixtures = [
                {
                    "name": "sample_dataset",
                    "value": "{'text': ['hello', 'world'], 'label': [0, 1]}",
                    "description": "Sample dataset for testing"
                }
            ]
        spec['dataset_fixtures'] = dataset_fixtures

        # Mark dataset-related tests
        for test in spec['tests']:
            test['is_dataset_test'] = any(
                keyword in test.get('description', '').lower()
                for keyword in ['dataset', 'ipfs', 'load', 'save', 'transform']
            )

            # Add dataset operations for dataset tests
            if test['is_dataset_test'] and 'dataset_operations' not in test:
                test['dataset_operations'] = [
                    "# Dataset operations will be added based on test description",
                    "# This is a placeholder for dataset-specific logic"
                ]

        return spec

    def _generate_test_content(self, spec: Dict[str, Any]) -> str:
        """
        Generate test file content from specification.

        Args:
            spec: Parsed test specification

        Returns:
            Generated test file content
        """
        # Add timestamp
        spec['timestamp'] = datetime.now().isoformat()

        # Select template based on harness
        if spec['harness'].lower() == 'pytest':
            template_str = self._get_pytest_template()
        else:
            template_str = self._get_unittest_template()

        # Render template
        template = self.jinja_env.from_string(template_str)
        return template.render(**spec)

    async def _execute_core(self, **kwargs) -> Dict[str, Any]:
        """
        Core test generation logic.

        Args:
            name: Test name
            description: Test description
            test_specification: JSON test specification (str or dict)
            output_dir: Output directory (optional)
            harness: Test harness (unittest/pytest, optional)

        Returns:
            Test generation result
        """
        # Extract parameters
        name = kwargs.get('name')
        description = kwargs.get('description', '')
        test_spec = kwargs.get('test_specification')
        output_dir = kwargs.get('output_dir', self.config.output_dir)
        harness = kwargs.get('harness', self.config.harness)

        # Validate inputs
        if not name:
            raise DevelopmentToolValidationError("Test name is required")
        if not test_spec:
            raise DevelopmentToolValidationError("Test specification is required")

        # Parse specification
        spec = self._parse_test_specification(test_spec)
        spec['name'] = name
        spec['description'] = description
        spec['class_name'] = self._generate_class_name(name)
        if harness:
            spec['harness'] = harness

        # Enhance with dataset features
        spec = self._enhance_with_dataset_features(spec)

        # Validate and create output directory
        output_path = self._validate_output_dir(output_dir)

        # Generate test file content
        test_content = self._generate_test_content(spec)

        # Generate filename
        test_filename = f"test_{name.lower().replace(' ', '_')}.py"
        test_file_path = output_path / test_filename

        # Write test file
        try:
            with open(test_file_path, 'w', encoding='utf-8') as f:
                f.write(test_content)
        except Exception as e:
            raise DevelopmentToolExecutionError(f"Failed to write test file: {e}")

        # Prepare result
        result = {
            "test_file": str(test_file_path),
            "test_name": name,
            "harness": spec['harness'],
            "num_tests": len(spec['tests']),
            "has_fixtures": len(spec.get('fixtures', [])) > 0,
            "has_dataset_features": any(test.get('is_dataset_test', False) for test in spec['tests']),
            "file_size": test_file_path.stat().st_size
        }

        return self._create_success_result(result, {
            "specification_keys": list(spec.keys()),
            "output_directory": str(output_path)
        })


# MCP tool function (synchronous wrapper)
def test_generator(
    name: str,
    description: str = "",
    test_specification: Union[str, Dict[str, Any]] = None,
    output_dir: str = None,
    harness: str = None
) -> Dict[str, Any]:
    """
    Generate test files from JSON specifications.

    Args:
        name: Name of the test suite
        description: Description of what the tests cover
        test_specification: JSON specification as string or dict containing test definitions
        output_dir: Directory to output test files (defaults to config setting)
        harness: Test framework to use - 'unittest' or 'pytest' (defaults to config setting)

    Returns:
        Dict containing generation results and metadata

    Example test_specification:
    {
        "imports": ["import requests", "from mymodule import MyClass"],
        "fixtures": [
            {"name": "sample_data", "value": "{'key': 'value'}"}
        ],
        "tests": [
            {
                "name": "example_test",
                "description": "Test example functionality",
                "assertions": ["self.assertEqual(1, 1)", "self.assertTrue(True)"],
                "parametrized": false
            }
        ]
    }
    """
    tool = TestGeneratorTool()

    # Handle async execution - try to use existing loop, or create a new one
    try:
        # Check if there's already an event loop running
        try:
            loop = asyncio.get_running_loop()
            # If we get here, there's a running loop, so we need to use a different approach
            import concurrent.futures
            import threading

            def run_in_thread():
                new_loop = asyncio.new_event_loop()
                asyncio.set_event_loop(new_loop)
                try:
                    return new_loop.run_until_complete(tool.execute(
                        name=name,
                        description=description,
                        test_specification=test_specification,
                        output_dir=output_dir,
                        harness=harness
                    ))
                finally:
                    new_loop.close()

            with concurrent.futures.ThreadPoolExecutor() as executor:
                future = executor.submit(run_in_thread)
                return future.result()

        except RuntimeError:
            # No running loop, we can use asyncio.run
            return asyncio.run(tool.execute(
                name=name,
                description=description,
                test_specification=test_specification,
                output_dir=output_dir,
                harness=harness
            ))
    except Exception as e:
        # Fallback to synchronous execution if async fails
        return {
            "success": False,
            "error": "execution_error",
            "message": f"Failed to execute test generator: {e}",
            "metadata": {
                "tool": "test_generator",
                "timestamp": datetime.now().isoformat()
            }
        }
