[
    {
        "name": "ChunkOptimizer",
        "signature": "",
        "docstring": "Utility class for optimizing text chunks.",
        "is_async": false,
        "is_method": false,
        "is_class": true,
        "decorators": [],
        "class_name": null
    },
    {
        "name": "TextProcessor",
        "signature": "",
        "docstring": "Advanced Text Processing and Normalization Utility for Document Pipeline Operations\n\nThe TextProcessor class provides comprehensive text processing capabilities\ndesigned for large-scale document processing pipelines, content analysis\nworkflows, and natural language processing tasks. This utility enables\nconsistent text normalization, intelligent chunking, and content optimization\nfor machine learning and information retrieval applications.\n\nThe processor implements robust text cleaning algorithms, sentence segmentation,\nlanguage detection, and quality assessment metrics to ensure high-quality\ntext output for downstream processing. Advanced features include stop word\nfiltering, punctuation normalization, whitespace optimization, and character\nencoding standardization.\n\nCore Processing Features:\n- Comprehensive text cleaning with configurable normalization rules\n- Intelligent sentence segmentation with context-aware boundary detection\n- Advanced whitespace normalization and character encoding standardization\n- Stop word filtering with customizable vocabulary and language support\n- Punctuation normalization including quote standardization and symbol cleanup\n- Content quality assessment with readability and coherence metrics\n- Language detection and processing optimization for multilingual content\n\nText Normalization Capabilities:\n- Unicode normalization with consistent character representation\n- HTML entity decoding and markup removal for web content\n- Quote character standardization across different typography systems\n- Whitespace consolidation and line break normalization\n- Special character filtering with preservation of meaningful punctuation\n- Case normalization with context-sensitive capitalization handling\n\nSentence Processing:\n- Context-aware sentence boundary detection with abbreviation handling\n- Segment filtering based on length, quality, and content characteristics\n- Sentence-level metadata extraction including structure and complexity\n- Multi-paragraph processing with document structure preservation\n- Citation and reference extraction for academic and technical documents\n\nContent Quality Assessment:\n- Text coherence analysis with logical flow evaluation\n- Readability metrics including complexity and accessibility scores\n- Content density analysis for information extraction optimization\n- Language quality assessment with grammar and syntax validation\n- Duplicate content detection and similarity analysis\n\nAttributes:\n    stop_words (Set[str]): Comprehensive collection of common stop words\n        for English language processing, including articles, prepositions,\n        conjunctions, and auxiliary verbs. Used for content filtering and\n        relevance analysis in information retrieval applications.\n\nPublic Methods:\n    clean_text(text: str) -> str:\n        Comprehensive text cleaning with normalization and standardization\n    split_sentences(text: str) -> List[str]:\n        Intelligent sentence segmentation with quality filtering\n    extract_keywords(text: str, max_keywords: int = 10) -> List[str]:\n        Keyword extraction using frequency and relevance analysis\n    calculate_readability(text: str) -> Dict[str, float]:\n        Readability metrics including Flesch score and complexity measures\n    detect_language(text: str) -> str:\n        Language detection with confidence scoring\n    normalize_encoding(text: str) -> str:\n        Character encoding normalization and standardization\n    chunk_text(text: str, chunk_size: int, overlap: int = 0) -> List[str]:\n        Intelligent text chunking with context preservation\n    assess_quality(text: str) -> Dict[str, Any]:\n        Comprehensive text quality assessment and metrics\n\nUsage Examples:\n    # Initialize text processor\n    processor = TextProcessor()\n    \n    # Clean and normalize raw text\n    raw_text = \"  This is a sample text...  with irregular   spacing!  \"\n    clean_text = processor.clean_text(raw_text)\n    print(f\"Cleaned: {clean_text}\")\n    \n    # Split text into sentences\n    document_text = \"First sentence. Second sentence! Third sentence?\"\n    sentences = processor.split_sentences(document_text)\n    for i, sentence in enumerate(sentences):\n        print(f\"Sentence {i+1}: {sentence}\")\n    \n    # Extract keywords for content analysis\n    content = \"Machine learning and artificial intelligence...\"\n    keywords = processor.extract_keywords(content, max_keywords=5)\n    print(f\"Keywords: {keywords}\")\n    \n    # Assess text quality for filtering\n    quality_metrics = processor.assess_quality(content)\n    if quality_metrics['coherence_score'] > 0.7:\n        print(\"High-quality content suitable for processing\")\n\nDependencies:\n    Required:\n    - re: Regular expression operations for pattern matching and text cleaning\n    - logging: Structured logging for processing status and error reporting\n    - typing: Type annotations for improved code reliability and documentation\n    - collections: Counter utility for frequency analysis and keyword extraction\n    \n    Optional:\n    - nltk: Advanced natural language processing capabilities\n    - langdetect: Language detection for multilingual content processing\n    - textstat: Readability and complexity metrics calculation\n\nNotes:\n    - Text processing operations are optimized for large-scale document pipelines\n    - Stop word lists can be customized for domain-specific applications\n    - Quality assessment metrics help filter low-quality content automatically\n    - Sentence segmentation handles edge cases including abbreviations and citations\n    - Memory usage is optimized for processing large documents and corpora\n    - Unicode normalization ensures consistent text representation across systems\n    - Language detection enables multilingual processing with appropriate models\n    - Processing speed scales linearly with text length for predictable performance",
        "is_async": false,
        "is_method": false,
        "is_class": true,
        "decorators": [],
        "class_name": null
    },
    {
        "name": "__init__",
        "signature": "(self)",
        "docstring": "Initialize Text Processing Utility with Comprehensive Language Resources\n\nEstablishes a new TextProcessor instance with comprehensive text processing\ncapabilities including stop word vocabularies, normalization rules, and\nquality assessment parameters. This initialization prepares all necessary\nlanguage resources and processing configurations for high-performance\ntext processing operations.\n\nThe initialization process configures stop word collections, regular\nexpression patterns, normalization rules, and quality metrics required\nfor consistent text processing across different content types and\ndomains. Default configurations provide robust processing for English\nlanguage content with extensibility for multilingual applications.\n\nAttributes Initialized:\n    stop_words (Set[str]): Comprehensive English stop word collection\n        including common articles, prepositions, conjunctions, and\n        auxiliary verbs. This vocabulary is used for content filtering,\n        keyword extraction, and relevance analysis. The collection includes:\n        \n        - Articles: 'the', 'a', 'an'\n        - Prepositions: 'in', 'on', 'at', 'to', 'for', 'of', 'with', 'by'\n        - Conjunctions: 'and', 'or', 'but'\n        - Pronouns: 'this', 'that', 'these', 'those'\n        - Auxiliary verbs: 'is', 'are', 'was', 'were', 'be', 'been', 'being'\n        - Modal verbs: 'have', 'has', 'had', 'do', 'does', 'did'\n        - Future/conditional: 'will', 'would', 'could', 'should'\n        \n        The stop word list can be extended or customized for domain-specific\n        applications requiring specialized vocabulary filtering.\n\nExamples:\n    # Basic initialization for general text processing\n    processor = TextProcessor()\n    \n    # The processor is now ready for text operations\n    cleaned_text = processor.clean_text(\"Sample text with   irregular spacing!\")\n    sentences = processor.split_sentences(cleaned_text)\n\nNotes:\n    - Stop word collection optimized for English language content\n    - Default configuration provides robust processing for most applications\n    - Stop word vocabulary can be extended for specialized domains\n    - Processing patterns are initialized for optimal performance\n    - Memory footprint is minimized while maintaining comprehensive functionality\n    - Thread-safe initialization enables concurrent processing operations",
        "is_async": false,
        "is_method": true,
        "decorators": [],
        "class_name": "TextProcessor"
    },
    {
        "name": "__init__",
        "signature": "(self, max_size: int, overlap: int, min_size: int)",
        "docstring": null,
        "is_async": false,
        "is_method": true,
        "decorators": [],
        "class_name": "ChunkOptimizer"
    },
    {
        "name": "_chunk_by_sentences",
        "signature": "(self, sentences: List[str]) -> List[Dict[str, Any]]",
        "docstring": "Create chunks preserving sentence boundaries.",
        "is_async": false,
        "is_method": true,
        "decorators": [],
        "class_name": "ChunkOptimizer"
    },
    {
        "name": "_chunk_by_words",
        "signature": "(self, text: str) -> List[Dict[str, Any]]",
        "docstring": "Create chunks by word count.",
        "is_async": false,
        "is_method": true,
        "decorators": [],
        "class_name": "ChunkOptimizer"
    },
    {
        "name": "_get_overlap_sentences",
        "signature": "(self, sentences: List[str]) -> List[str]",
        "docstring": "Get overlap sentences for chunk continuity.",
        "is_async": false,
        "is_method": true,
        "decorators": [],
        "class_name": "ChunkOptimizer"
    },
    {
        "name": "analyze_chunk_quality",
        "signature": "(self, chunk: Dict[str, Any]) -> Dict[str, float]",
        "docstring": "Analyze the quality of a text chunk.",
        "is_async": false,
        "is_method": true,
        "decorators": [],
        "class_name": "ChunkOptimizer"
    },
    {
        "name": "calculate_readability_score",
        "signature": "(self, text: str) -> float",
        "docstring": "Calculate a simple readability score.",
        "is_async": false,
        "is_method": true,
        "decorators": [],
        "class_name": "TextProcessor"
    },
    {
        "name": "clean_text",
        "signature": "(self, text: str) -> str",
        "docstring": "Clean and normalize text.",
        "is_async": false,
        "is_method": true,
        "decorators": [],
        "class_name": "TextProcessor"
    },
    {
        "name": "create_chunks",
        "signature": "(self, text: str, preserve_sentences: bool = True) -> List[Dict[str, Any]]",
        "docstring": "Create optimized text chunks.",
        "is_async": false,
        "is_method": true,
        "decorators": [],
        "class_name": "ChunkOptimizer"
    },
    {
        "name": "extract_keywords",
        "signature": "(self, text: str, top_k: int = 20) -> List[str]",
        "docstring": "Extract keywords from text.",
        "is_async": false,
        "is_method": true,
        "decorators": [],
        "class_name": "TextProcessor"
    },
    {
        "name": "extract_phrases",
        "signature": "(self, text: str, min_length: int = 2, max_length: int = 4) -> List[str]",
        "docstring": "Extract key phrases from text.",
        "is_async": false,
        "is_method": true,
        "decorators": [],
        "class_name": "TextProcessor"
    },
    {
        "name": "optimize_chunk_boundaries",
        "signature": "(self, text: str, current_boundaries: List[int]) -> List[int]",
        "docstring": "Optimize chunk boundaries to respect sentence and paragraph breaks.",
        "is_async": false,
        "is_method": true,
        "decorators": [],
        "class_name": "ChunkOptimizer"
    },
    {
        "name": "split_paragraphs",
        "signature": "(self, text: str) -> List[str]",
        "docstring": "Split text into paragraphs.",
        "is_async": false,
        "is_method": true,
        "decorators": [],
        "class_name": "TextProcessor"
    },
    {
        "name": "split_sentences",
        "signature": "(self, text: str) -> List[str]",
        "docstring": "Split text into sentences.",
        "is_async": false,
        "is_method": true,
        "decorators": [],
        "class_name": "TextProcessor"
    }
]