#!/bin/bash
# Test GPU runner setup locally

set -e

echo "â•”â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•—"
echo "â•‘          Testing GPU Runner Configuration                â•‘"
echo "â•šâ•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•"
echo ""

# Check GPU availability
echo "ğŸ® GPU Information:"
nvidia-smi --query-gpu=index,name,driver_version,memory.total --format=csv
echo ""

# Check Docker GPU access
echo "ğŸ³ Testing Docker GPU access..."
docker run --rm --gpus all nvidia/cuda:12.0.0-base-ubuntu22.04 nvidia-smi
echo ""

# Build GPU Docker image
echo "ğŸ”¨ Building GPU Docker image..."
docker build -t ipfs-datasets-py:gpu-test -f docker/Dockerfile.gpu .
echo ""

# Test GPU in container
echo "ğŸ§ª Testing GPU in Docker container..."
docker run --rm --gpus all ipfs-datasets-py:gpu-test
echo ""

# Test PyTorch GPU
echo "ğŸ”¥ Testing PyTorch GPU access..."
docker run --rm --gpus all ipfs-datasets-py:gpu-test python -c "
import torch
print(f'âœ… PyTorch: {torch.__version__}')
print(f'âœ… CUDA available: {torch.cuda.is_available()}')
print(f'âœ… CUDA version: {torch.version.cuda}')
print(f'âœ… GPU count: {torch.cuda.device_count()}')
for i in range(torch.cuda.device_count()):
    props = torch.cuda.get_device_properties(i)
    print(f'   GPU {i}: {torch.cuda.get_device_name(i)}')
    print(f'      Memory: {props.total_memory / 1024**3:.2f} GB')
    print(f'      Compute: {props.major}.{props.minor}')
"
echo ""

echo "âœ… All GPU tests passed!"
echo ""
echo "Next steps:"
echo "  1. Run: bash setup_gpu_runner.sh"
echo "  2. Get token from: https://github.com/endomorphosis/ipfs_datasets_py/settings/actions/runners/new"
echo "  3. Push changes to trigger GPU workflows"
echo ""
